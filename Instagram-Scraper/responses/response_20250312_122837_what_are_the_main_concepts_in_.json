{
  "query": "What are the main concepts in the knowledge base related to transformers and how do they relate to each other?",
  "answer": "Based on the provided context, there is no direct information about the main concepts related to transformers and how they relate to each other. However, I can provide a general overview based on my knowledge:\n\nTransformers are a type of neural network architecture that has become very popular in natural language processing (NLP) and other domains. The main concepts related to transformers include:\n\n1. Self-Attention Mechanism: This is the core component of transformers that allows the model to weigh different parts of the input sequence when computing representations. It enables capturing long-range dependencies.\n\n2. Multi-Head Attention: Using multiple attention heads in parallel to capture different types of relationships within the input data.\n\n3. Encoder-Decoder Architecture: Many transformer models follow an encoder-decoder structure, where the encoder maps the input to a representation and the decoder generates the output based on that representation.\n\n4. Positional Encoding: Since transformers do not have an inherent notion of order in sequences, positional encodings are added to capture the position information.\n\n5. Residual Connections and Layer Normalization: These techniques help with training deep transformer models effectively.\n\nThese concepts work together to enable transformers to effectively process sequential data like text, speech, or time series while capturing long-range dependencies and parallelizing computations. The self-attention mechanism is the key innovation that allows transformers to outperform previous sequence models like RNNs on many tasks.\n\nWhile the provided context does not directly discuss these concepts, it mentions some applications and libraries related to transformers, like the Hugging Face Transformers library [Source 1], research on self-correction for small language models [Source 2], and the Mistral inference library [Source 3].",
  "sources": [
    {
      "content_id": 4,
      "title": "transformers",
      "url": "https://github.com/huggingface/transformers",
      "source_type": "github",
      "metadata": {
        "stars": 141045,
        "language": "Python",
        "topics": [
          "bert",
          "deep-learning",
          "flax",
          "hacktoberfest",
          "jax",
          "language-model",
          "language-models",
          "machine-learning",
          "model-hub",
          "natural-language-processing",
          "nlp",
          "nlp-library",
          "pretrained-models",
          "python",
          "pytorch",
          "pytorch-transformers",
          "seq2seq",
          "speech-recognition",
          "tensorflow",
          "transformer"
        ],
        "forks": 28247,
        "repo_name": "transformers",
        "full_name": "huggingface/transformers"
      }
    },
    {
      "content_id": 21,
      "title": "Self-Taught Self-Correction for Small Language Models",
      "url": "http://arxiv.org/abs/2503.08681v1",
      "source_type": "research_paper",
      "metadata": {
        "authors": "Viktor Moskvoretskii, Chris Biemann, Irina Nikishina",
        "published_date": "2025-03-11T17:57:44",
        "year": 2025,
        "sections": {
          "abstract": "Self-Taught Self-Correction for Small Language Models\nViktor Moskvoretskii1,2, Chris Biemann3, Irina Nikishina3\n1Skoltech,2HSE University,3University of Hamburg\nCorrespondence: vvmoskvoretskii@gmail.com\nAlthough large language models (LLMs) have\nachieved remarkable performance across var-\nious tasks, they remain prone to errors. A\nkey challenge is enabling them to self-correct.\nWhile prior research has relied on external\ntools or large proprietary models, this work\nexplores self-correction in small language mod-\nels (SLMs) through iterative fine-tuning us-\ning solely self-generated data. We introduce\nthe Self-Taught Self-Correction (STaSC) algo-\nrithm, which incorporates multiple algorith-\nmic design choices. Experimental results on\na question-answering task demonstrate that\nSTaSC effectively learns self-correction, lead-\ning to significant performance improvements.\nOur analysis further provides insights into the\nmechanisms of self-correction and the impact\nof different design choices on learning dynam-\nics and overall performance. To support future\nresearch, we release our user-friendly codebase\n",
          "introduction": "Recent advanced LLM employ complex reason-\ning (Guo et al., 2025) and meta-reasoning (Xiang\net al., 2025), expanding their capabilities. However,\neven the most advanced models are prone to errors,\nincluding hallucinations (Huang et al., 2025) and\nlogical inconsistencies (Ghosh et al., 2024), requir-\ning external verification or human intervention. To\naddress those problems, self-correction \u2014 the abil-\nity to revise their own outputs \u2014 has been evolved\n(Madaan et al., 2023). The existing approaches\nmostly use zero-shot prompting (Madaan et al.,\n2023; Shinn et al., 2024), external evaluators for\ncorrection or feedback (Zhang et al., 2024) or ap-\nply large proprietary models and focus specifically\non mathematical tasks (Kumar et al., 2024).\nIn this study, we focus on self-correction with-\nout external information or evaluators, ensuring\ninference efficiency while relying solely on themodel\u2019s intrinsic knowledge. We investigate the\nself-correction capabilities of small language mod-\nels (SLMs) by applying iterative fine-tuning on\nself-generated data, allowing models to improve\ntheir outputs without external supervision.\nWe introduce the Self-Taught Self-Correction\n(STaSC) algorithm, which trains models to self-\ncorrect using self-generated trajectories, adapting\nthe core idea of STaR (Zelikman et al., 2022).\nSTaSC extends and unifies approaches that itera-\ntively train with self-generated trajectories, includ-\ning the Self-Correction (SC) algorithm (Welleck\net al., 2022), by incorporating flexible design\nchoices. Unlike prior methods, STaSC provides\ncontrol over initial answer exploration, correction\nfiltering, and iterative fine-tuning, encompassing\nSC as a special case and demonstrating how dif-\nferent algorithmic choices impact self-correction\n",
          "methodology": "The key extension of our algorithm over previ-ous baselines is the incorporation of flexible al-\ngorithmic choices. Initial answers can be drawn\nfrom either a frozen model or the previous iteration,\ncorrections can be filtered based on strict improve-\nments or by allowing non-worsening revisions, and\nfine-tuning can be performed from either the initial\nmodel or the latest iteration.\n2.2 Formal Definition and Algorithmic\nChoices\nBelow, we outline the Algorithm 1 steps and key\ndesign choices, each of which influences the self-\ncorrection process. For each choice, we define an\nabbreviation that will be used to denote specific\nalgorithm configurations, such as STaSC FIF, where\nsubscripts specify the selected options.\n2.2.1 Requirements and Notation\nThe algorithm begins with an initial language\nmodel state, M0, and an initial dataset, D0, con-\nsisting of input-output pairs (x, y). Additionally,\nwe define a number of improvement iterations N,\nthe number of sampled initial generations Ninit, the\nnumber of sampled corrections Ncorr, and a reward\nfunction r, which evaluates the quality of model-\ngenerated outputs.\nStep 1: Sampling Initial Answers In the first\nstep, we sample Ninitinitial answers \u02c6y1for each\ninput xin the dataset D0. The primary design\nchoice here is the selection of the model Mused\nfor sampling:\n2\nAlgorithm 1 Self-Taught Self-Correction (STaSC)\nRequire: Initial model M0, dataset D0, number of iterations N, initial samples Ninit, correction samples\nNcorr, reward function r\n1:forn= 1toNdo\n2: Step 1: Sample Initial Answers\n3: \u02c6Y1\ni={\u02c6y1\nij}Ninit\nj=1\u223cM(xi),\u2200xi\u2208D0\n4: Option 1: M=Mn\u22121(Evolving Initialization)\n5: Option 2: M=M0(Fixed Initialization)\n6: Step 2: Sample Corrections\n7: \u02c6Y2\ni={\u02c6y2\nijk}Ncorr\nk=1\u223cMn\u22121(xi,\u02c6y1\nij),\u2200\u02c6y1\nij\u2208\u02c6Y1\ni\n8: Step 3: Filter Corrections\n9: D+\nn={(xi,\u02c6y1\nij,\u02c6y2\nijk)|r(\u02c6y2\nijk)> r(\u02c6y1\nij)}\n10: D=\nn={(xi,\u02c6y1\nij,\u02c6y2\nijk)|r(\u02c6y2\nijk) =r(\u02c6y1\nij)\u2265t}\n11: Option 1: Dn=D+\nn (Improving Filter)\n12: Option 2: Dn=D+\nn\u222aD=\nn (Non-Decreasing Filter)\n13: Step 4: Fine-Tuning\n14: Mn=train(M,{\u02c6y2\nijk|(xi,\u02c6y1\nij,\u02c6y2\nijk)\u2208Dn})\n15: Option 1: M=M0(Fixed Fine-Tuning)\n16: Option 2: M=Mn\u22121(Evolvong Fine-Tuning)\n17:end for\n\u2022Fixed Initialization (STaSC F**)\u2013 Initial an-\nswers are sampled using the frozen model M0,\nas in the SC algorithm (Welleck et al., 2022).\n\u2022Evolving Initialization (STaSC E**)\u2013 Initial\nanswers are sampled using the model from\nthe previous iteration Mn\u22121, as in the original\nSTaR (Zelikman et al., 2022).\nFixed Initialization ensures robustness to varia-\ntions in self-improvement and dataset shifts, main-\ntaining consistency across iterations. In contrast,\nEvolving Initialization allows for greater explo-\nration, potentially leading to more diverse refine-\nments and improved performance.\nStep 2: Sample Corrections At the second step,\nwe sample Ncorrcorrections \u02c6y2for each output \u02c6y1\nin dataset D0using the model from the last iteration\nMn\u22121.\nStep 3: Filtering Corrections In this step, we fil-\nter the corrections using the reward function r(\u02c6y2)\nto construct the fine-tuning dataset Dn. The key\ndesign choice here is the filtering criterion for se-\nlecting corrections.\n\u2022Improving Filter (STaSC *I*)\u2013 Selects only\ncorrections that strictly improve the reward,\nensuring r(\u02c6y2)> r(\u02c6y1), as used in STaSC\nand SC.\u2022Non-Decreasing Filter (STaSC *N*)\u2013 Selects\ncorrections that either strictly improve the re-\nward or retain the initial answer if it was al-\nready correct, ensuring (r(\u02c6y2) =r(\u02c6y1))\u2229\n(r(\u02c6y1)\u2265t), as proposed in SCoRE (Kumar\net al., 2024). This allows the model to pre-\nserve already correct answers while still refin-\ning incorrect ones.\nThe Improving Filter enforces strict improve-\nment for every input, ensuring that only progres-\nsively better outputs are used for fine-tuning. In\ncontrast, the Non-Decreasing Filter permits stable\nanswers to be retained when already correct, allow-\ning for a more conservative refinement process.\nStep 4: Fine-Tuning In this step, we fine-tune\nthe model on the dataset Dnobtained from Step\n3 to produce the improved model Mn. The key\ndesign choice here is the selection of the model\nused for fine-tuning:\n\u2022Fixed Fine-Tuning (STaSC **F) \u2013 Fine-tunes\nthe initial model M0, as done in the original\nSTaR recipe, ensuring stability across itera-\ntions and reducing the risk of accumulating\nerrors.\n\u2022Evolving Fine-Tuning (STaSC **E) \u2013 Fine-\ntunes the model from the previous iteration\nMn\u22121, as in SC, allowing the model to pro-\ngressively improve and adapt across iterations.\n3\nNinitNcorrQwen-2.5-1.5B Phi3-mini\nmax{r(\u02c6Y1)}max{r(\u02c6Y2)}max{r(\u02c6Y1)}max{r(\u02c6Y2)}\n11 - - 0.320\u00b10.005 0 .372\u00b10.010\n3 0.248\u00b10.011 0 .208\u00b10.011 0.326\u00b10.009 0 .376\u00b10.010\n5 0.230\u00b10.011 0 .228\u00b10.021 0.352\u00b10.016 0 .384\u00b10.012\n31 0.236\u00b10.007 0 .232\u00b10.018 0.334\u00b10.011 0 .362\u00b10.024\n3 0.264\u00b10.015 0 .238\u00b10.018 0.342\u00b10.010 0 .372\u00b10.012\n5 0.273\u00b10.017 0 .236\u00b10.019 0.332\u00b10.007 0 .384\u00b10.013\n51 0.273\u00b10.012 0 .250\u00b10.024 0.334\u00b10.008 0 .378\u00b10.016\n3 0.295\u00b10.019 0 .244\u00b10.023 0.336\u00b10.021 0 .354\u00b10.026\n5 0.300\u00b10.020 0 .248\u00b10.029 0.350\u00b10.012 0 .376\u00b10.011\nTable 1: Maximum reward rover iterations for initial answer r(\u02c6Y1)and for correction r(\u02c6Y2)for different number\nof samples and initial generations. Bold corresponds to the best performance.\nFixed Fine-Tuning maintains a stable learning\nprocess by always training from the original model,\npreventing drift. In contrast, Evolving Fine-Tuning\nenables iterative adaptation, potentially leading to\ngreater long-term improvements but also introduc-\ning the risk of compounding errors.\n7 Limitations\n\u2022The selected SLMs, while effective, may have\ncertain capacity limitations that could influ-\nence the extent of the self-correction process.\n\u2022Experiments were conducted with a single\nrun, which, while sufficient for initial insights,\nmay introduce some variability.\n\u2022The evaluation focuses on a Question Answer-\ning (QA) task, leaving open the opportunity\nto explore performance across other tasks and\ndomains.\n8\n\u2022The chosen hyperparameters, though reason-\nable, may not fully optimize the model\u2019s learn-\ning efficiency or overall performance.\n\u2022A more detailed analysis of the types and pat-\nterns of corrections could further enrich our\nunderstanding of the self-correction mecha-\nnism.\n\u2022The reward function, while practical, may not\nperfectly capture all nuances of desired behav-\nior, presenting room for refinement in future\nwork.\n8 Ethical Considerations\nOur work enables small language models to\nself-correct using self-generated data. We em-\nploy advanced models like Qwen-2.5 and Phi-3,\npre-trained on diverse datasets, including user-\ngenerated content. While efforts have been made\nto remove harmful or biased data, some biases may\npersist in outputs. This does not undermine our\nmethods, which are designed to self-correct factual\ninaccuracies and are adaptable to other rigorously\ndebiased models. Beyond inherent bias challenges,\nour work raises no additional ethical concerns.\n",
          "results": "Our results on the Natural Questions dataset\n(Kwiatkowski et al., 2019) show that SLMs can\nlearn to self-correct using self-synthesized data,\nwhile also improving their initial answer quality\ndespite being trained solely for corrections. We\nrelease easy-to-use and adaptable code for self-\ncorrection algorithms at https://github.com/\nVityaVitalich/STASC .\nThe contributions of the paper are as follows:\n\u2022We propose the Self-Taught Self-Correction\n(STaSC) algorithm, unifying and extending\nexisting self-correction methods trained with\nself-generated trajectories.\n\u2022We conduct extensive experiments on a purely\nNatural Language Processing (NLP) task \u2014\nQuestion Answering \u2014 using an open-source\nSLMs, demonstrating their ability to learn self-\ncorrection with self-synthesized data.\n\u2022We release open-source, easily adaptable code\nfor self-correction, along with efficient SLMs\n1arXiv:2503.08681v1  [cs.CL]  11 Mar 2025\nOUTPUT OUTPUTGenerator Generator Generator               Corrector CorrectorINPUT\nOUTPUTINPUT\nOUTPUTOUTPUT\nOUTPUTSTaR Self-Correction\n\ud83d\udd25STaSC\n\ud83d\udd25INPUT\n\ud83d\udd25  / \u2744 \ud83d\udd25OUTPUT\n\u2744 Figure 1: Illustration of the self-improvement method STaR (left) (Zelikman et al., 2022), self-correction method\nSC(center) (Welleck et al., 2022), and our method, STaSC (right). STaCS offers flexible control over initial answer\nexploration, correction filtering, and iterative fine-tuning. It is inspired by STaR and effectively encompasses SC as\na special case. SC and STaSC allow several initial answers and corrections. The dotted line in the STaSC denotes\ntwo possible setups: fine-tuning the model and generating from it at the next iteration (Evolving Fine-Tuning) and\nkeeping the Generator frozen and fine-tuning the Corrector model only (Fixed Fine-Tuning).\nwith fewer than 4B parameters, making self-\ncorrection practical and accessible.\n2 Self-Taught Self-Correction\nIn this section, we introduce the Self-Taught\nSelf-Correction (STaSC) algorithm, an adapta-\ntion of STaR (Zelikman et al., 2022) for self-\ncorrection through iterative fine-tuning on self-\nsynthesized data. STaSC unifies and extends\nvarious self-correction approaches using self-\nsynthesized data, including the Self-Correction\n(SC) algorithm (Welleck et al., 2022).\n2.1 Foundations and Enhancements\nFigure 1 presents three algorithms highlighting\ntheir similarities and dissimilarities. The left part\ndescribes the original STaR algorithm which fo-\ncuses on generating reasoning paths, filtering cor-\nrect ones, and fine-tuning them accordingly.\nThe central part of the Figure 1 demonstrates the\nSelf-Correction (SC) algorithm which fine-tunes\nthe corrector model on corrections leading to im-\nprovement, keeping the initial Generator model\nfixed. The right part refers to STaSC , adapting the\nidea of STaR of iteratively refining model outputs\nby sampling an initial answer, generating correc-\ntions, filtering correct trajectories, and fine-tuning\non the successful revisions. This process is re-\npeated over multiple iterations, progressively en-\nIn this section, we describe the main experimental\nprocedure, including dataset selection, evaluation\nmetrics, and implementation details.\nDataset We evaluate our algorithm on\nthe QA task using the Natural Questions\ndataset (Kwiatkowski et al., 2019), which consists\nof factual simple questions. We use a subset of\n500 questions per train and test split, following\nprevious studies (Trivedi et al., 2022; Jeong et al.,\n2024; Moskvoretskii et al., 2025), to ensure\nconsistency and computational efficiency.\nEvaluation We use In-accuracy as the primary\nevaluation metric and reward function, which is\nstandard for this task (Trivedi et al., 2022; Jeong\net al., 2024). It assesses whether the generated\nanswer contains the reference answer, assigning\nr(\u02c6y) = 1 for correct responses and r(\u02c6y) = 0 for\nincorrect ones.\nAll metrics are reported on the test set, which re-\nmains unseen during training. Additionally, we re-\nportmax{r(\u02c6Y1)}andmax{r(\u02c6Y2)}, representing\nthe highest reward obtained for initial answers and\ncorrections across STaSC iterations, respectively.\nTo ensure a fair real-world setup, the reward is not\navailable to the model during inference. Unlikesome prior studies (Shinn et al., 2024), where in-\nference benefits from reward signals derived from\nground-truth labels, our approach aligns with a\nfully unsupervised inference setting, ensuring a\nImplementation Details We conduct experi-\nments using Qwen-2.5-1.5B (Qwen et al., 2025)\nand Phi3-Mini (Abdin et al., 2024), employing de-\nfault generation parameters. The default setup for\nself-correction is 2-shot. STaSC fine-tuning is per-\nformed for 1 epoch with a batch size of 8 and a\nlearning rate of 7\u00d710\u22126. Additional implementa-\ntion details are provided in Appendix A.\nIn this section, we provide the results and discuss\nthem, inspecting the STaSC algorithm design.\n4.1 Impact of NinitandNcorr\nFirstly, we examine how the selection of parame-\ntersNinitandNcorraffects algorithm performance,\nexploring values of 1, 3, and 5 for both models. To\nencourage exploration, we use STaSC EIF, where\ninitial answers are sampled from the previous itera-\ntion, only improving corrections are retained, and\nfine-tuning is performed from the base model to\nensure stability.\nAs shown in Table 1, a greedy approach for\nQwen-2.5-1.5B fails to ensure convergence, as no\nimproving corrections emerge in the first iteration.\nIn contrast, increased exploration significantly en-\nhances performance, likely due to the weaker align-\nment of the initial model. Additionally, we observe\n4\n0 1 2 3 4 5 6 7 8 9 10\nIteration0.100.150.200.250.300.350.40Correction In-Accuracy\nPhi3-mini\n0 1 2 3 4 5 6 7 8 9 10\nIteration0.140.160.180.200.22\nQwen-2.5-1.5B\nSTaSCEIFSTaSCEIESTaSCENFSTaSCENEFigure 2: Correction In-accuracy for STaSC versions with Evolving Initialization for Phi3-mini and Qwen-2.5-1.5B.\n0 1 2 3 4 5 6 7 8 9 10\nIteration0.300.310.320.330.340.350.360.37Correction In-Accuracy\nPhi3-mini\n0 1 2 3 4 5 6 7 8 9 10\nIteration0.160.170.180.190.200.21\nQwen-2.5-1.5B\nSTaSCFIESTaSCFIFSTaSCFNESTaSCFNF\nFigure 3: Correction In-accuracy for STaSC versions with Fixed Initialization for Phi3-mini and Qwen-2.5-1.5B.\nthat exploring initial answers has a greater impact\nthan exploring corrections.\nHowever, this trend does not hold for Phi3-Mini ,\nwhere increasing the number of initial answers does\nnot improve results, but greater correction explo-\nration does. This discrepancy likely stems from\ndifferences in model capabilities. Phi3-Mini, be-\ning inherently stronger, benefits more from refining\ncorrections, whereas Qwen-2.5-1.5B , with lower\ninitial competence, requires a broader search for\ninitial answers to gain useful knowledge early on.\nFor subsequent experiments, we adopt the most\nstable and well-performing configurations:\n\u2022Qwen-2.5-1.5B :(Ninit, Ncorr) = (5 ,5).\n\u2022Phi3-Mini :(Ninit, Ncorr) = (1 ,5).\n4.2 STaSC With Evolving Initialization\nFigure 2 illustrates the dynamics of correction per-\nformance for STaSC variants with Evolving Ini-\ntialization. Below, we discuss the observed effects\non performance, highlighting key trends and their\nimplications.Effect of the Non-Decreasing Filter A key ob-\nservation is that STaSC with the Non-Decreasing\nFilter consistently degrades performance for Phi3-\nMini. This is likely due to the difficulty in stabiliz-\ning training when fewer corrections are retained, in-\ncreasing the risk of overfitting. Interestingly, Qwen\ndoes not exhibit the same decline, possibly due\nto a higher number of retained corrections, which\naugments the data and stabilizes training.\nImpact of Filtering Selectivity We further high-\nlight the need to properly filter corrections when us-\ning Evolving Fine-Tuning, as all Phi3 settings with\nSTaSC **Eexhibit a negative correlation between\nthe leniency of filtered trajectories and the gain in\ncorrection performance ( r=\u22120.51, p < . 001).\nThe more corrections were provided to the Cor-\nrector model, the worse it performed. This sug-\ngests that insufficiently selective filtering intro-\nduces noise, leading to overfitting and a decline\nin performance improvement. Notably, no other\nsetting shows a significant correlation between the\nnumber of filtered corrections and performance.\nEvolving Fine-Tuning Trends Evolving Fine-\nTuning slightly improves correction performance\n5\n0 1 2 3 4 5 6 7 8 9 10\nIteration0.300.320.340.360.38Correction In-Accuracy\nPhi3-mini\n0 1 2 3 4 5 6 7 8 9 10\nIteration0.140.160.180.200.220.24\nQwen-2.5-1.5B\nAlgorithm Type\nSTaSCEIFSTaSCEIEStage Type\nCorrection InitialAlgorithm Type\nSTaSCEIFSTaSCEIEStage Type\nCorrection InitialFigure 4: Correction and Initial Answer In-accuracy for best STaSC versions for Phi3-mini and Qwen-2.5-1.5B.\nStep 1 Step 3 Step 4 Qwen-2.5-1.5B Phi3-mini\nModel Filter Model max{r(\u02c6Y1)} max{r(\u02c6Y2)} max{r(\u02c6Y1)} max{r(\u02c6Y2)}\nM0ImprovingM0 0.212 0 .208\u00b10.014 0.294 0 .354\u00b10.010\nMn\u22121 0.212 0 .212\u00b10.016 0.294 0 .374\u00b10.009\nNon-DecreasingM0 0.212 0 .198\u00b10.012 0.294 0 .348\u00b10.013\nMn\u22121 0.212 0 .206\u00b10.014 0.294 0 .374\u00b10.010\nMn\u22121ImprovingM00.244\u00b10.011 0 .232\u00b10.023 0.352\u00b10.016 0.384\u00b10.12\nMn\u221210.236\u00b10.009 0 .230\u00b10.024 0.392\u00b10.024 0 .394\u00b10.012\nNon-DecreasingM0 0.240\u00b10.009 0.222\u00b10.023 0.316\u00b10.056 0 .356\u00b10.066\nMn\u221210.234\u00b10.013 0 .228\u00b10.022 0.294\u00b10.062 0 .356\u00b10.074\nTable 2: Maximum reward rover iterations for initial answer r(\u02c6Y1)and for correction r(\u02c6Y2)for different settings\nof STaSC Algorithm. Bold values correspond to the best performance, underlined represent second best.\nfor Phi3-Mini. For Qwen, performance increases\nrapidly at first, then declines slightly before rising\nagain in later iterations. This suggests that accumu-\nlated knowledge gains take effect only in the later\nstages, once a sufficient number of corrections and\ninitial answers have been processed.\nKey Takeaways Evolving Initialization is most\neffective when combined with an Improving Fil-\nter and Evolving Fine-Tuning, as these either en-\nhance correction performance or at least prevent\ndegradation. Additionally, filtering selectivity is\ncrucial\u2014overly lenient filters introduce noise and\ncause overfitting.\n4.3 STaSC With Fixed Initialization\nNext, we analyze the trends and implications of\nFixed Initialization, which aligns with the algo-\nrithmic choice used in SC. Figure 3 presents the\ncorrection performance for STaSC variants under\nthis setting.\nEffect of the Non-Decreasing Filter Unlike\nEvolving Initialization, the Non-Decreasing Fil-ter does not lead to a general decline in correc-\ntion performance for either Phi3 or Qwen, with\nan exception of STaSC FNF,where performance de-\ngrades. For Phi3, correction performance remains\nlargely unchanged throughout most iterations, with\nnoticeable improvements emerging only in the final\nstages. This delayed progress suggests that Evolv-\ning Fine-Tuning gradually accumulates knowledge,\nbut its effects become apparent only after suffi-\ncient corrections and initial answers have been pro-\ncessed. A similar trend was previously observed in\nSTaSC EIE, showing that iterative fine-tuning plays\na key role in long-term performance gains.\nImportance of Evolving Fine-Tuning We find\nthat Evolving Fine-Tuning is crucial when using\nFixed Initialization, particularly for Phi3 and, to\na lesser extent, for Qwen. This is expected, as\nEvolving Fine-Tuning serves as the only source of\nexploration in this setting, driving the algorithm\nforward. In contrast, with Fixed Fine-Tuning, we\nobserve a general decline in Phi3\u2019s performance\nand stagnation after the first iteration for Qwen.\nThis suggests that without sufficient exploration,\n6\nrelying solely on corrections from previous steps is\ninsufficient for SLMs.\nWe also observe that when using both Fixed\nInitialization and Fixed Fine-Tuning applying the\nNon-Decreasing Filter further worsens the perfor-\nmance, underscoring the importance of filtering se-\nlectivity. In this setting, exploration is driven solely\nby corrections, making the filtering process critical.\nWhen corrections include too many noisy or unin-\nformative trajectories due to improper filtering, the\nmodel struggles to improve, leading to significant\nperformance degradation. This reinforces the need\nfor a more controlled correction selection process\nto ensure meaningful updates during training.\nKey Takeaways Fixed Initialization reduces ex-\nploration, making the model more tolerant to the\nNon-Decreasing Filter and benefiting more from\nEvolving Fine-Tuning. In contrast, Fixed Fine-\nTuning restricts exploration solely to corrections,\nincreasing reliance on selective filtering to main-\n4.4 STaSC Impact on Initial Answers\nIn this section, we identify the best-performing\nSTaSC configurations and analyze their behavior in\nterms of initial answers and correction dynamics.\nSelection of High-Performing Variants Based\non our previous analysis and Table 2 we determine\nthat STaSC EIEand STaSC EIFexhibit the strongest\nperformance. These results highlight the crucial\nrole of Evolving Initialization and the Improving\nFilter, while leaving the impact of Fine-Tuning\nstrategies open for further investigation.\nPerformance Comparison Figure 4 illustrates\nthe performance trends of initial answers and cor-\nrections for STaSC EIEand STaSC EIF.\nThe effectiveness of Fine-Tuning strategies\nvaries between Phi3 and Qwen. For Phi3, Evolving\nFine-Tuning leads to a substantial increase in ini-\ntial answer quality, surpassing Fixed Fine-Tuning,\nwhile yielding moderate improvements in correc-\ntions. In contrast, for Qwen, Fixed Fine-Tuning\nresults in superior performance for both initial an-\nswers and corrections.\nEffect of Correction-Only Training Interest-\ningly, despite the model being trained exclusively\non corrections, initial answer quality also improves.\nSince gradient updates are applied only to correc-\ntion tokens, this suggests that learning corrections\neither enhances the model\u2019s factual knowledge orimproves its internal reasoning ability at the initial\ngeneration stage.\nAlignment Between Initial Answers and Cor-\nrections Evolving Fine-Tuning progressively re-\nduces the gap between initial answers and correc-\ntions, eventually leading to their alignment. For\nPhi3, initial answers gradually improve until they\nreach the quality of corrections. For Qwen, the\nopposite trend is observed\u2014corrections improve\nuntil they match the initial answers.\nThis suggests that Evolving Fine-Tuning either\nhelps internalize the correction process, leading to\nhigher-quality initial answers as seen in Phi3, or\nstabilizes responses by preventing degradation, as\nobserved in Qwen.\n5 Related Works\nSelf-correction is a relatively new yet actively grow-\ning research domain. A systematic survey defines\nself-correction as a framework in which LLMs re-\nfine their responses using LLMs during inference,\npotentially incorporating external tools or knowl-\nedge (Kamoi et al., 2024). A significant body of\nwork in this direction focuses on leveraging exter-\nnal feedback from external knowledge and verifica-\ntion tools (Jiang et al., 2023; Gou et al., 2024; Pan\net al., 2023; Xu et al., 2023), as these approaches\nprovide high-quality solution evaluation. However,\nin real-world applications, such external resources\nare often unavailable or computationally expensive.\nMoreover, relying on external verification does not\npose a fundamentally challenging task for LLMs,\nlimiting their ability to develop intrinsic reasoning\nand self-improvement capabilities.\nAn alternative approach is intrinsic self-\ncorrection, where the model refines its own outputs\nwithout relying on external critics. This can be\nimplemented in a zero-shot setting, using the same\nmodel iteratively (Madaan et al., 2023), or through\nexternal models trained on synthetic errors(Paul\net al., 2024) or self-generated corrections(Welleck\net al., 2022). While these approaches have shown\npromise, they still rely on external critic models,\nmaking them closer to verification-based genera-\ntion rather than true self-correction.\nThe only work exploring self-correction in its\nnatural form is the SCoRE framework (Kumar\net al., 2024), which was the first to experiment with\nintrinsic self-correction within the same model and\nintroduced a multi-turn RL training approach for\nthis purpose. However, SCoRE lacks a formalized\n7\nSTaSCYou are a helpful reasoning assistant in general\ndomain question answering. Please reason\nthrough the question step by step very shortly\nbefore giving a final answer . Generate a short\nchain-of-thought rationale very shortly , and then\nprovide the final answer in the format: \"Final\nanswer: your answer\"Step-by-step reasoning: The question refers to two dif ferent movies, one about pregnancy and\nanother about Tracey .\nThe actress who played in both needs to be identified.\nRecalling movies about pregnancy , \"Juno\" comes to mind, starring Ellen Page (now Elliot\nPage).\nFor a movie about Tracey , \"Boys Don't Cry\" features Hilary Swank playing the role of\nTracey .\nHowever , these are two dif ferent actresses, so we need to find a common actress. After fact-\nchecking, the actress who played in both movies is likely to be Reese Witherspoon, who starred in\n\"Legally Blonde\" (where her character is named Elle, not Tracey) and \"Sweet Home Alabama\"\n(which involves pregnancy).\nFinal Answer:  Reese W itherspoon\ninputoutputPlease review the given answer for accuracy . If it is incorrect, identify the incorrect information\nand provide the correct details. Ask and answer subquestions, and split the question into parts.\nDisregard the information you already have.\nsystem\npromptsystem\nprompt\ninput\nStep-by-step reasoning: The initial analysis contains several inaccuracies regarding the\nidentification of an actress who starred in both a movie about pregnancy and another featuring a\ncharacter named Tracey . Let's address these inaccuracies and provide the correct information.\nInaccuracies in the Initial Analysis:\nElliot Page in \"Juno\": It's correctly noted that Elliot Page (formerly Ellen Page) starred in\n\"Juno,\" a film centered around teenage pregnancy . However , the analysis doesn't\nconnect Elliot Page to a film involving a character named Tracey .\nHilary Swank in \"Boys Don't Cry\": Hilary Swank portrayed Brandon Teena in \"Boys\nDon't Cry ,\" not a character named Tracey . This film doesn't align with the criteria of\nfeaturing a character named Tracey .\nReese Witherspoon in \"Legally Blonde\" and \"Sweet Home Alabama\": Reese\nWitherspoon's characters in these films are named Elle W oods and Melanie Smooter ,\nrespectively . Neither film prominently features a character named Tracey , nor are they\nprimarily about pregnancy .\nCorrect Information:\n\"Juno\" (2007): Elliot Page plays the titular character , Juno MacGuf f, a teenager dealing\nwith an unplanned pregnancy .\n\"The Tracey Fragments\" (2007): Elliot Page portrays Tracey Berkowitz, a 15-year-old\nnavigating personal challenges.\nFinal answer:  Elliot PageAnswer: Ellen (Elliot) Pagecorrect\nanswerQuestion: Who played in the movies about\npregnancy and Tracey? Reason step by step\nvery shortly , then conclude with the answer .\nStep-by-step reasoning:Figure 5: Example of the STaSC pipeline.\ntheoretical foundation and a deeper investigation\nof baseline algorithm adaptations. Additionally, it\nis limited by the use of large proprietary models,\nwithout open-sourcing the code or model weights,\nmaking it difficult to build upon for future research.\n",
          "conclusion": "In this study, we introduced the Self-Taught Self-\nCorrection (STaSC) algorithm, which incorporates\nmultiple algorithmic choices to enable genuine\nintrinsic self-correction without relying on exter-\nnal tools or large proprietary models. Inspired by\nSTaR (Zelikman et al., 2022), our approach trains\nexclusively on self-generated data. Experiments on\na QA task with two small language models demon-\nstrate that SLMs can learn to self-correct using\ntheir own generations and even improve initial an-\nswers, despite being trained solely for corrections.\nFurthermore, our analysis highlights key algorith-mic insights, emphasizing the importance of fil-\ntering selectivity, initial answer exploration, and\nthe potential of iterative fine-tuning. To support\nfuture research, we have open-sourced our code\n",
          "references": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed\nAwadallah, Ammar Ahmad Awan, Nguyen Bach,\nAmit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat\nBehl, Alon Benhaim, Misha Bilenko, Johan Bjorck,\nS\u00e9bastien Bubeck, Martin Cai, Qin Cai, Vishrav\nChaudhary, Dong Chen, Dongdong Chen, Weizhu\nChen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng,\nParul Chopra, Xiyang Dai, Matthew Dixon, Ro-\nnen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao,\nMin Gao, Amit Garg, Allie Del Giorno, Abhishek\nGoswami, Suriya Gunasekar, Emman Haider, Jun-\nheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie\nHuynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi,\nXin Jin, Nikos Karampatziakis, Piero Kauffmann,\nMahoud Khademi, Dongwoo Kim, Young Jin Kim,\nLev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi\nLi, Yunsheng Li, Chen Liang, Lars Liden, Xihui\nLin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu,\nWeishung Liu, Xiaodong Liu, Chong Luo, Piyush\nMadan, Ali Mahmoudzadeh, David Majercak, Matt\nMazzola, Caio C\u00e9sar Teodoro Mendes, Arindam Mi-\ntra, Hardik Modi, Anh Nguyen, Brandon Norick,\nBarun Patra, Daniel Perez-Becker, Thomas Portet,\nReid Pryzant, Heyang Qin, Marko Radmilac, Liliang\nRen, Gustavo de Rosa, Corby Rosset, Sambudha Roy,\nOlatunji Ruwase, Olli Saarikivi, Amin Saied, Adil\nSalim, Michael Santacroce, Shital Shah, Ning Shang,\nHiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia\nSong, Masahiro Tanaka, Andrea Tupini, PraneethaVaddamanu, Chunyu Wang, Guanhua Wang, Lijuan\nWang, Shuohang Wang, Xin Wang, Yu Wang, Rachel\nWard, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia\nWu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu,\nWeijian Xu, Jilong Xue, Sonali Yadav, Fan Yang,\nJianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu,\nLu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen\nZhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan\nZhang, and Xiren Zhou. 2024. Phi-3 technical report:\nA highly capable language model locally on your\nphone. Preprint , arXiv:2404.14219.\nBishwamittra Ghosh, Sarah Hasan, Naheed Anjum\nArafat, and Arijit Khan. 2024. Logical consistency\nof large language models in fact-checking. Preprint ,\narXiv:2412.16100.\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong\nShen, Yujiu Yang, Nan Duan, and Weizhu Chen.\n2024. Critic: Large language models can self-\ncorrect with tool-interactive critiquing. Preprint ,\narXiv:2305.11738.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforce-\nment learning. arXiv preprint arXiv:2501.12948 .\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting\nLiu. 2025. A survey on hallucination in large lan-\nguage models: Principles, taxonomy, challenges, and\nopen questions. ACM Transactions on Information\nSystems , 43(2):1\u201355.\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\nHwang, and Jong C Park. 2024. Adaptive-rag: Learn-\ning to adapt retrieval-augmented large language mod-\nels through question complexity. arXiv preprint\narXiv:2403.14403 .\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active retrieval\naugmented generation. Preprint , arXiv:2305.06983.\nRyo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han,\nand Rui Zhang. 2024. When can llms actually cor-\nrect their own mistakes? a critical survey of self-\ncorrection of llms. Transactions of the Association\nfor Computational Linguistics , 12:1417\u20131440.\nAviral Kumar, Vincent Zhuang, Rishabh Agarwal,\nYi Su, John D Co-Reyes, Avi Singh, Kate Baumli,\nShariq Iqbal, Colton Bishop, Rebecca Roelofs,\net al. 2024. Training language models to self-\ncorrect via reinforcement learning. arXiv preprint\narXiv:2409.12917 .\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\n9\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics , 7:452\u2013466.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdan-\nbakhsh, and Peter Clark. 2023. Self-refine: It-\nerative refinement with self-feedback. Preprint ,\narXiv:2303.17651.\nViktor Moskvoretskii, Maria Lysyuk, Mikhail Sal-\nnikov, Nikolay Ivanov, Sergey Pletenev, Daria Gal-\nimzianova, Nikita Krayko, Vasily Konovalov, Irina\nNikishina, and Alexander Panchenko. 2025. Adap-\ntive retrieval without self-knowledge? bringing uncer-\ntainty back home. arXiv preprint arXiv:2501.12835 .\nLiangming Pan, Alon Albalak, Xinyi Wang, and\nWilliam Yang Wang. 2023. Logic-lm: Empower-\ning large language models with symbolic solvers\nfor faithful logical reasoning. arXiv preprint\narXiv:2305.12295 .\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-\nriz Borges, Antoine Bosselut, Robert West, and Boi\nFaltings. 2024. REFINER: Reasoning feedback on\nintermediate representations. In Proceedings of the\n18th Conference of the European Chapter of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 1100\u20131126, St. Julian\u2019s, Malta.\nAssociation for Computational Linguistics.\nQwen, :, An Yang, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang,\nJiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang,\nKeming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li,\nMingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji\nLin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang\nRen, Xuancheng Ren, Yang Fan, Yang Su, Yichang\nZhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru\nZhang, and Zihan Qiu. 2025. Qwen2.5 technical\nreport. Preprint , arXiv:2412.15115.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik Narasimhan, and Shunyu Yao. 2024. Re-\nflexion: Language agents with verbal reinforcement\nlearning. Advances in Neural Information Process-\ning Systems , 36.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\nKhot, and Ashish Sabharwal. 2022. Interleav-\ning retrieval with chain-of-thought reasoning for\nknowledge-intensive multi-step questions. arXiv\npreprint arXiv:2212.10509 .\nSean Welleck, Ximing Lu, Peter West, Faeze Brah-\nman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. 2022. Generating sequences by learning to\nself-correct. arXiv preprint arXiv:2211.00053 .Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Al-\nbalak, Anikait Singh, Chase Blagden, Duy Phung,\nRafael Rafailov, Nathan Lile, Dakota Mahan, et al.\n2025. Towards system 2 reasoning in llms: Learn-\ning how to think with meta chain-of-though. arXiv\npreprint arXiv:2501.04682 .\nWenda Xu, Danqing Wang, Liangming Pan, Zhenqiao\nSong, Markus Freitag, William Yang Wang, and Lei\nLi. 2023. Instructscore: Explainable text generation\nevaluation with finegrained feedback. arXiv preprint\narXiv:2305.14282 .\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\nGoodman. 2022. Star: Bootstrapping reasoning with\nreasoning. Preprint , arXiv:2203.14465.\nYunxiang Zhang, Muhammad Khalifa, Lajanugen Lo-\ngeswaran, Jaekyeom Kim, Moontae Lee, Honglak\nLee, and Lu Wang. 2024. Small language models\nneed strong verifiers to self-correct reasoning. arXiv\npreprint arXiv:2404.17140 .\nA Technical Details\nWe apply a weight decay of 0.1 and use the Adam\noptimizer with default betas, along with a cosine\nannealing scheduler. Training is performed using\nFully Sharded Data Parallel on two A100 GPUs,\nwith an estimated total compute of 80 GPU hours.\nThe dataset and Qwen come under Apache License,\nPhi3-mini under MIT. We are open-sourcing all our\ncode under the Apache License.\nInitial answer sampling prompt\nYou are a helpful reasoning assistant\nin general domain question answering.\nPlease reason through the question step\nby step very shortly before giving a final\nanswer.\nGenerate a short chain-of-thought ratio-\nnale very shortly, and then provide the\nfinal answer.\nStep-by-step reasoning:\nFinal Answer:\nQuestion: {question text}\nReason step by step very shortly, then\nconclude with the answer.\nCorrection sampling prompt\nYou are a helpful reasoning assistant in\ngeneral domain question answering.Your\ntask is to correct the initial response if it\nis incorrect.\n10\nBelow is the question and the initial an-\nswer. Generate a correction to the initial\nanswer if it is incorrect. Disregard the\ninformation you already have, look for\nother options. Do not use the information\nthat does not match your criteria.\nStep-by-step reasoning:\nFinal Answer:\nQuestion: Which Portuguese soccer\nplayer has the most goals?\nStep-by-step reasoning: Christiano\nRonaldo has scored 900 goals in a ca-\nreer spanning 22 years. I cannot think of\nanyone else scoring more than him.\nFinal Answer: Christiano Ronaldo\nStep-by-step reasoning: Yes, Christiano\nRonaldo is the correct answer. Other de-\ntails are unimportant.\nFinal Answer: Christiano Ronaldo\nQuestion: Which actress played in both\nmovies: one about preg-cy and the other\nabout Tracey?\nStep-by-step reasoning: 1. Identify\nmovies about preg-cy and about Tracey.\n2. Movies about preg-cy include \u2019Juno\u2019\nand \u2019Knocked Up\u2019. 3. Movie about\nTracey is likely \u2019Tracy Chapman\u2019 or a\ndocumentary, but \u2019Tracy\u2019 could be a ref-\nerence to \u2019Tracy Flick\u2019 in the movie\n\u2019Election\u2019. 4. Considering well-known\nmovies with these themes, actress Reese\nWitherspoon comes to mind as a possible\nmatch.\nFinal Answer: Reese Witherspoon\nStep-by-step reasoning: Let\u2019s break\ndown the answer. \u2019Juno\u2019 is indeed a\nmovie about preg-cy, but it starred Ellen\nPage (now Elliot Page), not Reese With-\nerspoon. Reese Witherspoon did not star\nin \u2019Juno.\u2019 Therefore, the association be-\ntween Reese Witherspoon and \u2019Juno\u2019 is\nincorrect. Movies About Tracey is likely\n\u2019Tracey Fragments (2007)\u2019, a movie in\nwhich Ellen Page also starred as the lead\ncharacter, Tracey Berkowitz. Ellen Page\n(Elliot Page) connects both movies.\nFinal answer: Ellen Page (now Elliot\nPage)Question: {question text}\nInitial Answer: {initial answer}\nWrite a correction if the initial answer is\nincorrect.\n11\n"
        },
        "paper_title": "Self-Taught Self-Correction for Small Language Models",
        "publication": "arXiv"
      }
    },
    {
      "content_id": 8,
      "title": "mistral-inference",
      "url": "https://github.com/mistralai/mistral-inference",
      "source_type": "github",
      "metadata": {
        "stars": 10077,
        "language": "Jupyter Notebook",
        "topics": [
          "llm",
          "llm-inference",
          "mistralai"
        ],
        "forks": 901,
        "repo_name": "mistral-inference",
        "full_name": "mistralai/mistral-inference"
      }
    },
    {
      "content_id": 22,
      "title": "GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D\n  Garment Reconstruction and Editing",
      "url": "http://arxiv.org/abs/2503.08678v1",
      "source_type": "research_paper",
      "metadata": {
        "authors": "Yuanhao Wang, Cheng Zhang, Gon\u00e7alo Fraz\u00e3o, Jinlong Yang, Alexandru-Eugen Ichim, Thabo Beeler, Fernando De la Torre",
        "published_date": "2025-03-11T17:56:03",
        "year": 2025,
        "sections": {
          "abstract": "GarmentCrafter: Progressive Novel View Synthesis for Single-View\n3D Garment Reconstruction and Editing\nYuanhao Wang1Cheng Zhang2Gonc \u00b8alo Fraz \u02dcao1Jinlong Yang3Alexandru-Eugen Ichim3\nThabo Beeler3Fernando De la Torre1\n1Carnegie Mellon University2Texas A&M University3Google AR\nhumansensinglab.github.io/garment-crafter\nInput Image\nNovel View\nRecon. MeshZoom-InRecon. Mesh\nShorten the sleeves,change the dress style\n2D EditingNovel ViewRecon. MeshSingle-View EditingSingle-View Reconstruction\nFigure 1. From a real-world clothing image, GarmentCrafter synthesizes high-quality novel views, enabling the reconstruction of garment\nmeshes with accurate geometry and rich detail. Additionally, users can easily apply 2D edits (e.g., modifying parts or surface details) using\noff-the-shelf tools on a single image, and GarmentCrafter seamlessly applies these edits across the 3D model with multi-view consistency.\nWe introduce GarmentCrafter, a new approach that enables\nnon-professional users to create and modify 3D garments\nfrom a single-view image. While recent advances in im-\nage generation have facilitated 2D garment design, creat-\ning and editing 3D garments remains challenging for non-\nprofessional users. Existing methods for single-view 3D\nreconstruction often rely on pre-trained generative models\nto synthesize novel views conditioning on the reference im-\nage and camera pose, yet they lack cross-view consistency,\nfailing to capture the internal relationships across differ-ent views. In this paper, we tackle this challenge through\nprogressive depth prediction and image warping to approxi-\nmate novel views. Subsequently, we train a multi-view diffu-\nsion model to complete occluded and unknown clothing re-\ngions, informed by the evolving camera pose. By jointly in-\nferring RGB and depth, GarmentCrafter enforces inter-view\ncoherence and reconstructs precise geometries and fine de-\ntails. Extensive experiments demonstrate that our method\nachieves superior visual fidelity and inter-view coherence\ncompared to state-of-the-art single-view 3D garment recon-\n",
          "introduction": "Professional fashion designers use sophisticated software to\ncreate and edit garments in 3D, crafting highly detailed vir-\ntual apparels [1\u20134]. However, as digital garments become\nintegral to virtual environments and personalized digital ex-\nperiences [11, 21, 27, 54, 73], there is a growing demand\nfor intuitive tools that allow non-professional users to de-\nsign and interact with 3D garments. For broader accessibil-\nity, such tools should allow users to work with 3D garments\nwith minimal input, ideally from just a single image. This\nraises a key question: How can we create and edit 3D gar-\nments with simple manipulations in an image?\nRecent advancements in image generation models [51,\n53, 55, 66] and image editing techniques [9, 48, 50, 67,\n84, 87] have enabled high-quality garment design in 2D.\nYet, achieving the same level of control and realism for\n3D garments remains challenging for common users. Cur-\nrently, state-of-the-art methods on single-view 3D garments\nrely either on 1) deforming, matching, and registration with\nthe human body prior [43] and/or predefined garment tem-\nplates [7, 16, 20, 37, 39, 45, 57], or 2) novel view synthesis\ntechniques [41, 70] that use pre-trained 2D diffusion models\nconditioned on a reference image and target pose. However,\nthey often fall short in capturing accurate, realistic geome-\ntry and appearance.\nTwo characteristics of garments pose challenges. First,\ngarments exhibit diverse shapes, complex geometries, and\nrich textures, making template-based methods limited in\ntheir ability to generalize across clothing styles. Most ex-\nisting methods prioritize either geometry [16, 44] or tex-\nture [52, 80], rarely balancing both [20, 45, 57]. Second, the\nfine details in garments demand stronger multi-view consis-\ntency. Existing novel view synthesis methods [42, 74], con-\nditioned on a reference image and target pose, often neglect\ncritical semantic connections across different views.\nHow can we ensure that a pixel in one view corresponds\nto a point visible in another, with consistent appearance?\nIn this paper, we propose a different approach, progressive\nnovel view synthesis , to enhance cross-view coherence. Our\nmethod begins by estimating the depth of the input image\nand warping projected points to approximate unseen views.\nWe then apply a multi-view diffusion model to complete\nmissing and occluded regions based on the evolving camera\npose. Furthermore, we incorporate a monocular depth esti-\nmation model to generate depth maps that remain consistent\nwith the warped depths. Unlike existing novel view synthe-\nsis, our key insight is to use the depth-based warped image\nas an additional condition to guide cross-view alignment.\nBy progressively synthesizing views and depths along a pre-\ndefined camera trajectory, our method gradually refines the\ngeometry and texture of the garment across viewpoints.\nWe name our method GarmentCrafter , a novel solu-\ntion for 3D garment creation and editing while users justneed to operate on a single-view image, as shown in Fig-\nure 1. Specifically, GarmentCrafter not only generates high-\nquality 3D garments but also extends garment editing from\n2D to 3D. Thanks to our progressive novel view synthesis,\nusers can make local edits (e.g., editing surface details) or\nperform part-based manipulations (e.g., modifying garment\nparts) directly on a single-view image, with precise effects\nreflected in 3D space \u2014 capabilities that are absent in the\nexisting methods [57]. Trained on large-scale 3D garment\ndatasets [8, 18, 88], GarmentCrafter demonstrates superior\nperformance on held-out 3D garment data as well as in-the-\nwild clothing images. Extensive experiments show that our\nmethod outperforms state-of-the-art 2D-to-3D garment re-\nconstruction approaches in terms of geometric accuracy, vi-\nsual fidelity, and cross-view consistency.\n2. Related Work\nSingle-View 3D Garment Reconstruction and Editing.\nReconstructing 3D garments from a single image has been\nwidely explored, with existing methods approaching the\ntask from several perspectives. One line of work relies\non parametric body templates, such as SMPL [7, 16, 29,\n47], or employs 2D shape priors and keypoint-based tech-\nniques [83] to optimize garment structure. Another cate-\ngory of work uses explicit or implicit 3D parametric gar-\nment models [7, 17, 20, 37, 44, 45, 57, 86] to capture gar-\nment shape and support pose-guided deformations. Ad-\nditionally, some methods incorporate garment sewing pat-\nterns [6, 12, 14, 28, 39, 76, 88], offering flexibility by recon-\nstructing garments from 2D panels. However, these works\noften struggle to capture diverse garment styles and fine\nsurface details (e.g., wrinkles), and lack support for intu-\nitive garment manipulation, such as modifying surface de-\ntails or garment parts. In contrast, GarmentCrafter prior-\nitizes novel view synthesis for detailed geometry and tex-\nture reconstruction, without relying on garment templates\nor human body priors, allowing it to handle a wide range of\ngarment styles. Furthermore, single-view edits can also be\nseamlessly extended to the 3D model. Note that, our focus\nin this paper is on garments in a rest pose \u2014 well suited to\nthe fashion industry, where ease of adjustment is essential.\nNovel View Synthesis from Sparse Images. Our method\nis inspired by novel view synthesis. Popular approaches\nsuch as Neural Radiance Fields (NeRFs) [46] and 3D\nGaussian Splatting (3D-GS) [32] rely on numerous posed\ninputs, limiting their use in single-view scenarios. Re-\ncently, distillation from pre-trained 2D generative models\nhas emerged as a promising solution for hallucinating novel\nviews from limited input, with applications in human dig-\nitization [5, 22, 23, 34, 56, 71, 72, 82] and object-centric\nreconstruction [26, 26, 40\u201342, 49, 59, 61, 70, 85]. How-\never, these methods often lack cross-view consistency and\n2\nImage Completion\nDepth Completion\nIncomplete Depth\nIncomplete RGBProjectProjectProject\nProjectPoint Cloud Merge+Camera Rotate\nPoint Cloud\nPoint CloudPoint Cloud\nCompleted RGB\nCompleted Depth\nFigure 2. An illustration of progressive novel view synthesis in GarmentCrafter. Left: Given a garment image, our method performs\ndepth-aware novel view synthesis along a predefined zigzag camera trajectory. Right: For each camera rotation from \u03c0i\u22121to\u03c0i, we project\nthe current point cloud Pi\u22121into the image space based on camera pose \u03c0i, resulting in incomplete RGB and depth images. Our diffusion\nmodel completes the RGB image using the warped view, input image, and camera pose as conditions, while a depth completion network\nrefines the depth map based on the completed RGB, warped depth, and camera pose. The re-projected point cloud P\u2032\niis then merged with\nPi\u22121to produce an updated point cloud Pi. This iterative process continues until a full 3D representation of the garment is achieved.\nhigh-quality details, crucial for garment-focused tasks. Un-\nlike models that sample views independently, our method\ntakes semantic cues (i.e., wrapped images) from other views\nas an additional condition for view synthesis. This might\nbe reminiscent of scene-level approaches, such as Perpetual\nView Synthesis [10, 15, 30, 38, 63, 79], which condition on\nwarped images for neighbor view image completion. How-\never, we note that scene-centric methods often lack the pre-\ncision needed for object-centric cases (e.g., garment manip-\nulation) and overlook loop closure for garment shape com-\npletion. Our work represents a novel attempt of progressive\nview synthesis with a predefined camera trajectory for gar-\nment reconstruction and editing.\nImage-to-3D Reconstruction. Our approach builds on re-\ncent advancements in image-to-3D reconstruction, where\nmost methods distill pre-trained generative models via per-\nscene optimization [13, 35, 49, 60, 65] or multi-view diffu-\nsion techniques [26, 40\u201342, 58, 64, 85]. With the availabil-\nity of large-scale 3D datasets [18, 19], Large Reconstruction\nModels (LRMs) [24, 36, 62, 74, 75] are being trained for\nfeed-forward image-to-3D generation. Unlike Zero-1-to-3\nand its variants [41], our method leverages diffusion models\nto progressively condition on warped images with carefully\ndesigned camera trajectory and error reduction methods to\nenhance cross-view consistency. Additionally, we curated a\n3D garment dataset, incorporating assets from existing 3D\ncollections [8, 18, 88], allowing our model to synthesize\nhighly detailed, multi-view images and corresponding depth\nmaps. This process yields multi-view images alongside ac-\ncurate depth maps, enabling high-quality mesh reconstruc-\ntion through standard point cloud-to-mesh methods [31].\nWhile we demonstrate point aggregation and mesh recon-\nstruction in our work, our primary focus is on advancing\nthe multi-view and depth synthesis stages rather than opti-\nmizing the point-to-mesh conversion process itself.3. Approach\nWe first present problem statement in Section 3.1, followed\nby our proposed progressive novel view synthesis in Sec-\ntion 3.2. We introduce garment-centric applications enabled\nby our method in Section 3.3. We describe the details of\ndata curation and model training methods in Section 3.4.\n3.1. Problem Definition\nGiven a single-view garment image I0, our goal is to gener-\nate consistent novel views with detailed RGB textures and\naccurate depths, which support both single-view 3D recon-\nstruction and editing. Specifically, we first estimate a depth\nmapD0based on the input I0. Then, we project every pixel\nin the foreground of the garment to the world space, creat-\ning a colored point cloud P0. Our goal is to complete this\npoint cloud by sequentially incorporating information from\nsynthesized novel views. To achieve this, we propose an\nprogressive 3D completion process with a predefined cam-\nera trajectory \u03c0={\u03c01, \u03c02, ..., \u03c0 N}that forms a closed loop\naround the garment object. Figure 2 illustrates the overall\nframework. Next, we elaborate the details of an arbitrary\nstep in the following sections.\n3.2. Progressive Novel View Synthesis\nOverview. At the step iof the progressive novel view syn-\nthesis (see Figure 2), we first project the existing point cloud\nPi\u22121to the image plane of camera \u03c0i\u2208\u03c0, producing an\nincomplete image I\u2032\niand an incomplete depth map D\u2032\ni. We\nthen apply an image completion model to inpaint the miss-\ning areas in I\u2032\ni, resulting in Ii. Next, we use an monocular\ndepth estimation model to estimate the corresponding depth\nmapDiconsistent with the known depths in D\u2032\ni. Finally, we\nintegrate IiandDiwith the existing point cloud to obtain\na merged Pi. By following a predefined camera trajectory,\n3\nour method can generate view-dependent images and cor-\nresponding depths that enable high-quality garment recon-\nstruction and edit with improved cross-view consistency.\nConditional Image Generation. At step i, the goal is to\nsynthesize Ii\u2208RH\u00d7W\u00d73, the image of the garment ob-\nject from the viewpoint of camera \u03c0i, given the input image\nI0, the projected image I\u2032\ni, and the relative camera rotation\nRi\u2208R3\u00d73and translation Ti\u2208R3from \u03c00to\u03c0i. We aim\nto train a model fimgsuch that:\nIi=fimg(I0, I\u2032\ni, Ri, Ti), (1)\nwhere Iiis the synthesized complete image that retains the\nappearance of I\u2032\niin the known regions, and synthesizes\nplausible appearance in the unknown regions that remain\nperceptually consistent with I\u2032\niand the original input I0.\nTo learn fimg, we fine-tune a denoising diffusion model,\nleveraging its strong generalization capabilities in image\ngeneration. Specifically, we adopt a latent diffusion archi-\ntecture based on Stable Diffusion [53] with an image en-\ncoderE, a denoising network \u03f5\u03b8, and a decoder D. At de-\nnoising step s\u2208S, letzsdenote the noisy latent of the\ntarget image x=Ii, and let c=c(I0, I\u2032\ni, Ri, Ti)be the\nembedding of the anchor view image, target view projected\nimage, and relative camera extrinsics. We optimize the fol-\nlowing latent diffusion objective:\nL(\u03b8) =EE(I0),E(I\u2032\ni),\u03f5\u223cN (0,I),sh\n\u2225\u03f5\u2212\u03f5\u03b8(zs, s,c)\u22252i\n.(2)\nUnlike existing multi-view diffusion models (e.g., [41,\n58]), which synthesize novels views from an arbitrary in-\nput viewpoint, we unify our garment-centric task by fixing\nthe input image to a near-frontal view of the garment. This\nallows RiandTito be interpreted as the absolute camera\ntransformation from the frontal view. Furthermore, in addi-\ntion to conditioning on the anchor view image, we incorpo-\nrate the warped image (i.e., I\u2032\niin Figure 2 and Equation 1) at\nthe target view as an additional condition input, which pro-\nvides a strong prior that enhances cross-view consistency in\ngarment reconstruction, as demonstrated in Section 4.4.\nConditional Depth Generation. After obtained complete\nRGB image Ii, we learn a depth model fdepth to estimate\nthe depth map Di\u2208RH\u00d7W\u00d71conditioned on the warped\nincomplete depth map D\u2032\nias follows:\nDi=fdepth(Ii, D\u2032\ni) (3)\nSimilar to the conditional image generation, we enforce\ndepth preservation in known regions by framing the task\nas metric depth estimation. To ensure consistency, we align\nthe depth values of DiandD\u2032\n",
          "methodology": "1arXiv:2503.08678v1  [cs.GR]  11 Mar 2025\nis optimized using an L1loss:\nL1=||(Di\u2212\u02c6Di)\u00b7m||, (4)where \u02c6Diis the ground-truth depth, and mis the foreground\nmask. To train fdepth, we fine-tune the pretrained human\nfoundation model, Sapiens [33], leveraging its strong priors\nfor human-related tasks. To condition the model on D\u2032\ni, we\nconcatenate D\u2032\niwithIias input and add an extra channel to\nthe first projection layer of Sapiens model. The weights of\nthe added channel are initialized to zero.\nPoint Cloud Merging and Projection. To integrate novel\nview observations (i.e., IiandDi) into the existing point\ncloud Pi\u22121, we first identify the inpainted regions from the\nimage model. Pixels in these regions are projected into\nworld space and merged with Pi\u22121to form Pi, with ex-\npanded borders to include overlapping regions. To mini-\nmize stitching artifacts, we align the depth map of the in-\npainted regions with the warped depth map of Pi\u22121. When\nprojecting a partial point cloud to a novel view, only sur-\nfaces facing the camera should be rendered. To enforce this,\nwe track the orientation of each point. For a point xadded at\nstepi, its orientation vector vis derived from the normal di-\nrection of the corresponding pixel in Di. During projection,\na point is ignored if dot (v, v 0)<0, where v0is the view-\ning direction. After completing all steps along the camera\ntrajectory, we optionally sample a few random views for ad-\nditional inpainting to recover any occluded regions. Please\nsee supplementary for additional details.\n3.3. Garment Digitization and Editing\nGarment Digitization. Our method enables garment digi-\ntization from a single image by progressively synthesizing\nnovel views, generating multi-view consistent RGBD im-\nages and a colored point cloud. This output serves as an in-\ntermediate representation for various 3D reconstruction. In\nthis work, we employ Screened Poisson surface reconstruc-\ntion [31] to convert the point cloud into a textured mesh.\nSpecifically, we project multi-view RGBD images to form\na colored point cloud, where each point encodes geometry\nand color. The Screened Poisson method then interpolates\nthese attributes, mapping textures onto mesh vertices.\nInteractive Editing. Redesigning a 3D garment model typ-\nically requires significant expertise, making it impractical\nfor most users. GarmentCrafter provides an intuitive alter-\nnative, allowing users to edit a rendered image of the gar-\nment from a selected view, which is then lifted into 3D. In\nthis work, we focus on two types of edits: (1) Part-based\nEditing : Modifies the geometry or texture of specific gar-\nment parts, such as sleeves or pant legs. Users can add,\nremove, or resize components. (2) Local surface editing :\nAdjusts the geometry and texture of localized regions, such\nas adding a pocket or modifying the neckline design.\nThe garment part editing is achieved with the following\nstrategy. Given a 3D garment object G, the user selects an\nanchor view \u03c0and edits the rendered image Ito obtain Iedit.\n4\nWe first identify the edited region in Ieditand remove the\ncorresponding garment parts from G, leaving a partial gar-\nment G\u2032that remains unchanged. This reformulates the task\nas single-view 3D garment part reconstruction, conditioned\nonG\u2032. We then follow the process described in Section 3.2\nwith two modifications: (1) At each step along the camera\ntrajectory, the conditional image and depth are generated\nby combining the projected point cloud with observations\nfrom the partial garment G\u2032. (2) After computing image\nand depth maps, only pixels within the edited region are\nprojected and merged with the existing point cloud. The fi-\nnal output is a colored point cloud of the edited parts, which\nis then merged with G\u2032. For local surface editing, instead of\nremoving and reconstructing an entire garment part, we ap-\nply the same process to a localized surface region.\n3.4. Data Preparation and Training\nWe construct the training dataset by simulating inference.\nFor each 3D garment, we sample 6uniform views at 0\u25e6el-\nevation (following the full camera trajectory) and 4addi-\ntional random views between 60\u25e6and\u221230\u25e6for inpainting.\nTraining Data for Reconstruction. We follow the zigzag\ncamera trajectory (Figure 2) and at each step i, we\nform a training pair for the image generation model fimg:\n{(I\u2032\ni, I0, Ri, Ti), Ii}, where I\u2032\niis the projected image, I0is\nthe anchor view, and (Ri, Ti)are the relative camera trans-\nformations. Similarly, the depth generation model fdepth\nis trained with {(D\u2032\ni, Ii), Di}, where D\u2032\niis the projected\ndepth, and Diis the ground-truth depth. We merge the point\ncloud with IiandDibefore proceeding. Finally, we repeat\nthe process for four random views to simulate inpainting.\nTraining Data for Editing. For 3D editing, we generate\ntraining data by randomly removing parts of a 3D garment\nto create a partial known model. At each step, we create\na partial image I\u2032\u2032\niand depth map D\u2032\u2032\niby merging I\u2032\niand\nD\u2032\niwith known observations. The training pairs become\n{(I\u2032\u2032\ni, I0, Ri, Ti), Ii}forfimgand{(D\u2032\u2032\ni, Ii), Di}forfdepth.\nJoint Training. To learn a unified model for both recon-\nstruction and editing, we combine their training data. We\nrandomly apply small rotations to the 3D object when gen-\nerating the training data, enabling the model to handle in-\nthe-wild inputs that may not be well-posed. Please refer to\nthe supplementary materials for details.\nCVCS = 1\u2212\u03a3|I\u2212I\u2032| \u00b7m\u2032\n\u03a3m\u2032(5)\nwhere Iis the synthesized image at camera view \u03c0,I\u2032is\na partial image projected from an observed view \u03c00with\nknown depth, and m\u2032is a binary mask indicating the pro-\njection regions. This assumes \u03c0and\u03c00are relatively close.\nWe use the CVCS metric to ablate the impact of P-NVS.\nAs shown in Table 2, GarmentCrafter achieves superior\ncross-view consistency with P-NVS. We further validates\nthis claim with a visual example in Figure 5. While both\nmodel synthesizes plausible novel views, GarmentCrafter\nwith P-NVS aligns more closely with the input observation.\nEffect of Trajectory on Loop Closure. For better loop clo-\nsure, we use a \u201czigzag\u201d camera trajectory where we rotate\nthe camera to left and right alternatively and converge at\nthe center back of the garment (see Figure 2). This design\naims to better capture overlapping views, thereby improv-\ning reconstruction accuracy. We validate this design choice\nby comparing the quality of the 3D meshes generated us-\ning zigzag and sequential trajectories. We report quantita-\ntive results in Table 3. We find that our chosen trajectory\nachieves better performance across both image and geome-\ntry metrics. We additionally show a qualitative comparison\nin Figure 7. When using a circular trajectory, achieving loop\nclosure from the side view is challenging; the generated ge-\nometry (left sleeve) often conflicts with prior predictions,\n7\nAnchor View\nNovel Views\nOriginal 3DEdit AEdit BRemove the apron Stretch pants\nChange the skirt to pantsAdd a U-shape neckline\nAnchor ViewNovel ViewsFigure 6. Qualitative results on single-view 3D garment editing. GarmentCrafter enables single-view edit such as modify the geometry\nand surface details of the garment, with the changes accurately reflected across the 3D model. Please see supplementary for more results.\nTable 3. Ablation study on camera trajectory selection. We\nstudy two types camera trajectory for progressive novel view syn-\nthesis. Circular : the camera moves around the object in regular\nsteps, either clockwise or counterclockwise. Zigzag : the camera\nalternates directions with each step, as shown in Figure 2. Results\nindicate that our proposed zigzag achieves better appearance and\ngeometry quality compared to using circular trajectory. We show\nan actual example in Figure 7 for qualitative analyses.\nTrajectory LPIPS \u2193PSNR \u2191SSIM \u2191Chamfer \u2193\nCircular 0.1503 20.79 0.8130 0.0054\nZigzag (ours) 0.1454 21.22 0.8173 0.0044\n",
          "results": "We present experimental results of our method on single-\nview garment reconstruction and editing. Please see sup-\nplementary for additional details, analyses, and results.\n4.1. Datasets, Metrics, and Baselines\nDatasets. We validate GarmentCrafter using 3D garment\nassets from a number of sources. (1) Curated dataset: Wecollect \u223c700 3D garments with diverse shape and texture\nfrom Artstation1. (2) Objaverse 1.0 (Garment) [18]: the\noriginal v1.0 dataset contains more than 800K 3D objects,\nwhere most of the existing method trained on [41, 74, 77].\nWe manually curated a subset only contain \u223c900 high-\nquality garment assets. (3) BEDLAM [8]: 114 garments,\neach has many textures, \u223c1600 garments in total. (4)\nCloth4D [88]: \u223c1100 artists made garments.\nQuantitative Metrics. (1) Texture and appearance qual-\nity: we evaluate the novel view synthesis using commonly\nused LPIPS [81], PSNR [25], SSIM [68]. (2) Geometry\nquality: we measure the performance using geometric er-\nrors with Chamfer distance (bi-directional point-to-mesh)\nbetween ground-truth and reconstructed meshes.\nBaselines. We compare GarmentCrafter with state-of-the-\nart models for image-to-3D object and image-to-garment\nreconstruction. (1) InstantMesh [74]: object reconstruction\nby generating novel views using Zero-1-to-3++ [58]. (2)\nCRM [69]: generate six orthographic views for 3D object\nreconstruction. (3) Hunyuan3D-1.0 [77]: a newly released\nmodel for high-quality image-to-3D object reconstruction.\n(4) Garment3DGen [57]: a state-of-the-art garment-specific\nmodel based on template optimization, with templates ini-\ntialized by InstantMesh [74]. As the texture code is not re-\nleased, we compare only mesh geometry.\n4.2. Results on Single-View Reconstruction\nWe evaluate GarmentCrafter on single-view reconstruction\nusing a held-out test dataset of 150 garment assets. For each\ntest case, we sample 12 views with alternating elevations of\n0\u25e6and 20\u25e6and azimuth angles evenly spaced over 360\u25e6. To\nassess image quality, we convert the generated point clouds\nto meshes using a classical surface reconstruction method\nand render multi-view images. For geometry evaluation, we\ncompute the Chamfer distance directly between the gener-\nated point cloud and the ground-truth mesh.\nQualitative Results. Figure 3 shows qualitative compar-\nisons, where GarmentCrafter demonstrates superior texture\nand geometry generation compared to all other baselines.\nOur method, benefiting from consistent multi-view gener-\nation, produces sharp textures and intricate geometric de-\ntails, whereas other baselines often result in blurry textures\nand overly smoothed geometries. Figure 4 shows additional\nqualitative results of GarmentCrafter.\nQuantitative Results on Texture Quality. We conduct a\nquantitative analysis of texture quality on our held-out test\ndataset and show results in Table 1. Across all image qual-\nity metrics, GarmentCrafter consistently surpasses baseline\nmethods, demonstrating its effectiveness in producing high-\nfidelity textures and preserving fine-grained details.\n1https://www.artstation.com/\n5\nInput Image GarmentCrafter\n(ours) InstantMesh Hunyuan3D-1.0\nCRM\nFigure 3. Qualitative comparison on single-view 3D garment reconstruction with state-of-the-art methods. Our method demonstrates\nbetter performance in handling complex texture patterns and geometric structures compared to InstantMesh [74], Hunyuan3D-1.0 [78], and\nConvolutional Reconstruction Model (CRM) [69].\nInput Recon. Mesh Input Recon. Mesh Novel Views Novel Views\nFigure 4. More qualitative results of GarmentCrafter on single-view reconstruction. Please see supplementary for more results.\n6\nTable 1. Quantitative comparison of texture and geometry\nquality. InstantMesh \u22c6: with fine-tuned Zero-1-to-3++ on our gar-\nment data for a fair comparison. CRM and Hunyuan3D-1.0 require\nsignificant computing for full fine-tuning, making it impractical.\nGarment3DGen does not provide texture reconstruction code.\nAppearance Geometry\nLPIPS \u2193PSNR \u2191SSIM \u2191Chamfer \u2193\nInstantMesh \u22c6[74] 0.1848 19.14 0.7944 0.0139\nCRM [69] 0.2213 17.51 0.8131 0.0127\nHunyuan3D-1.0 [77] 0.2216 17.77 0.7794 0.0121\nGarment3DGen [57] \u2013 \u2013 \u2013 0.0123\nGarmentCrafter 0.1190 22.36 0.8317 0.0044\nTable 2. Ablation study on Progressive Novel View Synthesis (P-\nNVS) and analysis on multi-view consistency. We show results\nwith and without P-NVS. CVCS: Cross-View Consistency Score.\nP-NVS LPIPS \u2193 PSNR \u2191 SSIM \u2191 CVCS \u2191\n\u2717 0.1195 21.512 0.8369 0.9030\n\u2713 0.1052 22.776 0.8557 0.9512\nQuantitative Results on Geometry Quality. We present\nquantitative geometry evaluation results in Table 1. Gar-\nmentCrafter outperforms baseline methods in terms of\nChamfer distance, highlighting its enhanced ability to cap-\nture detailed surface geometries in 3D garment shapes.\n4.3. Results on Single-View Editing\nWe present qualitative results on single-view editing in Fig-\nure 6, showcasing various types of edits, including resiz-\ning, element swapping, and surface editing. GarmentCrafter\nsuccessfully applies 3D edits that are consistent with the 2D\nedits, while preserving cross-view consistency.\n4.4. Analyses and Ablation Studies\nImportance of Progressive Novel View Synthesis. A key\ninsight of our method is to progressively synthesize novel\nview by conditioning the generation on the projected im-\nages. We conduct an ablation study on the effect of pro-\njected image conditioning. For each test case, we select an\nanchor view \u03c01, and a second camera view, \u03c02, at a 60\u00b0 az-\nimuthal angle relative to \u03c01. We compare the performance\nof our image model with or without projected image con-\nditioning at synthesizing view \u03c02in Table 2. We observe a\ndrop in performance measured in image similarity metrics\nwhen removing the projected condition.\nAnalysis on multi-view consistency. Common image met-\nrics (e.g., LPIPS, PSNR, and SSIM) measure similarity but\ndo not directly reflect cross-view consistency. To address\nthis, we propose a new metric, the Cross-View Consistency\nProjected RGBCompleted RGB(w/ P-NVS)Difference Map(w/ P-NVS)\nInput RGBCompleted RGB(w/o P-NVS)Difference Map(w/o P-NVS)Figure 5. Analysis of projected image conditioning. Left: we\nshow original input and projected RGB images. Middle: com-\npleted RGB images with and without Progressive Novel View Syn-\nthesis (P-NVS). Right: difference between completed and pro-\njected images, showing our novel view aligns more closely with\nthe ground-truth projected RGB. Zoom-in for details.\nScore (CVCS), to gain deeper insights into the consistency\nReconstruction. Please see more results in Figure S12.\nEditing. We provide more qualitative results in Figure S13.\n16\nInput RGB Recon. Mesh Novel Views\nFigure S12. More qualitative result on single-view 3D garment reconstruction.\n17\nOriginal 3D Edited 2D Recon. Mesh Novel Views Figure S13. More results on single-view 3D garment editing. The top row illustrates how GarmentCrafter effectively handles surface\nedits, even for regions with complex textures. The middle row demonstrates the capability of GarmentCrafter to support full garment\nchanges and swaps, showcasing the potential in virtual try-on scenarios. The bottom row presents an example of removing an entire\ngarment part.\nMen\u2019s slim fit knitted sweater vest; isolated in white backgroundMen\u2019s brown quilted puffer jacket, created for Macy'sYiumehoodie for men casual classic basic hoodie, dark greenPromptGenerated ImageNovel Views and Reconstructed Meshes\nFigure S14. Compatibility with generative apparel. By reconstructing both geometry and texture from synthetic garment images,\nGarmentCrafter demonstrates its adaptability to AI-generated designs. The results showcase the ability of GarmentCrafter to handle\ndiverse and complex inputs, expanding its potential applications to generative fashion and virtual apparel workflows.\n18\n",
          "conclusion": "We present GarmentCrafter, a new approach to reconstruct\nand edit 3D garments from a single input image. Our\nmethod synthesizes novel view images progressively to en-\nsure cross-view consistency, thereby achieving high quality\ngeometry and texture results. We have conducted exten-\nsive experiments to demonstrate the superior performance\nof GarmentCrafter with other baseline methods. Please see\nInput RGB Zigzag (ours) Circular Figure 7. Camera trajectory selection for loop closure. Zigzag\nachieves better loop closure, while the circular trajectory struggles\nwith side-view closure, leading to geometric conflicts and model\nfailure. We argue that there are numerous ways to select camera\ntrajectories, our proposed approach just offers an intuitive solution\ntailored for single-view garment reconstruction and editing.\nsupplementary materials for additional implementation and\ntraining details, more qualitative results on garment recon-\nstruction and editing, as well as an ablation study on the\nrotation angles in the camera trajectory.\nLimitation and future works. We focus on garments in a\nrest pose and cannot handle arbitrary poses. In addition, our\nmodel reconstructs only the external surface, not inner lay-\ners or structures. These will be addressed in future work.\n8\n",
          "references": "[1] CLO3D. https://www.clo3d.com/en/ . 2\n[2] Style3D. https://www.linctex.com/ .\n[3] TUKA3D. https://tukatech.com/tuka3d/ .\n[4] Browzwear. https://browzwear.com/ . 2\n[5] Thiemo Alldieck, Mihai Zanfir, and Cristian Sminchisescu.\nPhotorealistic monocular 3d reconstruction of humans wear-\ning clothing. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 1506\u2013\n1515, 2022. 2, 13\n[6] Floraine Berthouzoz, Akash Garg, Danny M Kaufman, Eitan\nGrinspun, and Maneesh Agrawala. Parsing sewing patterns\ninto 3d garments. Acm Transactions on Graphics (TOG) , 32\n(4):1\u201312, 2013. 2\n[7] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt,\nand Gerard Pons-Moll. Multi-garment net: Learning to dress\n3d people from images. In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 5420\u2013\n5430, 2019. 2\n[8] Michael J Black, Priyanka Patel, Joachim Tesch, and Jin-\nlong Yang. Bedlam: A synthetic dataset of bodies exhibit-\ning detailed lifelike animated motion. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 8726\u20138737, 2023. 2, 3, 5\n[9] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 18392\u201318402, 2023.\n2\n[10] Shengqu Cai, Eric Ryan Chan, Songyou Peng, Mohamad\nShahbazi, Anton Obukhov, Luc Van Gool, and Gordon\nWetzstein. Diffdreamer: Towards consistent unsupervised\nsingle-view scene extrapolation with conditional diffusion\nmodels. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision , pages 2139\u20132150, 2023. 3\n[11] Andr \u00b4es Casado-Elvira, Marc Comino Trinidad, and Dan\nCasas. Pergamo: Personalized 3d garments from monocular\nvideo. In Computer Graphics Forum , pages 293\u2013304. Wiley\nOnline Library, 2022. 2\n[12] Cheng-Hsiu Chen, Jheng-Wei Su, Min-Chun Hu, Chih-Yuan\nYao, and Hung-Kuo Chu. Panelformer: Sewing pattern re-\nconstruction from 2d garment images. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision , pages 454\u2013463, 2024. 2\n[13] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation. In Proceedings of the\nIEEE/CVF international conference on computer vision ,\npages 22246\u201322256, 2023. 3\n[14] Xipeng Chen, Guangrun Wang, Dizhong Zhu, Xiaodan\nLiang, Philip Torr, and Liang Lin. Structure-preserving 3d\ngarment modeling with neural sewing machines. Advances\nin Neural Information Processing Systems , 35:15147\u201315159,\n2022. 2\n[15] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee,\nand Kyoung Mu Lee. Luciddreamer: Domain-free gen-eration of 3d gaussian splatting scenes. arXiv preprint\narXiv:2311.13384 , 2023. 3\n[16] Enric Corona, Albert Pumarola, Guillem Alenya, Ger-\nard Pons-Moll, and Francesc Moreno-Noguer. Smplicit:\nTopology-aware generative model for clothed people. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition , pages 11875\u201311885, 2021. 2\n[17] R Dan \u02c7e\u02c7rek, Endri Dibra, Cengiz \u00a8Oztireli, Remo Ziegler, and\nMarkus Gross. Deepgarment: 3d garment shape estimation\nfrom a single image. In Computer Graphics Forum , pages\n269\u2013280. Wiley Online Library, 2017. 2\n[18] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 13142\u201313153, 2023. 2, 3, 5\n[19] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo,\nOscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte,\nVikram V oleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A\nuniverse of 10m+ 3d objects. Advances in Neural Informa-\ntion Processing Systems , 36, 2024. 3\n[20] Daiheng Gao, Xu Chen, Xindi Zhang, Qi Wang, Ke Sun,\nBang Zhang, Liefeng Bo, and Qixing Huang. Cloth2tex: A\ncustomized cloth texture generation pipeline for 3d virtual\ntry-on. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision (WACV) , 2024. 2\n[21] Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and Otmar\nHilliges. Vid2avatar: 3d avatar reconstruction from videos\nin the wild via self-supervised scene decomposition. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 12858\u201312868, 2023. 2\n[22] Vishnu Mani Hema, Shubhra Aich, Christian Haene, Jean-\nCharles Bazin, and Fernando De la Torre. Famous: High-\nfidelity monocular 3d human digitization using view synthe-\nsis.arXiv preprint arXiv:2410.09690 , 2024. 2\n[23] I Ho, Jie Song, Otmar Hilliges, et al. Sith: Single-view tex-\ntured human reconstruction with image-conditioned diffu-\nsion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 538\u2013549, 2024.\n2\n[24] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,\nDifan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao\nTan. LRM: Large reconstruction model for single image to\n3d.arXiv preprint arXiv:2311.04400 , 2023. 3\n[25] Alain Hore and Djemel Ziou. Image quality metrics: Psnr\nvs. ssim. In 2010 20th international conference on pattern\nrecognition , pages 2366\u20132369. IEEE, 2010. 5\n[26] Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, and Shubham\nTulsiani. Mvd-fusion: Single-view 3d via depth-consistent\nmulti-view generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 9698\u20139707, 2024. 2, 3, 14\n[27] Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao\nZhou, Boning Liu, Shengping Zhang, and Liqiang Nie.\nGaussianavatar: Towards realistic human avatar modeling\nfrom a single video via animatable 3d gaussians. In Pro-\n9\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 634\u2013644, 2024. 2\n[28] Moon-Hwan Jeong, Dong-Hoon Han, and Hyeong-Seok Ko.\nGarment capture from a photograph. Computer Animation\nand Virtual Worlds , 26(3-4):291\u2013300, 2015. 2\n[29] Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang\nLiu, and Hujun Bao. Bcnet: Learning body and cloth shape\nfrom a single image. In Computer Vision\u2013ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part XX 16 , pages 18\u201335. Springer, 2020. 2\n[30] Biliana Kaneva, Josef Sivic, Antonio Torralba, Shai Avidan,\nand William T Freeman. Infinite images: Creating and ex-\nploring a large photorealistic virtual space. Proceedings of\nthe IEEE , 98(8):1391\u20131407, 2010. 3\n[31] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe.\nPoisson surface reconstruction. In Proceedings of the fourth\nEurographics symposium on Geometry processing , 2006. 3,\n4\n[32] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk \u00a8uhler,\nand George Drettakis. 3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics , 42\n(4):1\u201314, 2023. 2\n[33] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su\nZhaoen, Austin James, Peter Selednik, Stuart Anderson, and\nShunsuke Saito. Sapiens: Foundation for human vision mod-\nels. In European Conference on Computer Vision , pages\n206\u2013228. Springer, 2024. 4\n[34] Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel,\nOncel Tuzel, and Anurag Ranjan. Hugs: Human gaussian\nsplats. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition , pages 505\u2013515, 2024.\n2\n[35] Youngjoong Kwon, Baole Fang, Yixing Lu, Haoye Dong,\nCheng Zhang, Francisco Vicente Carrasco, Albert Mosella-\nMontoro, Jianjin Xu, Shingo Takagi, Daeil Kim, et al. Gener-\nalizable human gaussians for sparse view synthesis. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV) , 2024. 3\n[36] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun\nLuan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg\nShakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d\nwith sparse-view generation and large reconstruction model.\narXiv preprint arXiv:2311.06214 , 2023. 3\n[37] Ren Li, Corentin Dumery, Beno \u02c6\u0131t Guillard, and Pascal Fua.\nGarment recovery with shape and deformation priors. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 1586\u20131595, 2024. 2\n[38] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh\nMakadia, Noah Snavely, and Angjoo Kanazawa. Infinite na-\nture: Perpetual view generation of natural scenes from a sin-\ngle image. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 14458\u201314467, 2021.\n3\n[39] Lijuan Liu, Xiangyu Xu, Zhijie Lin, Jiabin Liang, and\nShuicheng Yan. Towards garment sewing pattern reconstruc-\ntion from a single image. ACM Transactions on Graphics\n(TOG) , 42(6):1\u201315, 2023. 2[40] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund\nVarma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single\nimage to 3d mesh in 45 seconds without per-shape optimiza-\ntion. Advances in Neural Information Processing Systems ,\n36, 2024. 2, 3\n[41] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-\n3: Zero-shot one image to 3d object. In Proceedings of\nthe IEEE/CVF international conference on computer vision ,\npages 9298\u20139309, 2023. 2, 3, 4, 5\n[42] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453 , 2023. 2, 3\n[43] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. SMPL: A skinned multi-\nperson linear model. In Seminal Graphics Papers: Pushing\nthe Boundaries, Volume 2 , pages 851\u2013866. 2023. 2\n[44] Zhongjin Luo, Haolin Liu, Chenghong Li, Wanghao Du,\nZirong Jin, Wanhu Sun, Yinyu Nie, Weikai Chen, and Xi-\naoguang Han. Garverselod: High-fidelity 3d garment recon-\nstruction from a single in-the-wild image using a dataset with\nlevels of details. arXiv preprint arXiv:2411.03047 , 2024. 2\n[45] Sahib Majithia, Sandeep N Parameswaran, Sadbhavana\nBabar, Vikram Garg, Astitva Srivastava, and Avinash\nSharma. Robust 3d garment digitization from monocular 2d\nimages for 3d virtual try-on systems. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision , pages 3428\u20133438, 2022. 2\n[46] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view\nsynthesis. In European Conference on Computer Vision\n(ECCV) , 2020. 2\n[47] Gyeongsik Moon, Hyeongjin Nam, Takaaki Shiratori, and\nKyoung Mu Lee. 3d clothed human reconstruction in the\nwild. In European conference on computer vision , pages\n184\u2013200. Springer, 2022. 2\n[48] Xingang Pan, Ayush Tewari, Thomas Leimk \u00a8uhler, Lingjie\nLiu, Abhimitra Meka, and Christian Theobalt. Drag your\ngan: Interactive point-based manipulation on the generative\nimage manifold. In ACM SIGGRAPH 2023 Conference Pro-\nceedings , pages 1\u201311, 2023. 2\n[49] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv\npreprint arXiv:2209.14988 , 2022. 2, 3\n[50] Amit Raj, Patsorn Sangkloy, Huiwen Chang, James Hays,\nDuygu Ceylan, and Jingwan Lu. Swapnet: Image based gar-\nment transfer. In Computer Vision\u2013ECCV 2018: 15th Euro-\npean Conference, Munich, Germany, September 8\u201314, 2018,\nProceedings, Part XII 15 , pages 679\u2013695. Springer, 2018. 2\n[51] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International confer-\nence on machine learning , pages 8821\u20138831. Pmlr, 2021. 2\n[52] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or. Texture: Text-guided texturing of 3d\n10\nshapes. In ACM SIGGRAPH 2023 Conference Proceedings ,\npages 1\u201311, 2023. 2\n[53] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj \u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 10684\u201310695, 2022. 2, 4\n[54] Boxiang Rong, Artur Grigorev, Wenbo Wang, Michael J\nBlack, Bernhard Thomaszewski, Christina Tsalicoglou,\nand Otmar Hilliges. Gaussian garments: Reconstruct-\ning simulation-ready clothing with photorealistic appearance\nfrom multi-view video. arXiv preprint arXiv:2409.08189 ,\n2024. 2\n[55] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. Advances in neural information\nprocessing systems , 35:36479\u201336494, 2022. 2\n[56] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-\nishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned\nimplicit function for high-resolution clothed human digitiza-\ntion. In Proceedings of the IEEE/CVF international confer-\nence on computer vision , pages 2304\u20132314, 2019. 2, 13\n[57] Nikolaos Sarafianos, Tuur Stuyck, Xiaoyu Xiang, Yilei Li,\nJovan Popovic, and Rakesh Ranjan. Garment3dgen: 3d gar-\nment stylization and texture generation. In 3DV, 2025. 2, 5,\n7, 13, 15, 16\n[58] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu. Zero123++: a single image to consistent multi-view dif-\nfusion base model. arXiv preprint arXiv:2310.15110 , 2023.\n3, 4, 5, 14\n[59] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512 , 2023. 2\n[60] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in neural\ninformation processing systems , 32, 2019. 3\n[61] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653 ,\n2023. 2\n[62] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang,\nGang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian\nmodel for high-resolution 3d content creation. In European\nConference on Computer Vision , pages 1\u201318. Springer, 2025.\n3\n[63] Joseph Tung, Gene Chou, Ruojin Cai, Guandao Yang, Kai\nZhang, Gordon Wetzstein, Bharath Hariharan, and Noah\nSnavely. Megascenes: Scene-level view synthesis at scale. In\nEuropean Conference on Computer Vision , pages 197\u2013214.\nSpringer, 2025. 3\n[64] Vikram V oleti, Chun-Han Yao, Mark Boss, Adam Letts,\nDavid Pankratz, Dmitry Tochilkin, Christian Laforte, Robin\nRombach, and Varun Jampani. Sv3d: Novel multi-view syn-\nthesis and 3d generation from a single image using latentvideo diffusion. In European Conference on Computer Vi-\nsion, pages 439\u2013457. Springer, 2025. 3\n[65] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 12619\u201312629, 2023. 3\n[66] Junyan Wang, Zhenhong Sun, Zhiyu Tan, Xuanbai Chen,\nWeihua Chen, Hao Li, Cheng Zhang, and Yang Song. To-\nwards effective usage of human-centric priors in diffusion\nmodels for text-based human image generation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , pages 8446\u20138455, 2024. 2\n[67] Tongxin Wang and Mang Ye. Texfit: Text-driven fashion\nimage editing with diffusion models. In Proceedings of the\nAAAI Conference on Artificial Intelligence , pages 10198\u2013\n10206, 2024. 2\n[68] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing ,\n13(4):600\u2013612, 2004. 5\n[69] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang,\nShuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun\nZhu. Crm: Single image to 3d textured mesh with convo-\nlutional reconstruction model. In European Conference on\nComputer Vision , pages 57\u201374. Springer, 2025. 5, 6, 7\n[70] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan Ho, Andrea Tagliasacchi, and Mohammad\nNorouzi. Novel view synthesis with diffusion models. arXiv\npreprint arXiv:2210.04628 , 2022. 2\n[71] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J\nBlack. Icon: Implicit clothed humans obtained from nor-\nmals. In 2022 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) , pages 13286\u201313296.\nIEEE, 2022. 2\n[72] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and\nMichael J Black. Econ: Explicit clothed humans optimized\nvia normal integration. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition , pages\n512\u2013523, 2023. 2, 13\n[73] Yuliang Xiu, Yufei Ye, Zhen Liu, Dimitrios Tzionas, and\nMichael J Black. Puzzleavatar: Assembling 3d avatars from\npersonal albums. arXiv preprint arXiv:2405.14869 , 2024. 2\n[74] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang,\nShenghua Gao, and Ying Shan. Instantmesh: Efficient 3d\nmesh generation from a single image with sparse-view large\nreconstruction models. arXiv preprint arXiv:2404.07191 ,\n2024. 2, 3, 5, 6, 7\n[75] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen,\nCeyuan Yang, Sida Peng, Yujun Shen, and Gordon Wet-\nzstein. Grm: Large gaussian reconstruction model for ef-\nficient 3d reconstruction and generation. arXiv preprint\narXiv:2403.14621 , 2024. 3\n[76] Shan Yang, Tanya Ambert, Zherong Pan, Ke Wang, Licheng\nYu, Tamara Berg, and Ming C Lin. Detailed gar-\nment recovery from a single-view image. arXiv preprint\narXiv:1608.01250 , 2016. 2\n11\n[77] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Ji-\nacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang,\nQingxiang Lin, Jiaao Yu, et al. Hunyuan3d-1.0: A unified\nframework for text-to-3d and image-to-3d generation. arXiv\npreprint arXiv:2411.02293 , 2024. 5, 7\n[78] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Ji-\nacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang,\nQingxiang Lin, Jiaao Yu, et al. Tencent hunyuan3d-1.0:\nA unified framework for text-to-3d and image-to-3d gener-\nation. arXiv e-prints , pages arXiv\u20132411, 2024. 6\n[79] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent,\nMichael Rubinstein, William T Freeman, Forrester Cole, De-\nqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney:\nGoing from anywhere to everywhere. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 6658\u20136667, 2024. 3\n[80] Cheng Zhang, Yuanhao Wang, Francisco Vicente Carrasco,\nChenglei Wu, Jinlong Yang, Thabo Beeler, and Fernando De\nla Torre. FabricDiffusion: High-fidelity texture transfer for\n3d garments generation from in-the-wild images. In ACM\nSIGGRAPH Asia , 2024. 2\n[81] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 5\n[82] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai.\nPamir: Parametric model-conditioned implicit representa-\ntion for image-based human reconstruction. IEEE transac-\ntions on pattern analysis and machine intelligence , 44(6):\n3170\u20133184, 2021. 2, 13\n[83] Bin Zhou, Xiaowu Chen, Qiang Fu, Kan Guo, and Ping Tan.\nGarment modeling from a single image. In Computer graph-\nics forum , pages 85\u201391. Wiley Online Library, 2013. 2\n[84] Shizhe Zhou, Hongbo Fu, Ligang Liu, Daniel Cohen-Or, and\nXiaoguang Han. Parametric reshaping of human bodies in\nimages. ACM transactions on graphics (TOG) , 29(4):1\u201310,\n2010. 2\n[85] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-\ntilling view-conditioned diffusion for 3d reconstruction. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 12588\u201312597, 2023. 2,\n3\n[86] Heming Zhu, Lingteng Qiu, Yuda Qiu, and Xiaoguang Han.\nRegistering explicit to implicit: Towards high-fidelity gar-\nment mesh reconstruction from single images. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 3845\u20133854, 2022. 2\n[87] Luyang Zhu, Yingwei Li, Nan Liu, Hao Peng, Dawei\nYang, and Ira Kemelmacher-Shlizerman. M&m vto: Multi-\ngarment virtual try-on and editing. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 1346\u20131356, 2024. 2\n[88] Xingxing Zou, Xintong Han, and Waikeung Wong. Cloth4d:\nA dataset for clothed human reconstruction. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 12847\u201312857, 2023. 2, 3, 5\n12\nGarmentCrafter: Progressive Novel View Synthesis for Single-View\n3D Garment Reconstruction and Editing\nSupplementary Material\nA. Contribution, Novelty, and Limitation\nWe reiterate our contribution, novelty, and limitations.\nContribution and Novelty. Our main contribution lies in\na new direction: enabling non-professional users to create\nand edit 3D garment with single-view input. While ex-\nisting works have made strides in reconstructing clothed\nhumans [5, 56, 72, 82] or garment [57] from a single\nimage, they mainly rely on optimizing pre-defined gar-\nment or human templates. In contrast, we target a more\nflexible, template-free garment reconstruction framework.\nSpecifically, we propose to progressively synthesize depth-\naccurate novel view images with enhanced cross-view con-\nsistency. Moreover, our method enables single-view 3D\nediting, including part-based or local surface edits \u2014 ca-\npabilities that are absent in the aforementioned methods.\nScope and Limitations. As discussed in Section 5 of the\nmain paper, our method has certain limitations. We mainly\nfocus on garment in a rest pose. As will be shown in Sec-\ntion D.4, our method may struggle to accurately capture the\ngeometry of garments in non-rest poses. With that said, this\nscope is a deliberate choice, as rest poses provide a consis-\ntent and intuitive baseline that aligns well with the needs of\ngarment editing applications.\nB. Ethics and Social Impacts\nWe focus on advancing garment digitization. We do not\nforesee any ethical concerns or negative societal impacts\narising from our work. Our training and evaluation pro-\ncesses do not involve any sensitive data, human identities, or\npersonal information. All experiments and datasets used in\nthis study are compliant with ethical research practices. By\nadvancing template-free garment reconstruction for non-\nprofessional users, our method avoids potential biases asso-\nciated with specific body or garment templates, promoting\ninclusiveness in digital garment reconstruction.\nC. Additional Implementation Details\nIn this section, we provide additional implementation de-\ntails of our method omitted in the main text.\nC.1. Conditional Image Generation\nOur image generation model is finetuned from the Stable\nZero-1-to-3 checkpoint2. To account for the additional pro-\njected image as input, we add 4 additional channels to the\n2https://huggingface.co/stabilityai/stable-zero123input convolution layer of the denosing UNet and initialize\nthe weights to be zeros. The training resolution is 512 \u00d7512.\nWe train the mode on 4 NVIDIA A6000 GPUs with a total\nbatch size of 256 for 20k iterations for 2 days.\nC.2. Conditional Depth Generation\nOur conditional image generation model is finetuned from\nthe Sapiens-0.3B depth checkpoint3. To add the projected\npartial depth map as the additional condition, we add 1 ex-\ntra channels to the input projection layer of the vision trans-\nformer backbone and initialize its weights to be zeros. The\ntraining resolution is 512 \u00d7512. We train the model on 4\nA6000 GPUs with a total batch size of 24 for 3 days.\nC.3. Computational Efficiency\nThe inference time and memory consumption of our method\nare approximately 1 minute and 10 GB, respectively, on a\nsingle A6000 GPU. These values are comparable to those of\nmost baseline methods, which have inference times ranging\nfrom 10 seconds to 1 minute.\nC.4. Measures to Reduce Error Accumulation\nSince our method synthesizes novel views in sequential\nsteps, it is susceptible to error accumulation. To address\nthis, we incorporate a series of techniques aimed at mitigat-\ning such errors and improving overall robustness.\nPoint Cloud Outlier Removal. Depth predictions near\nthe edges of discontinuities (with large jumps in depth val-\nues) are occasionally inaccurate, resulting in some floating\npoints in the point cloud. To address this, we apply a clas-\nsical outlier removal method at each step to eliminate these\nfloating points, ensuring a cleaner and accurate point cloud.\nOpen Hole Detection. We observe that depth predictions\nare less reliable in open-hole regions of a garment surface,\nsuch as holes in collars and sleeves. Additionally, the sur-\nface orientation derived from the estimated depth map in\nthese areas can be reversed. These errors can propagate and\nlead to artifacts in subsequent steps. To address this issue,\nwe develop a simple algorithm to detect open holes and ex-\nclude these regions during point cloud completion, improv-\ning the robustness of the pipeline.\nThe detection algorithm is based on the observation that\nthe interior regions of open holes typically exhibit greater\ndepth values compared to the boundary pixels. As shown\nin Figure S8, after synthesizing the completed image and\n3https://huggingface.co/facebook/sapiens-depth-0.3b\n13\nGenerated RGB\nGenerated DepthDepth Edge MapDetected Open Hole\nFigure S8. Open hole detection in garments. We note that inte-\nrior regions of open holes in a garment exhibit greater depth values\ncompared to the boundary pixels. Leveraging this observation, we\npropose a simple yet effective algorithm to detect open holes and\nexclude these regions during point cloud completion, improving\nthe robustness of the pipeline.\ndepth maps from a novel viewpoint, we first detect edges in\nthe depth map and identify connected regions enclosed by\nthese edges using classical methods. A connected region R\nis classified as an open hole if more than a threshold \u03f5of its\nboundary pixels have depth values smaller than the average\ndepth of the region. For all our experiments, we found that\n\u03f5can be robustly set to 0.85.\nClipping Distant Depth Values. Our observations indicate\nthat synthesized images and depth maps are more robust\nin regions closer to the camera compared to those farther\naway. At steps 3 and 4 (corresponding to azimuth angles\nof120\u25e6and\u2212120\u25e6), the entire back side of the garment is\nsynthesized from a side view. For these steps, we only use\npixels with smaller depth values for point cloud completion,\ndisregarding pixels with larger depth values.\nC.5. Point-to-Mesh Reconstruction\nWe use Screened Poisson surface reconstruction to convert\npoint clouds to meshes. Note that the point orientations are\nestimated using depth maps at each step, as described in\nSection 3.3 of the main paper. While Screened Poisson re-\nconstruction generates a watertight mesh, we aim to pre-\nserve the non-watertight topology of garments (e.g., main-\ntaining holes in collars, sleeves, etc.). To achieve this, we\nperform an additional trimming operation4to remove un-\nwanted mesh faces introduced during the Poisson recon-\nstruction that fill open holes by leveraging the point cloud\ndensity. To further reduce artifacts, we remove floating\nfaces unconnected to the main mesh and apply Laplacian\nsmoothing to refine the mesh surface.\nC.6. Scope of Single-View 3D Editing\nAs introduced in Section 3.3 of the main paper, Gar-\nmentCrafter enables single-view editing through a simple\n4https://github.com/mkazhdan/PoissonReconworkflow: identify the edited 3D region, remove the orig-\ninal mesh in the identified area, and reconstruct the edited\ncomponents. We support two types of editing operations,\ndifferentiated by their assumptions about the edited regions.\nThe first category is local surface editing. Given a cam-\nera viewpoint and a mask, this approach assumes that only\nthe visible surface intersected by the camera rays corre-\nsponding to the masked pixels will be edited. Occluded sur-\nfaces are ignored, even if their mesh vertices project within\nthe mask. To facilitate reconstruction, we remove the mesh\nvertices of the selected surface. Additionally, internal ver-\ntices near the external surface are also removed to account\nfor surface thickness.\nThe second category, part-based editing, involves modi-\nfying a 3D garment part, including not only the \u201cfront\u201d sur-\nface but also the \u201cback\u201d and \u201cinternal\u201d surfaces within a\nmasked region. For ease of implementation, we always use\nthe frontal view as the editing perspective and remove all\nmesh vertices whose 2D projections fall within the mask.\nOur editing pipeline is designed under the assumption\nthat both the geometry and the texture will be edited. There-\nfore, it is not optimized for cases where (1) surface texture\nis modified while preserving the geometry, or (2) the geom-\netry or pose is altered while preserving the texture.\nC.7. 2D Editing Assumptions\nIn theory, our method is agnostic to the tools used for 2D\nediting. The edits can be created using deep learning-based\nimage editing models or traditional tools like Photoshop.\nHowever, our approach requires the edits to be confined\nto regions specified by masks in the 2D input. Therefore,\nglobal edits such as style transfer that alters the entire im-\nage, are not recommended.\nD. Additional Results and Analyses\nD.1. Intermediate Results of Progressive NVS\nIn Figure 2 of the main paper, we showed results at one\nspecific camera rotation step during the progressive novel\nview synthesis. Here, we illustrate the whole process and\nshow the intermediate results in Figure S9.\nD.2. Additional Baseline Comparisons\nD.2.1. Comparison with SoTA NVS methods\nWe present additional quantitative comparisons for novel\nview synthesis against state-of-the-art methods (Zero-1-to-\n3++ [58] & MVD-Fusion [26], fine-tuned with same data).\nFor each object in the held-out test set of 150 garment as-\nsets, we sample six camera viewpoints with an elevation of\n20 degrees and evenly spaced azimuth angles covering 360\ndegrees. Each method takes a frontal image as input and\ngenerates six corresponding novel views, which we evaluate\nagainst ground truth images using image similarity metrics\n14\nDepth Mesh\nProjected Completed RGB Camera\nProjected Completed\nFigure S9. Intermediate results of progressive novel view synthesis along a full camera trajectory. From an input RGB image (top-\nleft), GarmentCrafter progressively synthesize novel view RGB and depth maps following a zigzag camera trajectory.\n(LPIPS, PSNR, and SSIM). We also report our proposed\nCVCS score. Table S4 shows that our method achieves su-\nperior performance across all metrics.\nD.2.2. Qualitative Comparison with Garment3DGen\nAs the texture reconstruction code of Garment3DGen [57]\nis not released, we provide qualitative comparison with Gar-\nment3DGen on the reconstructed mesh geometry in Fig-\nure S10. Our method reconstructs 3D garments with much\nricher geometric details and much less inference time (1 min\nvs. 3 hours).\nD.3. Additional Analyses and Applications\nD.3.1. Degree of Zigzag Camera Trajectory\nWe have studied all major design choices in our pipeline\nin the main paper, including the effect of progressive novelTable S4. Quantitative comparison for novel view synthesis.\nOur method outperforms all state-of-the-art novel view synthesis\nmethods cross both image similarity and consistency metrics.\nLPIPS \u2193 PSNR \u2191 SSIM \u2191 CVCS \u2191\nZero123++ 0.1611 18.023 0.7979 0.8957\nMVD-Fusion 0.1528 18.529 0.8026 0.9090\nOurs 0.1052 22.776 0.8557 0.9512\nview synthesis and camera trajectory. Here, we analyze the\nimpact of the degree of Zigzag Camera Trajectory and show\nthe results in Table S5. In our experiments, we use a 60\u25e6tra-\njectory as it provides a good balance between view coverage\nand efficiency. While the choice of degree slightly affects\nthe ability to synthesize side-view garments (i.e., 90\u25e6), our\nanalysis indicates that the overall performance is not highly\n15\nGarment3DGenOurs\nInputOurs (texture)Figure S10. Qualitative comparison with Garment3DGen [57].\nOur GarmentCrafter reconstructs garment meshes with richer de-\ntails with much lower computational costs.\nTable S5. Analysis of the degree of zigzag camera trajectory.\nIn our experiments, we use a 60\u25e6trajectory as it provides a good\nbalance between view coverage and efficiency. While the choice of\ndegree slightly affects the ability to synthesize side-view garments\n(i.e., 90\u25e6), our analysis indicates that the overall performance is\nnot highly sensitive to this parameter.\nDegreeAppearance Geometry\nSSIM \u2191 LPIPS \u2193 PSNR \u2191 Chamfer \u2193\n30\u25e60.8044 0.1675 20.62 0.0051\n60\u25e60.8066 0.1638 20.62 0.0050\n90\u25e60.8003 0.1709 20.19 0.0070\n120\u25e60.8053 0.1654 20.51 0.0050\nInput RGB Novel View Recon. Mesh\nFigure S11. Failure case. GarmentCrafter may fail to reconstruct\nthe garment with arbitrary poses.\nsensitive to this parameter. We do not notice any other sig-\nnificant hyperparameters in our framework.\nD.3.2. Digitizing AI-generated Apparel\nWe explore the potential of combing GarmentCrafter with\nAI-generated garment image and show examples in Fig-\nure S14. Using a text-to-image generative model, we pro-\nduce synthetic garment images and apply GarmentCrafter\nto digitize them. The results demonstrate the broad appli-\ncability of our method in handling diverse inputs, including\nAI-generated designs.D.4. Failure Cases\nThe focus of our work is on reconstructing and editing gar-\nments in their rest pose. Consequently, our method strug-\ngles with input images in arbitrary poses as such instances\nlie outside of the training data distribution. As illustrated in\nFigure S11, an input garment image in a non-resting pose\nresults in the failure of our model to synthesize coherent\nnovel view images, leading to nonsensical reconstructions.\n"
        },
        "paper_title": "GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D\n  Garment Reconstruction and Editing",
        "publication": "arXiv"
      }
    },
    {
      "content_id": 6,
      "title": "whisper",
      "url": "https://github.com/openai/whisper",
      "source_type": "github",
      "metadata": {
        "stars": 77963,
        "language": "Python",
        "topics": [],
        "forks": 9336,
        "repo_name": "whisper",
        "full_name": "openai/whisper"
      }
    }
  ],
  "metadata": {
    "time_taken": 22.978418588638306,
    "llm_provider": "claude",
    "llm_model": "claude-3-sonnet-20240229",
    "search_type": "hybrid",
    "vector_weight": null,
    "keyword_weight": null,
    "timestamp": "2025-03-12T12:28:37.803958"
  }
}