{
    "text": " So many zeros in my life, what could I do? Fear not, weary scientists. Zero, the hero is here. How did you get yourself into such a numerical nightmare? I did some one-hearted coding. My data set just exploded. A classic curse of high-dimensionality. But fear not, I have the perfect solution. Sparse representation data frames. Sparse? What's that? If you have a data set full of zeros, you don't need to store all the zeros. I think here, like, compression, all we need to do is know where the important numbers are. It's impressive. My user behavior, my text, social networks, all that data is mostly zeros. Nature loves sparsity. In recommenders, I use a sparse representation of users. But that's not all I do. I power dropout in neural networks, where we zero out neurons during training to make models more robust. And I'm guessing you're behind L1 regularization, indeed. Pushing those features down to zero. But you know, I also show up in models. Wait, you model too? Oh, yes, and I do it sparingly. Make mixture of experts models. There we have a router that figures out which expert model to use for each input, keeping most experts inactive or sparse. So the model itself becomes sparse during inference. Oh, so smart. Precisely saving an enormous amount of compute while still keeping that performance up. Wow, for someone so sparse, you sure do have a lot of uses.",
    "segments": [
        {
            "id": 0,
            "seek": 0,
            "start": 0.0,
            "end": 3.12,
            "text": " So many zeros in my life, what could I do?",
            "tokens": [
                50364,
                407,
                867,
                35193,
                294,
                452,
                993,
                11,
                437,
                727,
                286,
                360,
                30,
                50520
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 1,
            "seek": 0,
            "start": 3.12,
            "end": 5.04,
            "text": " Fear not, weary scientists.",
            "tokens": [
                50520,
                28054,
                406,
                11,
                47853,
                7708,
                13,
                50616
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 2,
            "seek": 0,
            "start": 5.04,
            "end": 7.12,
            "text": " Zero, the hero is here.",
            "tokens": [
                50616,
                17182,
                11,
                264,
                5316,
                307,
                510,
                13,
                50720
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 3,
            "seek": 0,
            "start": 7.12,
            "end": 10.16,
            "text": " How did you get yourself into such a numerical nightmare?",
            "tokens": [
                50720,
                1012,
                630,
                291,
                483,
                1803,
                666,
                1270,
                257,
                29054,
                18724,
                30,
                50872
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 4,
            "seek": 0,
            "start": 10.16,
            "end": 11.8,
            "text": " I did some one-hearted coding.",
            "tokens": [
                50872,
                286,
                630,
                512,
                472,
                12,
                25471,
                17720,
                13,
                50954
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 5,
            "seek": 0,
            "start": 11.8,
            "end": 13.68,
            "text": " My data set just exploded.",
            "tokens": [
                50954,
                1222,
                1412,
                992,
                445,
                27049,
                13,
                51048
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 6,
            "seek": 0,
            "start": 13.68,
            "end": 16.32,
            "text": " A classic curse of high-dimensionality.",
            "tokens": [
                51048,
                316,
                7230,
                17139,
                295,
                1090,
                12,
                13595,
                3378,
                1860,
                13,
                51180
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 7,
            "seek": 0,
            "start": 16.32,
            "end": 18.96,
            "text": " But fear not, I have the perfect solution.",
            "tokens": [
                51180,
                583,
                4240,
                406,
                11,
                286,
                362,
                264,
                2176,
                3827,
                13,
                51312
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 8,
            "seek": 0,
            "start": 18.96,
            "end": 21.8,
            "text": " Sparse representation data frames.",
            "tokens": [
                51312,
                1738,
                11668,
                10290,
                1412,
                12083,
                13,
                51454
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 9,
            "seek": 0,
            "start": 21.8,
            "end": 22.92,
            "text": " Sparse?",
            "tokens": [
                51454,
                1738,
                11668,
                30,
                51510
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 10,
            "seek": 0,
            "start": 22.92,
            "end": 23.76,
            "text": " What's that?",
            "tokens": [
                51510,
                708,
                311,
                300,
                30,
                51552
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 11,
            "seek": 0,
            "start": 23.76,
            "end": 26.32,
            "text": " If you have a data set full of zeros,",
            "tokens": [
                51552,
                759,
                291,
                362,
                257,
                1412,
                992,
                1577,
                295,
                35193,
                11,
                51680
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 12,
            "seek": 0,
            "start": 26.32,
            "end": 28.52,
            "text": " you don't need to store all the zeros.",
            "tokens": [
                51680,
                291,
                500,
                380,
                643,
                281,
                3531,
                439,
                264,
                35193,
                13,
                51790
            ],
            "temperature": 0.0,
            "avg_logprob": -0.18776103012434397,
            "compression_ratio": 1.5661764705882353,
            "no_speech_prob": 0.009946982376277447
        },
        {
            "id": 13,
            "seek": 2852,
            "start": 28.52,
            "end": 31.0,
            "text": " I think here, like, compression, all we need to do",
            "tokens": [
                50364,
                286,
                519,
                510,
                11,
                411,
                11,
                19355,
                11,
                439,
                321,
                643,
                281,
                360,
                50488
            ],
            "temperature": 0.0,
            "avg_logprob": -0.16652579605579376,
            "compression_ratio": 1.5859649122807018,
            "no_speech_prob": 0.003349938662722707
        },
        {
            "id": 14,
            "seek": 2852,
            "start": 31.0,
            "end": 32.84,
            "text": " is know where the important numbers are.",
            "tokens": [
                50488,
                307,
                458,
                689,
                264,
                1021,
                3547,
                366,
                13,
                50580
            ],
            "temperature": 0.0,
            "avg_logprob": -0.16652579605579376,
            "compression_ratio": 1.5859649122807018,
            "no_speech_prob": 0.003349938662722707
        },
        {
            "id": 15,
            "seek": 2852,
            "start": 32.84,
            "end": 33.88,
            "text": " It's impressive.",
            "tokens": [
                50580,
                467,
                311,
                8992,
                13,
                50632
            ],
            "temperature": 0.0,
            "avg_logprob": -0.16652579605579376,
            "compression_ratio": 1.5859649122807018,
            "no_speech_prob": 0.003349938662722707
        },
        {
            "id": 16,
            "seek": 2852,
            "start": 33.88,
            "end": 37.0,
            "text": " My user behavior, my text, social networks,",
            "tokens": [
                50632,
                1222,
                4195,
                5223,
                11,
                452,
                2487,
                11,
                2093,
                9590,
                11,
                50788
            ],
            "temperature": 0.0,
            "avg_logprob": -0.16652579605579376,
            "compression_ratio": 1.5859649122807018,
            "no_speech_prob": 0.003349938662722707
        },
        {
            "id": 17,
            "seek": 2852,
            "start": 37.0,
            "end": 38.8,
            "text": " all that data is mostly zeros.",
            "tokens": [
                50788,
                439,
                300,
                1412,
                307,
                5240,
                35193,
                13,
                50878
            ],
            "temperature": 0.0,
            "avg_logprob": -0.16652579605579376,
            "compression_ratio": 1.5859649122807018,
            "no_speech_prob": 0.003349938662722707
        },
        {
            "id": 18,
            "seek": 2852,
            "start": 38.8,
            "end": 40.480000000000004,
            "text": " Nature loves sparsity.",
            "tokens": [
                50878,
                20159,
                6752,
                637,
                685,
                507,
                13,
                50962
            ],
            "temperature": 0.0,
            "avg_logprob": -0.16652579605579376,
            "compression_ratio": 1.5859649122807018,
            "no_speech_prob": 0.003349938662722707
        },
        {
            "id": 19,
            "seek": 2852,
            "start": 40.480000000000004,
            "end": 44.84,
            "text": " In recommenders, I use a sparse representation of users.",
            "tokens": [
                50962,
                682,
                2748,
                433,
                11,
                286,
                764,
                257,
                637,
                11668,
                10290,
                295,
                5022,
                13,
                51180
            ],
            "temperature": 0.0,
            "avg_logprob": -0.16652579605579376,
            "compression_ratio": 1.5859649122807018,
            "no_speech_prob": 0.003349938662722707
        },
        {
            "id": 20,
            "seek": 2852,
            "start": 44.84,
            "end": 46.16,
            "text": " But that's not all I do.",
            "tokens": [
                51180,
                583,
                300,
                311,
                406,
                439,
                286,
                360,
                13,
                51246
            ],
            "temperature": 0.0,
            "avg_logprob": -0.16652579605579376,
            "compression_ratio": 1.5859649122807018,
            "no_speech_prob": 0.003349938662722707
        },
        {
            "id": 21,
            "seek": 2852,
            "start": 46.16,
            "end": 50.56,
            "text": " I power dropout in neural networks, where we zero out neurons",
            "tokens": [
                51246,
                286,
                1347,
                3270,
                346,
                294,
                18161,
                9590,
                11,
                689,
                321,
                4018,
                484,
                22027,
                51466
            ],
            "temperature": 0.0,
            "avg_logprob": -0.16652579605579376,
            "compression_ratio": 1.5859649122807018,
            "no_speech_prob": 0.003349938662722707
        },
        {
            "id": 22,
            "seek": 2852,
            "start": 50.56,
            "end": 52.879999999999995,
            "text": " during training to make models more robust.",
            "tokens": [
                51466,
                1830,
                3097,
                281,
                652,
                5245,
                544,
                13956,
                13,
                51582
            ],
            "temperature": 0.0,
            "avg_logprob": -0.16652579605579376,
            "compression_ratio": 1.5859649122807018,
            "no_speech_prob": 0.003349938662722707
        },
        {
            "id": 23,
            "seek": 2852,
            "start": 52.879999999999995,
            "end": 56.519999999999996,
            "text": " And I'm guessing you're behind L1 regularization, indeed.",
            "tokens": [
                51582,
                400,
                286,
                478,
                17939,
                291,
                434,
                2261,
                441,
                16,
                3890,
                2144,
                11,
                6451,
                13,
                51764
            ],
            "temperature": 0.0,
            "avg_logprob": -0.16652579605579376,
            "compression_ratio": 1.5859649122807018,
            "no_speech_prob": 0.003349938662722707
        },
        {
            "id": 24,
            "seek": 5652,
            "start": 56.52,
            "end": 59.120000000000005,
            "text": " Pushing those features down to zero.",
            "tokens": [
                50364,
                430,
                5371,
                729,
                4122,
                760,
                281,
                4018,
                13,
                50494
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 25,
            "seek": 5652,
            "start": 59.120000000000005,
            "end": 61.52,
            "text": " But you know, I also show up in models.",
            "tokens": [
                50494,
                583,
                291,
                458,
                11,
                286,
                611,
                855,
                493,
                294,
                5245,
                13,
                50614
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 26,
            "seek": 5652,
            "start": 61.52,
            "end": 63.32,
            "text": " Wait, you model too?",
            "tokens": [
                50614,
                3802,
                11,
                291,
                2316,
                886,
                30,
                50704
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 27,
            "seek": 5652,
            "start": 63.32,
            "end": 65.60000000000001,
            "text": " Oh, yes, and I do it sparingly.",
            "tokens": [
                50704,
                876,
                11,
                2086,
                11,
                293,
                286,
                360,
                309,
                637,
                1921,
                356,
                13,
                50818
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 28,
            "seek": 5652,
            "start": 65.60000000000001,
            "end": 67.16,
            "text": " Make mixture of experts models.",
            "tokens": [
                50818,
                4387,
                9925,
                295,
                8572,
                5245,
                13,
                50896
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 29,
            "seek": 5652,
            "start": 67.16,
            "end": 69.4,
            "text": " There we have a router that figures out",
            "tokens": [
                50896,
                821,
                321,
                362,
                257,
                22492,
                300,
                9624,
                484,
                51008
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 30,
            "seek": 5652,
            "start": 69.4,
            "end": 72.28,
            "text": " which expert model to use for each input,",
            "tokens": [
                51008,
                597,
                5844,
                2316,
                281,
                764,
                337,
                1184,
                4846,
                11,
                51152
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 31,
            "seek": 5652,
            "start": 72.28,
            "end": 76.16,
            "text": " keeping most experts inactive or sparse.",
            "tokens": [
                51152,
                5145,
                881,
                8572,
                294,
                12596,
                420,
                637,
                11668,
                13,
                51346
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 32,
            "seek": 5652,
            "start": 76.16,
            "end": 79.0,
            "text": " So the model itself becomes sparse during inference.",
            "tokens": [
                51346,
                407,
                264,
                2316,
                2564,
                3643,
                637,
                11668,
                1830,
                38253,
                13,
                51488
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 33,
            "seek": 5652,
            "start": 79.0,
            "end": 80.52000000000001,
            "text": " Oh, so smart.",
            "tokens": [
                51488,
                876,
                11,
                370,
                4069,
                13,
                51564
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 34,
            "seek": 5652,
            "start": 80.52000000000001,
            "end": 83.4,
            "text": " Precisely saving an enormous amount of compute",
            "tokens": [
                51564,
                48746,
                736,
                6816,
                364,
                11322,
                2372,
                295,
                14722,
                51708
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 35,
            "seek": 5652,
            "start": 83.4,
            "end": 85.2,
            "text": " while still keeping that performance up.",
            "tokens": [
                51708,
                1339,
                920,
                5145,
                300,
                3389,
                493,
                13,
                51798
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1674688720703125,
            "compression_ratio": 1.625925925925926,
            "no_speech_prob": 0.002790634287521243
        },
        {
            "id": 36,
            "seek": 8520,
            "start": 85.2,
            "end": 89.24000000000001,
            "text": " Wow, for someone so sparse, you sure do have a lot of uses.",
            "tokens": [
                50366,
                3153,
                11,
                337,
                1580,
                370,
                637,
                11668,
                11,
                291,
                988,
                360,
                362,
                257,
                688,
                295,
                4960,
                13,
                50566
            ],
            "temperature": 0.0,
            "avg_logprob": -0.2807314395904541,
            "compression_ratio": 0.9516129032258065,
            "no_speech_prob": 0.005348604638129473
        }
    ],
    "language": "en",
    "filename": "2025-03-01_22-04-36_DGrDqDaSrbO.mp4",
    "account": "rajistics",
    "shortcode": "DGrDqDaSrbO",
    "timestamp": "2025-03-01T22:04:36",
    "caption": "Sparsity is the concept of leveraging zeros in data and models for efficiency. Sparse representations enable working with massive datasets by storing only non-zero values, saving memory and computation. Applications include data compression, dropout in neural networks, L1 regularization, and conditional computation in models like Mixture of Experts.",
    "likes": 146,
    "comments": 0,
    "url": "https://www.instagram.com/p/DGrDqDaSrbO/",
    "username": "rajistics",
    "download_date": "2025-03-11T22:21:03.656526"
}