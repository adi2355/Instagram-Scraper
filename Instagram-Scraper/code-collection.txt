

================================================================================
File: app.py
================================================================================

"""
Web interface for the Instagram Knowledge Base
"""
import os
import re
import sqlite3
from functools import lru_cache
from flask import Flask, render_template, request, jsonify, g, send_from_directory, redirect, url_for

from config import (
    DB_PATH,
    WEB_PORT,
    DEBUG_MODE,
    DOWNLOAD_DIR,
    DATA_DIR
)

app = Flask(__name__)

def get_db():
    """Get database connection with row factory for easy access"""
    db = getattr(g, '_database', None)
    if db is None:
        db = g._database = sqlite3.connect(DB_PATH)
        db.row_factory = sqlite3.Row
    return db

@app.teardown_appcontext
def close_connection(exception):
    """Close database connection when app context ends"""
    db = getattr(g, '_database', None)
    if db is not None:
        db.close()

@app.route('/')
def index():
    """Home page with search interface"""
    return render_template('index.html')

@app.route('/search')
def search():
    """Search endpoint"""
    query = request.args.get('query', '')
    account_filter = request.args.get('account', '')
    tag_filter = request.args.get('tag', '')
    page = int(request.args.get('page', 1))
    per_page = int(request.args.get('per_page', 10))
    
    # Calculate offset
    offset = (page - 1) * per_page
    
    db = get_db()
    cursor = db.cursor()
    
    # Get available accounts for filtering
    cursor.execute("SELECT DISTINCT account FROM videos ORDER BY account")
    accounts = [row['account'] for row in cursor.fetchall()]
    
    # Get popular tags for filtering
    cursor.execute('''
    SELECT tag, COUNT(*) as count 
    FROM tags 
    GROUP BY tag 
    ORDER BY count DESC 
    LIMIT 20
    ''')
    tags = [{'tag': row['tag'], 'count': row['count']} for row in cursor.fetchall()]
    
    results = []
    total_results = 0
    
    if query or account_filter or tag_filter:
        # Base query
        sql = '''
        SELECT v.id, v.shortcode, v.account, v.caption, v.transcript, 
               v.timestamp, v.url, v.likes, v.comments
        FROM videos v
        '''
        
        params = []
        where_clauses = []
        
        # Add tag filter if specified
        if tag_filter:
            sql += "JOIN tags t ON v.id = t.video_id "
            where_clauses.append("t.tag = ?")
            params.append(tag_filter)
        
        # Add account filter if specified
        if account_filter:
            where_clauses.append("v.account = ?")
            params.append(account_filter)
        
        # Add search query if specified
        if query:
            sql += "JOIN videos_fts fts ON v.id = fts.docid "  # Changed from rowid to docid for FTS4
            where_clauses.append("videos_fts MATCH ?")
            params.append(query)
            
            # FTS4 doesn't support the snippet function, so we'll remove this line
            # and handle highlighting in application code later
        
        # Combine where clauses
        if where_clauses:
            sql += "WHERE " + " AND ".join(where_clauses)
        
        # Get total results count
        count_sql = f"SELECT COUNT(*) as count FROM ({sql})"
        cursor.execute(count_sql, params)
        total_results = cursor.fetchone()['count']
        
        # Add order and limit
        if query:
            # FTS4 doesn't have built-in rank function
            sql += " ORDER BY v.timestamp DESC"
        else:
            sql += " ORDER BY v.timestamp DESC"
            
        sql += " LIMIT ? OFFSET ?"
        params.extend([per_page, offset])
        
        # Execute query
        cursor.execute(sql, params)
        results = cursor.fetchall()
    
    # Format results for template
    formatted_results = []
    for row in results:
        # Get tags for this video
        cursor.execute("SELECT tag FROM tags WHERE video_id = ?", (row['id'],))
        video_tags = [tag['tag'] for tag in cursor.fetchall()]
        
        # Format the date
        timestamp = row['timestamp'].split('T')[0] if row['timestamp'] else 'Unknown'
        
        # Get video path
        video_path = f"/video/{row['account']}/{row['shortcode']}"
        
        # Create our own snippet since snippet() isn't available in FTS4
        transcript = row['transcript'] or ""
        
        # Default snippet (first ~200 chars of transcript)
        if len(transcript) > 200:
            snippet = transcript[:200] + '...'
        else:
            snippet = transcript
        
        # If we have a search query, try to find a relevant part of the transcript
        if query and transcript:
            query_words = query.lower().split()
            transcript_lower = transcript.lower()
            
            # Find the first occurrence of any query word
            position = -1
            for word in query_words:
                pos = transcript_lower.find(word)
                if pos != -1:
                    position = pos
                    break
            
            # Extract a section around the match if found
            if position != -1:
                # Get 50 chars before and 150 after the match position
                start = max(0, position - 50)
                end = min(len(transcript), position + 150)
                
                # Get the relevant section
                context = transcript[start:end]
                
                # Add ellipsis if needed
                if start > 0:
                    context = '...' + context
                if end < len(transcript):
                    context += '...'
                
                snippet = context
        
        # Simple highlighting for matched terms
        if query and transcript:
            for word in query.lower().split():
                # Case-insensitive replace with HTML highlighting
                word_pattern = word.lower()
                start = 0
                while True:
                    start_pos = snippet.lower().find(word_pattern, start)
                    if start_pos == -1:
                        break
                    
                    end_pos = start_pos + len(word)
                    original_word = snippet[start_pos:end_pos]
                    snippet = snippet[:start_pos] + f'<mark>{original_word}</mark>' + snippet[end_pos:]
                    
                    # Move past this match
                    start = start_pos + len(f'<mark>{original_word}</mark>')
        
        formatted_results.append({
            'id': row['id'],
            'shortcode': row['shortcode'],
            'account': row['account'],
            'caption': row['caption'],
            'snippet': snippet,
            'timestamp': timestamp,
            'url': row['url'],
            'likes': row['likes'],
            'comments': row['comments'],
            'tags': video_tags,
            'video_path': video_path
        })
    
    # Calculate pagination info
    total_pages = (total_results + per_page - 1) // per_page if total_results > 0 else 1
    has_prev = page > 1
    has_next = page < total_pages
    
    return render_template(
        'search.html',
        query=query,
        account_filter=account_filter,
        tag_filter=tag_filter,
        results=formatted_results,
        accounts=accounts,
        tags=tags,
        total_results=total_results,
        page=page,
        per_page=per_page,
        total_pages=total_pages,
        has_prev=has_prev,
        has_next=has_next
    )

@app.route('/video/<account>/<shortcode>')
def video(account, shortcode):
    """Video detail page"""
    db = get_db()
    cursor = db.cursor()
    
    # Get video details
    cursor.execute('''
    SELECT v.* FROM videos v
    WHERE v.account = ? AND v.shortcode = ?
    ''', (account, shortcode))
    video = cursor.fetchone()
    
    if not video:
        return "Video not found", 404
    
    # Get tags
    cursor.execute("SELECT tag FROM tags WHERE video_id = ?", (video['id'],))
    tags = [tag['tag'] for tag in cursor.fetchall()]
    
    # Find video file
    video_filename = None
    account_dir = os.path.join(DOWNLOAD_DIR, account)
    if os.path.exists(account_dir):
        for filename in os.listdir(account_dir):
            if shortcode in filename and filename.endswith('.mp4'):
                video_filename = f"/media/{account}/{filename}"
                break
    
    return render_template(
        'video.html',
        video=video,
        tags=tags,
        video_filename=video_filename
    )

@app.route('/api/search')
def api_search():
    """API endpoint for search (for AJAX requests)"""
    query = request.args.get('query', '')
    account = request.args.get('account', '')
    tag = request.args.get('tag', '')
    page = int(request.args.get('page', 1))
    per_page = int(request.args.get('per_page', 10))
    
    db = get_db()
    cursor = db.cursor()
    
    # Start building the query
    sql_select = """
        SELECT v.id, v.shortcode, v.account, v.caption, v.transcript, v.summary,
               v.timestamp, v.url, v.likes, v.comments, v.word_count, v.duration_seconds
    """
    
    sql_from = " FROM videos v"
    sql_where = ""
    sql_order = ""
    params = []
    
    # Add search condition if query provided
    if query:
        sql_from += " JOIN videos_fts fts ON v.id = fts.docid"
        sql_where = " WHERE videos_fts MATCH ?"
        params.append(query)
        
        # Custom snippets for transcript and summary
        sql_select += """,
            (SELECT substr(v.transcript, 
                max(0, instr(lower(v.transcript), lower(?)) - 50), 
                150)) AS transcript_snippet,
            (SELECT substr(v.summary, 
                max(0, instr(lower(v.summary), lower(?)) - 25), 
                100)) AS summary_snippet
        """
        params.extend([query, query])  # Add query params for snippets
        
        # Enhanced ordering that prioritizes matches in summary, then caption
        sql_order = """
        ORDER BY
            CASE 
                WHEN fts.summary MATCH ? THEN 1
                WHEN fts.caption MATCH ? THEN 2
                WHEN fts.transcript MATCH ? THEN 3
                ELSE 4
            END
        """
        params.extend([query, query, query])
    
    # Add account filter if specified
    if account:
        if sql_where:
            sql_where += " AND v.account = ?"
        else:
            sql_where = " WHERE v.account = ?"
        params.append(account)
    
    # Add tag filter if specified
    if tag:
        sql_from += " JOIN tags t ON v.id = t.video_id"
        if sql_where:
            sql_where += " AND t.tag = ?"
        else:
            sql_where = " WHERE t.tag = ?"
        params.append(tag)
    
    # Count total results for pagination
    count_sql = f"SELECT COUNT(*) as total_count {sql_from}{sql_where}"
    cursor.execute(count_sql, params)
    total_count = cursor.fetchone()['total_count']
    
    # Calculate total pages
    total_pages = (total_count + per_page - 1) // per_page
    
    # Add default sorting if no query or specific ordering
    if not sql_order:
        sql_order = " ORDER BY v.timestamp DESC"
    
    # Add pagination
    sql_limit = " LIMIT ? OFFSET ?"
    
    # Complete SQL query
    full_sql = f"{sql_select}{sql_from}{sql_where}{sql_order}{sql_limit}"
    
    # Execute with pagination parameters
    cursor.execute(full_sql, params + [per_page, (page - 1) * per_page])
    results = cursor.fetchall()
    
    # Convert to list of dicts with highlighted snippets
    result_list = []
    for row in results:
        result_dict = {key: row[key] for key in row.keys()}
        
        # Add highlighted snippets if search was performed
        if query:
            # Highlight the query term in snippets
            if 'transcript_snippet' in result_dict and result_dict['transcript_snippet']:
                result_dict['transcript_snippet'] = highlight_term(
                    result_dict['transcript_snippet'], query
                )
            
            if 'summary_snippet' in result_dict and result_dict['summary_snippet']:
                result_dict['summary_snippet'] = highlight_term(
                    result_dict['summary_snippet'], query
                )
        
        result_list.append(result_dict)
    
    return jsonify({
        'results': result_list,
        'total': total_count,
        'page': page,
        'per_page': per_page,
        'total_pages': total_pages,
        'query': query,
        'account': account,
        'tag': tag
    })

def highlight_term(text, term):
    """Highlight search term in text using HTML"""
    if not text or not term:
        return text
    
    # Simple case-insensitive replace
    # For more complex highlighting, consider using regex
    term_lower = term.lower()
    text_lower = text.lower()
    
    result = ""
    last_pos = 0
    
    # Find all occurrences of the term
    pos = text_lower.find(term_lower)
    while pos != -1:
        # Add text before the term
        result += text[last_pos:pos]
        # Add the highlighted term
        result += f"<mark>{text[pos:pos+len(term)]}</mark>"
        # Move past this occurrence
        last_pos = pos + len(term)
        # Find next occurrence
        pos = text_lower.find(term_lower, last_pos)
    
    # Add any remaining text
    result += text[last_pos:]
    
    return result

@app.route('/media/<path:path>')
def media(path):
    """Serve media files"""
    return send_from_directory(DOWNLOAD_DIR, path)

# Routes for static templates
@app.route('/about')
def about():
    """About page"""
    return render_template('about.html')

@app.route('/stats')
def stats():
    """Statistics page"""
    db = get_db()
    cursor = db.cursor()
    
    # Get general stats
    cursor.execute("SELECT COUNT(*) as total FROM videos")
    total_videos = cursor.fetchone()['total']
    
    # Get account stats
    cursor.execute('''
    SELECT account, COUNT(*) as count 
    FROM videos 
    GROUP BY account 
    ORDER BY count DESC
    ''')
    accounts = cursor.fetchall()
    
    # Get tag stats
    cursor.execute('''
    SELECT tag, COUNT(*) as count 
    FROM tags 
    GROUP BY tag 
    ORDER BY count DESC 
    LIMIT 50
    ''')
    tags = cursor.fetchall()
    
    # Get timeline stats
    cursor.execute('''
    SELECT substr(timestamp, 1, 7) as month, COUNT(*) as count 
    FROM videos 
    WHERE timestamp IS NOT NULL
    GROUP BY month 
    ORDER BY month
    ''')
    timeline = cursor.fetchall()
    
    return render_template(
        'stats.html',
        total_videos=total_videos,
        accounts=accounts,
        tags=tags,
        timeline=timeline
    )

# Caching utilities
@lru_cache(maxsize=100)
def get_video_by_shortcode(shortcode):
    """Get video details by shortcode with caching"""
    db = get_db()
    cursor = db.cursor()
    cursor.execute("SELECT * FROM videos WHERE shortcode = ?", [shortcode])
    return cursor.fetchone()

@lru_cache(maxsize=30)
def get_recent_videos(limit=10, account=None):
    """Get recent videos with caching"""
    db = get_db()
    cursor = db.cursor()
    
    if account:
        cursor.execute(
            "SELECT * FROM videos WHERE account = ? ORDER BY timestamp DESC LIMIT ?", 
            [account, limit]
        )
    else:
        cursor.execute(
            "SELECT * FROM videos ORDER BY timestamp DESC LIMIT ?", 
            [limit]
        )
    
    return cursor.fetchall()

@lru_cache(maxsize=20)
def get_video_statistics():
    """Get video statistics with caching"""
    db = get_db()
    cursor = db.cursor()
    
    # Get total videos
    cursor.execute("SELECT COUNT(*) as video_count FROM videos")
    total_videos = cursor.fetchone()['video_count']
    
    # Get videos per account
    cursor.execute(
        "SELECT account, COUNT(*) as count FROM videos GROUP BY account ORDER BY count DESC"
    )
    accounts = cursor.fetchall()
    
    # Get total duration (if available)
    cursor.execute(
        "SELECT SUM(duration_seconds) as total_duration FROM videos WHERE duration_seconds IS NOT NULL"
    )
    total_duration = cursor.fetchone()['total_duration'] or 0
    
    # Get total word count
    cursor.execute(
        "SELECT SUM(word_count) as total_words FROM videos WHERE word_count IS NOT NULL"
    )
    total_words = cursor.fetchone()['total_words'] or 0
    
    return {
        'total_videos': total_videos,
        'accounts': accounts,
        'total_duration_seconds': total_duration,
        'total_words': total_words
    }

# Clear caches when data changes
def clear_caches():
    """Clear all LRU caches"""
    get_video_by_shortcode.cache_clear()
    get_recent_videos.cache_clear()
    get_video_statistics.cache_clear()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=WEB_PORT, debug=DEBUG_MODE) 

================================================================================
File: arxiv_collector.py
================================================================================

"""
Module for collecting and processing AI/ML research papers from ArXiv
"""
import os
import json
import time
import logging
import sqlite3
import requests
import feedparser
from datetime import datetime, timedelta
import PyPDF2
import io
import config

# Configure logging
log_dir = os.path.join(config.DATA_DIR, 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'arxiv_collector.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('arxiv_collector')

def setup_directories():
    """Create necessary directories for storing paper data"""
    papers_dir = os.path.join(config.DATA_DIR, "papers")
    papers_pdf_dir = os.path.join(papers_dir, "pdfs")
    papers_text_dir = os.path.join(papers_dir, "text")
    
    os.makedirs(papers_dir, exist_ok=True)
    os.makedirs(papers_pdf_dir, exist_ok=True)
    os.makedirs(papers_text_dir, exist_ok=True)
    
    return papers_dir, papers_pdf_dir, papers_text_dir

def extract_text_from_pdf(pdf_path):
    """Extract text content from a PDF file"""
    try:
        text = ""
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            num_pages = len(reader.pages)
            
            # Extract text from each page
            for page_num in range(num_pages):
                page = reader.pages[page_num]
                text += page.extract_text() + "\n\n"
                
            logger.info(f"Extracted text from {pdf_path} ({num_pages} pages)")
        return text
    except Exception as e:
        logger.error(f"Error extracting text from PDF {pdf_path}: {str(e)}")
        return None

def parse_sections(text):
    """Attempt to parse sections from extracted PDF text"""
    sections = {
        "abstract": "",
        "introduction": "",
        "methodology": "",
        "results": "",
        "conclusion": "",
        "references": ""
    }
    
    if not text:
        return sections
        
    # Simple heuristic section detection
    lines = text.split('\n')
    current_section = "abstract"
    
    for line in lines:
        line = line.strip()
        line_lower = line.lower()
        
        # Check for section headers
        if "abstract" in line_lower and len(line) < 30:
            current_section = "abstract"
            continue
        elif any(x in line_lower for x in ["introduction", "background"]) and len(line) < 30:
            current_section = "introduction"
            continue
        elif any(x in line_lower for x in ["method", "approach", "model", "implementation"]) and len(line) < 30:
            current_section = "methodology"
            continue
        elif any(x in line_lower for x in ["result", "evaluation", "experiment", "performance"]) and len(line) < 30:
            current_section = "results"
            continue
        elif any(x in line_lower for x in ["conclusion", "discussion", "future work"]) and len(line) < 30:
            current_section = "conclusion"
            continue
        elif any(x in line_lower for x in ["reference", "bibliography"]) and len(line) < 30:
            current_section = "references"
            continue
            
        # Add text to current section
        if line and current_section in sections:
            sections[current_section] += line + "\n"
    
    # Clean up sections
    for section in sections:
        if len(sections[section]) > 100000:  # Limit section size
            sections[section] = sections[section][:100000] + "... [truncated]"
    
    return sections

def collect_papers(max_papers=None, force_update=False):
    """
    Collect papers from ArXiv based on configured topics
    
    Args:
        max_papers: Maximum number of papers to collect (None for no limit)
        force_update: Whether to force update of existing papers
        
    Returns:
        Number of new papers added
    """
    papers_dir, papers_pdf_dir, papers_text_dir = setup_directories()
    
    # Connect to database
    conn = sqlite3.connect(config.DB_PATH)
    
    # Get source type ID for 'research_paper'
    cursor = conn.cursor()
    cursor.execute("SELECT id FROM source_types WHERE name = 'research_paper'")
    result = cursor.fetchone()
    if result:
        source_type_id = result[0]
    else:
        logger.error("Source type 'research_paper' not found in database")
        conn.close()
        return 0
    
    papers_processed = 0
    papers_added = 0
    paper_topics = config.RESEARCH_PAPER_CONFIG.get('topics', [
        "large language models",
        "diffusion models",
        "transformers", 
        "generative ai",
        "reinforcement learning",
        "computer vision"
    ])
    
    max_papers_per_topic = config.RESEARCH_PAPER_CONFIG.get('max_papers_per_topic', 5)
    max_age_days = config.RESEARCH_PAPER_CONFIG.get('max_age_days', 60)
    
    # Adjust max_papers_per_topic if max_papers is specified
    if max_papers and len(paper_topics) > 0:
        max_papers_per_topic = min(max_papers_per_topic, max_papers // len(paper_topics) + 1)
    
    # Process each configured topic
    for topic in paper_topics:
        logger.info(f"Collecting papers for topic: {topic}")
        
        # Construct ArXiv API query
        query = f'all:"{topic}" AND (cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV)'
        url = f"http://export.arxiv.org/api/query?search_query={query.replace(' ', '+')}&sortBy=submittedDate&sortOrder=descending&max_results={max_papers_per_topic}"
        
        try:
            # Get papers from ArXiv
            response = feedparser.parse(url)
            
            if not response.entries:
                logger.warning(f"No papers found for topic: {topic}")
                continue
                
            logger.info(f"Found {len(response.entries)} papers for topic: {topic}")
            
            # Process each paper
            for entry in response.entries:
                # Debug to see the structure of the entry
                entry_keys = entry.keys()
                logger.debug(f"Available keys in entry: {entry_keys}")
                
                try:
                    # Safe extraction of paper ID
                    paper_id = entry.id.split('/abs/')[-1] if hasattr(entry, 'id') else f"unknown_{time.time()}"
                    papers_processed += 1
                    
                    if max_papers and papers_added >= max_papers:
                        logger.info(f"Reached maximum paper limit: {max_papers}")
                        break
                    
                    # Check if paper already exists
                    cursor.execute(
                        "SELECT id, last_crawled FROM research_papers WHERE doi = ?", 
                        (paper_id,)
                    )
                    existing = cursor.fetchone()
                    
                    # Skip if paper exists and was recently updated (unless force_update is True)
                    if existing and not force_update:
                        if existing[1]:  # Check if last_crawled is not None
                            try:
                                last_updated = datetime.fromisoformat(existing[1].replace('Z', '+00:00'))
                                if (datetime.now() - last_updated).days < 30:
                                    logger.debug(f"Skipping recent paper: {paper_id}")
                                    continue
                            except (ValueError, AttributeError):
                                # If date parsing fails, process the paper anyway
                                pass
                    
                    # Safe extraction of publication date
                    published = datetime.now()  # Default to current time
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            published_date = entry.published_parsed
                            published = datetime(
                                year=published_date[0],
                                month=published_date[1],
                                day=published_date[2],
                                hour=published_date[3],
                                minute=published_date[4],
                                second=published_date[5]
                            )
                        except (IndexError, TypeError, ValueError) as e:
                            logger.warning(f"Error parsing published date for {paper_id}: {str(e)}")
                    
                    if not force_update and (datetime.now() - published).days > max_age_days:
                        logger.debug(f"Skipping old paper: {paper_id} ({published})")
                        continue
                    
                    # Safe extraction of paper details
                    title = entry.get('title', f"Unknown Paper {paper_id}")
                    
                    # Safe extraction of authors
                    authors = ""
                    if hasattr(entry, 'authors'):
                        try:
                            authors = ", ".join(author.get('name', '') for author in entry.authors)
                        except (AttributeError, TypeError) as e:
                            logger.warning(f"Error extracting authors for {paper_id}: {str(e)}")
                    
                    # Safe extraction of summary
                    summary = entry.get('summary', "No abstract available")
                    
                    # Get PDF link
                    pdf_link = None
                    if hasattr(entry, 'links'):
                        for link in entry.links:
                            if hasattr(link, 'rel') and hasattr(link, 'type'):
                                if link.rel == 'alternate' and link.type == 'application/pdf':
                                    pdf_link = link.href
                                    break
                            if hasattr(link, 'title') and link.title == 'pdf':
                                pdf_link = link.href
                                break
                    
                    # Extract article URL
                    article_url = None
                    if hasattr(entry, 'links'):
                        for link in entry.links:
                            if hasattr(link, 'rel') and hasattr(link, 'type'):
                                if link.rel == 'alternate' and link.type == 'text/html':
                                    article_url = link.href
                                    break
                    
                    if not article_url:
                        article_url = f"https://arxiv.org/abs/{paper_id}"
                    
                    # Extract PDF content if available
                    sections = {"abstract": summary}
                    pdf_path = None
                    extracted_text = None
                    
                    if pdf_link:
                        try:
                            # Save PDF locally
                            pdf_path = os.path.join(papers_pdf_dir, f"{paper_id.replace('/', '_')}.pdf")
                            if not os.path.exists(pdf_path) or force_update:
                                logger.info(f"Downloading PDF for {paper_id} from {pdf_link}")
                                
                                # Add a delay to be respectful of the API
                                time.sleep(2)
                                
                                pdf_response = requests.get(pdf_link, timeout=30)
                                with open(pdf_path, 'wb') as f:
                                    f.write(pdf_response.content)
                                logger.info(f"Downloaded PDF for {paper_id}")
                            
                            # Extract text from PDF
                            extracted_text = extract_text_from_pdf(pdf_path)
                            if extracted_text:
                                # Save extracted text
                                text_path = os.path.join(papers_text_dir, f"{paper_id.replace('/', '_')}.txt")
                                with open(text_path, 'w', encoding='utf-8') as f:
                                    f.write(extracted_text)
                                
                                # Parse sections
                                sections = parse_sections(extracted_text)
                                if not sections["abstract"].strip():
                                    sections["abstract"] = summary
                            
                        except Exception as e:
                            logger.error(f"Error processing PDF for {paper_id}: {str(e)}")
                    
                    # Calculate publication year
                    publication_year = published.year
                    
                    # Prepare data for database
                    if existing:
                        # Update existing paper record
                        cursor.execute("""
                            UPDATE research_papers
                            SET title = ?, authors = ?, abstract = ?, 
                                url = ?, pdf_path = ?, content = ?, last_crawled = ?
                            WHERE id = ?
                        """, (
                            title,
                            authors,
                            sections["abstract"],
                            article_url,
                            pdf_path if pdf_path else None,
                            extracted_text,
                            datetime.now().isoformat(),
                            existing[0]
                        ))
                        
                        # Update ai_content record
                        cursor.execute("""
                            UPDATE ai_content
                            SET title = ?, description = ?, content = ?,
                                url = ?, date_collected = ?, metadata = ?
                            WHERE source_type_id = ? AND source_id = ?
                        """, (
                            title,
                            sections["abstract"],
                            extracted_text if extracted_text else sections["abstract"],
                            article_url,
                            datetime.now().isoformat(),
                            json.dumps({
                                "authors": authors,
                                "published_date": published.isoformat(),
                                "year": publication_year,
                                "sections": sections
                            }),
                            source_type_id,
                            str(existing[0])
                        ))
                        
                        logger.info(f"Updated paper: {title}")
                    else:
                        # Insert into research_papers
                        cursor.execute("""
                            INSERT INTO research_papers (
                                title, authors, abstract, publication, year,
                                url, doi, pdf_path, content, last_crawled
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            title,
                            authors,
                            sections["abstract"],
                            "arXiv",  # Publication name
                            publication_year,
                            article_url,
                            paper_id,  # Using arXiv ID as DOI
                            pdf_path if pdf_path else None,
                            extracted_text,
                            datetime.now().isoformat()
                        ))
                        
                        # Get the inserted ID
                        paper_db_id = cursor.lastrowid
                        
                        # Insert into ai_content
                        cursor.execute("""
                            INSERT INTO ai_content (
                                source_type_id, source_id, title, description, content,
                                url, date_created, date_collected, metadata, is_indexed
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            source_type_id,
                            str(paper_db_id),
                            title,
                            sections["abstract"],
                            extracted_text if extracted_text else sections["abstract"],
                            article_url,
                            published.isoformat(),
                            datetime.now().isoformat(),
                            json.dumps({
                                "authors": authors,
                                "published_date": published.isoformat(),
                                "year": publication_year,
                                "sections": sections
                            }),
                            0  # Not indexed yet
                        ))
                        
                        papers_added += 1
                        logger.info(f"Added new paper: {title}")
                    
                    conn.commit()
                
                except Exception as e:
                    logger.error(f"Error processing paper entry: {str(e)}")
                
                # Add a delay to be respectful of the API
                time.sleep(3)
                
        except Exception as e:
            logger.error(f"Error collecting papers for topic {topic}: {str(e)}")
            continue
    
    conn.close()
    logger.info(f"Paper collection complete. Processed {papers_processed} papers, added {papers_added} new papers.")
    return papers_added

if __name__ == "__main__":
    collect_papers() 

================================================================================
File: collectCode.js
================================================================================

import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

// Default configuration for file collection
const DEFAULT_CONFIG = {
  extensions: ['.ts', '.tsx', '.js', '.md', '.json', '.py', '.sql'],
  excludeDirs: [
  '.expo',
  'android',
  'CanovaReactNativeApp/app/dataOverviews',
  'assets',
  'data',
  'ios',
  'node_modules',
  'static',
  'templates',
  'venv',
  
  
    
   
  ],
  excludeFiles: [
    '.test.',
    '.spec.',
    '.d.ts',
    '.map',
    'next-env.d.ts',
    '.gitignore',
    '.eslintrc.json',
    '.env.example',
    'components.json',
    'package-lock.json',
  ],
  excludePaths: [
    'lib/supabase.ts',
    'lib/rate-limit.ts',
    'lib/monitoring.ts',
    'lib/cache.ts',
    'lib/auth.ts',
    'scripts/test-agent1.ts',
    'scripts/test-agent2.ts',
    'scripts/test-agent3.ts',
    'scripts/test-orchestrator.ts',
    'lib/graphql/queries.ts',
    'lib/graphql/client.ts',
    'hooks/use-toast.ts',
    'components/WalletConnect.tsx',
    'components/Learn.tsx',
    'components/ContractInteraction.tsx',
    'components/Analytics.tsx',
    'app/providers.tsx',
    'app/page.tsx',
    'app/globals.css',
    'app/layout.tsx',


    'CanovaReactNativeApp/tsconfig.json',
    'CanovaReactNativeApp/app.json',
    
  ],
  maxFileSize: 1024 * 1024, // 1MB
};

// Helper function to get relative path
function getRelativePath(fullPath, rootDir) {
  return path.relative(rootDir, fullPath);
}

// Check if a file should be excluded based on config
function shouldExcludeFile(filePath, config) {
  const normalizedPath = path.normalize(filePath);

  if (config.excludeFiles.some((pattern) => normalizedPath.includes(pattern))) {
    return true;
  }

  if (config.excludePaths.some((excludePath) =>
    normalizedPath.includes(path.normalize(excludePath))
  )) {
    return true;
  }

  return false;
}

// Check if a directory should be excluded based on config
function shouldExcludeDir(dirPath, config) {
  const normalizedPath = path.normalize(dirPath);
  return config.excludeDirs.some((excludeDir) =>
    normalizedPath.includes(path.normalize(excludeDir))
  );
}

// Recursively collect files from directory
function collectFiles(dir, rootDir, config) {
  let results = [];
  const items = fs.readdirSync(dir, { withFileTypes: true });

  for (const item of items) {
    const fullPath = path.join(dir, item.name);
    const relativePath = getRelativePath(fullPath, rootDir);

    if (item.isDirectory()) {
      if (!shouldExcludeDir(fullPath, config)) {
        results = results.concat(collectFiles(fullPath, rootDir, config));
      }
    } else {
      const ext = path.extname(item.name).toLowerCase();

      if (config.extensions.includes(ext) && !shouldExcludeFile(relativePath, config)) {
        const stats = fs.statSync(fullPath);
        if (stats.size <= config.maxFileSize) {
          results.push({ path: fullPath, relativePath });
        }
      }
    }
  }

  return results;
}

// Main function to collect code
function collectCode(outputFile, customConfig = {}) {
  const config = { ...DEFAULT_CONFIG, ...customConfig };

  try {
    fs.writeFileSync(outputFile, '');

    const rootDir = process.cwd();
    console.log(`Processing project directory: ${rootDir}`);
    const files = collectFiles(rootDir, rootDir, config);

    files.sort((a, b) => a.relativePath.localeCompare(b.relativePath));

    files.forEach(({ path: filePath, relativePath }) => {
      const content = fs.readFileSync(filePath, 'utf8');
      const separator = '='.repeat(80);
      fs.appendFileSync(
        outputFile,
        `\n\n${separator}\nFile: ${relativePath}\n${separator}\n\n${content}`
      );
    });

    console.log('Collection complete!');
  } catch (error) {
    console.error('Error during collection:', error);
    process.exit(1);
  }
}

// Check if running as main module
const isMainModule = process.argv[1] === fileURLToPath(import.meta.url);

if (isMainModule) {
  const outputFile = process.argv[2] || 'code-collection.txt';
  collectCode(outputFile);
}

export { collectCode }; 

================================================================================
File: concept_extractor.py
================================================================================

"""
Concept extractor module for identifying and extracting AI concepts from content

This module uses anthropic.claude to analyze content from various sources
(research papers, GitHub repositories, Instagram videos) and extract
AI/ML concepts, creating a structured knowledge graph of concepts.
"""
import os
import json
import time
import logging
import sqlite3
from datetime import datetime
import anthropic
import config

# Configure logging
log_dir = os.path.join(config.DATA_DIR, 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'concept_extractor.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('concept_extractor')

# Set up the Anthropic Claude client
client = anthropic.Anthropic(api_key=config.ANTHROPIC_API_KEY)

def extract_concepts_from_text(text, content_type, title="", context=""):
    """
    Extract AI concepts from text content using Claude
    
    Args:
        text (str): The text content to analyze
        content_type (str): The type of content (research_paper, github, instagram)
        title (str): Title of the content
        context (str): Additional context or metadata
        
    Returns:
        dict: Dictionary of extracted concepts
    """
    if not text or len(text.strip()) < 50:
        logger.warning(f"Text too short for concept extraction: {text[:50]}...")
        return {"concepts": [], "relationships": []}
    
    # Truncate text if it's too long
    max_length = 20000  # Claude can handle larger texts, but we'll keep it reasonable
    if len(text) > max_length:
        logger.info(f"Truncating text from {len(text)} to {max_length} characters")
        text = text[:max_length] + "..."
    
    # Prepare prompt based on content type
    type_context = {
        "research_paper": "This is content from a research paper on AI/ML.",
        "github": "This is content from a GitHub repository related to AI/ML.",
        "instagram": "This is a transcript or summary from an Instagram video about AI/ML."
    }
    
    content_context = type_context.get(content_type, "This is AI/ML related content.")
    
    prompt = f"""
    {content_context}
    Title: {title}
    Context: {context}
    
    I need you to analyze the following content and extract key AI/ML concepts. For each concept:
    1. Provide a short description
    2. Identify related concepts
    3. Categorize it (e.g., model architecture, training technique, dataset, evaluation metric)
    4. Assess its importance in the content (high/medium/low)
    
    Your response should be a well-structured JSON with the following format:
    {{
        "concepts": [
            {{
                "name": "concept name",
                "description": "brief description",
                "category": "category",
                "importance": "high/medium/low",
                "related_concepts": ["related concept 1", "related concept 2"]
            }}
        ],
        "relationships": [
            {{
                "source": "concept1",
                "target": "concept2",
                "relationship_type": "uses/is_part_of/improves/etc"
            }}
        ]
    }}
    
    Focus on technical AI/ML concepts only. Identify between 5-15 concepts depending on content length and density.
    
    Content to analyze:
    {text}
    """
    
    try:
        # Call Claude API
        message = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=4000,
            temperature=0,
            system="You are an AI expert who specializes in extracting and organizing AI/ML concepts from technical content.",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        # Extract JSON from response
        response_text = message.content[0].text
        
        # Try to parse the JSON
        try:
            # Find JSON in the response (might be wrapped in markdown code blocks)
            json_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
            import re
            json_match = re.search(json_pattern, response_text)
            
            if json_match:
                json_str = json_match.group(1)
                concepts_data = json.loads(json_str)
            else:
                # Try parsing the whole response as JSON
                concepts_data = json.loads(response_text)
                
            # Validate the structure
            if not isinstance(concepts_data, dict) or "concepts" not in concepts_data:
                raise ValueError("Invalid JSON structure")
                
            return concepts_data
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Failed to parse JSON from Claude: {str(e)}")
            logger.debug(f"Claude response: {response_text}")
            return {"concepts": [], "relationships": []}
    
    except Exception as e:
        logger.error(f"Error calling Claude API: {str(e)}")
        return {"concepts": [], "relationships": []}

def store_concepts(content_id, source_type_id, concepts_data):
    """
    Store extracted concepts in the database
    
    Args:
        content_id (int): ID of the content in ai_content table
        source_type_id (int): Source type ID
        concepts_data (dict): Dictionary containing concepts and relationships
        
    Returns:
        bool: Success or failure
    """
    # Connect to database
    conn = sqlite3.connect(config.DB_PATH)
    cursor = conn.cursor()
    
    try:
        # Check if concepts table exists
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='concepts'")
        has_concepts_table = cursor.fetchone() is not None
        
        # Create tables if they don't exist
        if not has_concepts_table:
            # Create concepts table
            cursor.execute("""
            CREATE TABLE concepts (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                category TEXT,
                UNIQUE(name)
            )
            """)
            
            # Create content_concepts table for many-to-many relationships
            cursor.execute("""
            CREATE TABLE content_concepts (
                id INTEGER PRIMARY KEY,
                content_id INTEGER,
                concept_id INTEGER,
                importance TEXT,
                metadata TEXT,
                UNIQUE(content_id, concept_id),
                FOREIGN KEY(content_id) REFERENCES ai_content(id),
                FOREIGN KEY(concept_id) REFERENCES concepts(id)
            )
            """)
            
            # Create concept relationships table
            cursor.execute("""
            CREATE TABLE concept_relationships (
                id INTEGER PRIMARY KEY,
                source_concept_id INTEGER,
                target_concept_id INTEGER,
                relationship_type TEXT,
                UNIQUE(source_concept_id, target_concept_id, relationship_type),
                FOREIGN KEY(source_concept_id) REFERENCES concepts(id),
                FOREIGN KEY(target_concept_id) REFERENCES concepts(id)
            )
            """)
            
            logger.info("Created concepts tables")
        
        # Store each concept
        concept_ids = {}
        for concept in concepts_data.get("concepts", []):
            name = concept.get("name", "").strip()
            if not name:
                continue
                
            # Check if concept already exists
            cursor.execute("SELECT id FROM concepts WHERE name = ?", (name,))
            result = cursor.fetchone()
            
            if result:
                concept_id = result[0]
            else:
                # Insert new concept
                cursor.execute("""
                INSERT INTO concepts (name, description, category) 
                VALUES (?, ?, ?)
                """, (
                    name,
                    concept.get("description", ""),
                    concept.get("category", "")
                ))
                concept_id = cursor.lastrowid
            
            concept_ids[name] = concept_id
            
            # Link concept to content
            try:
                cursor.execute("""
                INSERT INTO content_concepts (content_id, concept_id, importance, metadata)
                VALUES (?, ?, ?, ?)
                """, (
                    content_id,
                    concept_id,
                    concept.get("importance", "medium"),
                    json.dumps({
                        "related_concepts": concept.get("related_concepts", [])
                    })
                ))
            except sqlite3.IntegrityError:
                # Update existing link
                cursor.execute("""
                UPDATE content_concepts 
                SET importance = ?, metadata = ?
                WHERE content_id = ? AND concept_id = ?
                """, (
                    concept.get("importance", "medium"),
                    json.dumps({
                        "related_concepts": concept.get("related_concepts", [])
                    }),
                    content_id,
                    concept_id
                ))
        
        # Store relationships
        for relationship in concepts_data.get("relationships", []):
            source = relationship.get("source", "").strip()
            target = relationship.get("target", "").strip()
            rel_type = relationship.get("relationship_type", "related_to").strip()
            
            if not source or not target or source not in concept_ids or target not in concept_ids:
                continue
                
            source_id = concept_ids[source]
            target_id = concept_ids[target]
            
            try:
                cursor.execute("""
                INSERT INTO concept_relationships (source_concept_id, target_concept_id, relationship_type)
                VALUES (?, ?, ?)
                """, (source_id, target_id, rel_type))
            except sqlite3.IntegrityError:
                # Relationship already exists
                pass
        
        # Update ai_content to mark as processed for concepts
        cursor.execute("""
        UPDATE ai_content 
        SET metadata = json.set(metadata, '$.concepts_extracted', 1),
            date_indexed = ?
        WHERE id = ?
        """, (datetime.now().isoformat(), content_id))
        
        conn.commit()
        return True
        
    except Exception as e:
        logger.error(f"Error storing concepts: {str(e)}")
        conn.rollback()
        return False
        
    finally:
        conn.close()

def process_unprocessed_content(limit=5, source_type=None):
    """
    Process content that hasn't had concepts extracted yet
    
    Args:
        limit (int): Maximum number of items to process
        source_type (str, optional): If provided, only process this type of content
        
    Returns:
        int: Number of items processed
    """
    conn = sqlite3.connect(config.DB_PATH)
    cursor = conn.cursor()
    
    try:
        # Get source type IDs
        source_type_map = {}
        cursor.execute("SELECT id, name FROM source_types")
        for row in cursor.fetchall():
            source_type_map[row[1]] = row[0]
        
        # Build query
        query = """
        SELECT c.id, c.source_type_id, c.title, c.content, c.description, c.metadata
        FROM ai_content c
        WHERE 
            (json_extract(c.metadata, '$.concepts_extracted') IS NULL OR 
             json_extract(c.metadata, '$.concepts_extracted') = 0)
            AND c.content IS NOT NULL AND length(c.content) > 100
        """
        
        params = []
        if source_type and source_type in source_type_map:
            query += " AND c.source_type_id = ?"
            params.append(source_type_map[source_type])
        
        query += " ORDER BY c.date_collected DESC LIMIT ?"
        params.append(limit)
        
        cursor.execute(query, params)
        items = cursor.fetchall()
        
        processed_count = 0
        for item in items:
            content_id, source_type_id, title, content, description, metadata_json = item
            
            # Determine source type name
            source_type_name = next((name for name, id_val in source_type_map.items() if id_val == source_type_id), "unknown")
            
            # Parse metadata
            try:
                metadata = json.loads(metadata_json) if metadata_json else {}
            except:
                metadata = {}
            
            # Different handling based on content type
            context = ""
            if source_type_name == "github":
                context = f"GitHub repository: {metadata.get('full_name', '')}"
                # Use description for context if content is README
                if description:
                    context += f"\nDescription: {description}"
                
            elif source_type_name == "research_paper":
                authors = metadata.get("authors", "")
                year = metadata.get("year", "")
                context = f"Authors: {authors}\nYear: {year}"
                
                # Use abstract for context if we're processing full text
                if description and len(content) > len(description)*3:
                    context += f"\nAbstract: {description[:500]}..."
            
            # Extract concepts
            logger.info(f"Extracting concepts from {source_type_name} content: {title}")
            concepts_data = extract_concepts_from_text(
                content,
                source_type_name,
                title=title,
                context=context
            )
            
            # Store concepts
            if concepts_data and concepts_data.get("concepts"):
                logger.info(f"Found {len(concepts_data['concepts'])} concepts in {title}")
                if store_concepts(content_id, source_type_id, concepts_data):
                    processed_count += 1
            else:
                logger.warning(f"No concepts found in {title}")
                # Mark as processed even if no concepts found
                cursor.execute("""
                UPDATE ai_content 
                SET metadata = json.set(metadata, '$.concepts_extracted', 1),
                    date_indexed = ?
                WHERE id = ?
                """, (datetime.now().isoformat(), content_id))
                conn.commit()
                processed_count += 1
                
            # Sleep to avoid rate limiting
            time.sleep(2)
        
        return processed_count
    
    except Exception as e:
        logger.error(f"Error processing content for concept extraction: {str(e)}")
        return 0
    
    finally:
        conn.close()

if __name__ == "__main__":
    # Process some content from each source type
    for source in ["research_paper", "github", "instagram"]:
        count = process_unprocessed_content(limit=3, source_type=source)
        logger.info(f"Processed {count} items from {source}") 

================================================================================
File: config.py
================================================================================

"""
Configuration file for Instagram Knowledge Base
"""
import os

# Content sources configuration
# Currently supports Instagram, with framework for adding more sources
CONTENT_SOURCES = {
    'instagram': {
        'enabled': True,
        'accounts': [
            'example_account1',
            'example_account2',
            'example_account3',
        ]
    },
    'github': {
        'enabled': True,
        'rate_limit': 5000,  # GitHub API has a rate limit of 5000 requests per hour for authenticated requests
        'max_repos_per_run': 10,
        'api_token': '',  # Add your GitHub API token here to increase rate limits
        'topics': [
            'machine-learning',
            'artificial-intelligence',
            'deep-learning',
            'data-science',
            'nlp',
            'computer-vision',
            'reinforcement-learning'
        ],
        'repo_stars_minimum': 1000,  # Minimum number of stars for a repository to be considered
        'cache_ttl_hours': 24  # How long to cache GitHub API results in hours
    },
    'research_papers': {
        'enabled': True,
        'max_papers_per_run': 10,
        'sources': ['arxiv']  # Currently only supports ArXiv
    }
}

# Instagram accounts list (for backward compatibility)
INSTAGRAM_ACCOUNTS = [account for account in CONTENT_SOURCES['instagram']['accounts']]

# Instagram credentials (only needed for private accounts)
# IMPORTANT: Add your Instagram credentials here to avoid 401 Unauthorized errors
# You can either set environment variables or directly add your credentials below:
# INSTAGRAM_USERNAME = "your_instagram_username"
# INSTAGRAM_PASSWORD = "your_instagram_password"
INSTAGRAM_USERNAME = os.getenv("INSTAGRAM_USERNAME", "")
INSTAGRAM_PASSWORD = os.getenv("INSTAGRAM_PASSWORD", "")

# Instagram credentials in a dictionary format
INSTAGRAM_CREDENTIALS = {
    'username': INSTAGRAM_USERNAME,
    'password': INSTAGRAM_PASSWORD
}

# Multiple Instagram accounts for rotation (to reduce rate limiting)
# If using account rotation, populate this list with your accounts
INSTAGRAM_ACCOUNT_ROTATION = {
    'enabled': False,  # Disabled due to 2FA requirements
    'accounts': [
        {'username': 'account1', 'password': 'password1', 'last_used': 0, 'failed_attempts': 0},
        {'username': 'account2', 'password': 'password2', 'last_used': 0, 'failed_attempts': 0}
    ],
    'failure_threshold': 3,  # Number of consecutive failures before an account is marked for cooldown
    'cooldown_hours': 24,    # Hours to wait before trying a failed account again
    'min_rotation_interval': 10  # Minimum time in minutes between account rotations
}

# Proxy configuration
# Add your proxy servers here to rotate IPs and reduce rate limiting
PROXY_SERVERS = [
    "http://brd-customer-hl_c7bff232-zone-residential_proxy1-country-us:w46vs0z46xmc@brd.superproxy.io:33335",
    # Add more proxies as needed
]

# Configure if you want to use a specific country for proxies (US, UK, etc.)
PROXY_COUNTRY = "us"  # Change as needed, or set to None for random

# Proxy configuration in dict format
PROXY_CONFIG = {
    'enabled': True if PROXY_SERVERS else False,
    'type': 'rotating',  # 'rotating' or 'fixed'
    'test_url': 'https://geo.brdtest.com',
    'protocol': 'http',  # 'http', 'https', 'socks4', 'socks5'
    'host': 'brd.superproxy.io',
    'port': 33335,
    'username': 'brd-customer-hl_c7bff232-zone-residential_proxy1-country-us',
    'password': 'w46vs0z46xmc',
    'timeout': 10  # Seconds
}

# Claude API key for summarization
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")

# Directory settings
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
DOWNLOAD_DIR = os.path.join(DATA_DIR, "downloads")
AUDIO_DIR = os.path.join(DATA_DIR, "audio")
TRANSCRIPT_DIR = os.path.join(DATA_DIR, "transcripts")

# Whisper model size: tiny, base, small, medium, large
WHISPER_MODEL = "base"

# Database settings
DB_PATH = os.path.join(DATA_DIR, "knowledge_base.db")

# Web interface settings
WEB_PORT = 5001
DEBUG_MODE = True

# Rate limiting settings (to avoid IP blocks)
DOWNLOAD_DELAY = 10  # seconds between downloads (increased from 5 to reduce rate limiting)
MAX_DOWNLOADS_PER_RUN = 3
ACCOUNT_COOLDOWN_MINUTES = 60  # How long to wait before using an account again after a failure
PROXY_COOLDOWN_MINUTES = 30  # How long to wait before using a proxy again after a failure
RATE_LIMIT_WAIT = 3600  # Seconds to wait after hitting a rate limit

# Mistral transcription settings
MISTRAL_CONFIG = {
    'model_size': '7B',  # 7B, 16K
    'device': 'cuda',    # 'cuda', 'cpu'
    'max_duration': 600  # Maximum duration in seconds
}

# API Keys
API_KEYS = {
    'anthropic': 'your_anthropic_api_key',
    'openai': 'your_openai_api_key',
    'eleven_labs': 'your_eleven_labs_api_key'
}

# Web interface settings
WEB_CONFIG = {
    'host': '0.0.0.0',
    'port': 5000,
    'debug': True
}

# Rate limiting settings
RATE_LIMIT = {
    'enabled': True,
    'max_requests_per_minute': 20,
    'retry_after': 60  # seconds
}

# GitHub specific settings
GITHUB_CONFIG = {
    'target_repos': [
        # Foundational ML/DL libraries
        'tensorflow/tensorflow',
        'pytorch/pytorch',
        'scikit-learn/scikit-learn',
        'huggingface/transformers',
        'keras-team/keras',
        
        # LLM & Generative AI projects
        'openai/whisper',
        'facebookresearch/llama',
        'anthropics/claude-api',
        'google/gemma',
        'mistralai/mistral-src',
        
        # Training & Infrastructure
        'ray-project/ray',
        'microsoft/DeepSpeed',
        'google/jax',
        
        # Research implementations
        'facebookresearch/fairseq',
        'openai/CLIP',
        'LAION-AI/Open-Assistant',
        
        # Learning resources
        'datawhalechina/pumpkin-book',
        'afshinea/stanford-cs-229-machine-learning',
        'microsoft/ML-For-Beginners'
    ],
    'readme_max_length': 100000,  # Maximum length of README to store
    'metadata_fields': [
        'id', 'name', 'full_name', 'description', 'html_url', 
        'stargazers_count', 'watchers_count', 'forks_count', 
        'language', 'pushed_at', 'created_at', 'updated_at', 'topics'
    ],
    'update_frequency_days': 7,  # How often to update GitHub repository data
} 

# Research paper collection settings
RESEARCH_PAPER_CONFIG = {
    'topics': [
        "large language models",
        "diffusion models",
        "transformers",
        "generative ai",
        "ai alignment",
        "few-shot learning",
        "multimodal ai",
        "reinforcement learning",
        "computer vision",
        "natural language processing"
    ],
    'max_papers_per_topic': 5,
    'max_age_days': 60,  # Only collect papers published within this timeframe
    'update_frequency_days': 7,  # How often to update paper data
    'pdf_extract': True,  # Whether to extract and store full PDF text
    'download_directory': os.path.join(DATA_DIR, "papers", "pdfs"),
    'max_papers_per_run': 15  # Maximum number of papers to process in a single run
} 

================================================================================
File: create_db.sql
================================================================================

-- Content table stores all video data
CREATE TABLE IF NOT EXISTS videos (
    id INTEGER PRIMARY KEY,
    shortcode TEXT UNIQUE,
    account TEXT,
    filename TEXT,
    caption TEXT,
    transcript TEXT,
    summary TEXT,
    timestamp TEXT,
    download_date TEXT,
    url TEXT,
    likes INTEGER,
    comments INTEGER,
    word_count INTEGER,
    duration_seconds INTEGER,
    key_phrases TEXT
);

-- Tags table for improved filtering
CREATE TABLE IF NOT EXISTS tags (
    id INTEGER PRIMARY KEY,
    video_id INTEGER,
    tag TEXT,
    FOREIGN KEY (video_id) REFERENCES videos(id)
);

-- Improved indexing for better performance
CREATE INDEX IF NOT EXISTS idx_videos_account ON videos(account);
CREATE INDEX IF NOT EXISTS idx_videos_timestamp ON videos(timestamp);
CREATE INDEX IF NOT EXISTS idx_tags_tag ON tags(tag);

-- Virtual FTS4 table for full-text search (using FTS4 instead of FTS5 for better compatibility)
CREATE VIRTUAL TABLE IF NOT EXISTS videos_fts USING fts4(
    shortcode,
    account,
    caption,
    transcript,
    summary,
    timestamp,
    content=videos,
    tokenize=porter
);

-- Create triggers to keep FTS table synchronized
CREATE TRIGGER IF NOT EXISTS videos_ai AFTER INSERT ON videos BEGIN
    INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
    VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
END;

CREATE TRIGGER IF NOT EXISTS videos_ad AFTER DELETE ON videos BEGIN
    DELETE FROM videos_fts WHERE docid = old.id;
END;

CREATE TRIGGER IF NOT EXISTS videos_au AFTER UPDATE ON videos BEGIN
    DELETE FROM videos_fts WHERE docid = old.id;
    INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
    VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
END; 

================================================================================
File: db_migration.py
================================================================================

"""
Database migration script for the Instagram Knowledge Base
Adds GitHub-related tables and a unified content structure
"""
import os
import sqlite3
import logging
import json
from datetime import datetime

from config import DATA_DIR, DB_PATH

# Configure logging
log_dir = os.path.join(DATA_DIR, 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'db_migration.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('db_migration')

def migrate_database():
    """
    Migrate the database to support GitHub repositories and a unified content structure
    
    Returns:
        Boolean indicating success
    """
    logger.info("Starting database migration")
    
    try:
        # Connect to the database
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # Check if videos table exists
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='videos'")
        has_videos_table = cursor.fetchone() is not None
        
        if not has_videos_table:
            logger.error("Videos table does not exist in the database. Please initialize the database first.")
            return False
        
        # Check if source_types table exists
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='source_types'")
        has_source_types_table = cursor.fetchone() is not None
        
        if not has_source_types_table:
            logger.info("Creating source_types table")
            cursor.execute("""
            CREATE TABLE source_types (
                id INTEGER PRIMARY KEY,
                name TEXT UNIQUE NOT NULL,
                description TEXT
            )
            """)
            
            # Insert initial source types
            cursor.executemany(
                "INSERT INTO source_types (id, name, description) VALUES (?, ?, ?)",
                [
                    (1, "instagram", "Instagram videos and posts"),
                    (2, "github", "GitHub repositories"),
                    (3, "research_paper", "Scientific research papers")
                ]
            )
            logger.info("Added initial source types")
        
        # Check if github_repos table exists
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='github_repos'")
        has_github_repos_table = cursor.fetchone() is not None
        
        if not has_github_repos_table:
            logger.info("Creating github_repos table")
            cursor.execute("""
            CREATE TABLE github_repos (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL,
                full_name TEXT UNIQUE NOT NULL,
                description TEXT,
                url TEXT,
                stars INTEGER DEFAULT 0,
                watchers INTEGER DEFAULT 0,
                forks INTEGER DEFAULT 0,
                language TEXT,
                last_push TEXT,
                created_at TEXT,
                updated_at TEXT,
                topics TEXT,
                readme TEXT,
                last_crawled TEXT
            )
            """)
            
            # Create index on full_name
            cursor.execute("CREATE INDEX idx_github_repos_full_name ON github_repos (full_name)")
            cursor.execute("CREATE INDEX idx_github_repos_stars ON github_repos (stars)")
            logger.info("Created github_repos table and indices")
        
        # Check if ai_content table exists
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='ai_content'")
        has_ai_content_table = cursor.fetchone() is not None
        
        if not has_ai_content_table:
            logger.info("Creating ai_content table for unified content")
            cursor.execute("""
            CREATE TABLE ai_content (
                id INTEGER PRIMARY KEY,
                source_type_id INTEGER NOT NULL,
                source_id TEXT NOT NULL,
                title TEXT,
                description TEXT,
                content TEXT,
                url TEXT,
                date_created TEXT,
                date_collected TEXT,
                metadata TEXT,
                is_indexed INTEGER DEFAULT 0,
                embedding_file TEXT,
                FOREIGN KEY (source_type_id) REFERENCES source_types(id),
                UNIQUE (source_type_id, source_id)
            )
            """)
            
            # Create indexes
            cursor.execute("CREATE INDEX idx_ai_content_source ON ai_content (source_type_id, source_id)")
            cursor.execute("CREATE INDEX idx_ai_content_is_indexed ON ai_content (is_indexed)")
            logger.info("Created ai_content table and indices")
        
        # Check if research_papers table exists
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='research_papers'")
        has_research_papers_table = cursor.fetchone() is not None
        
        if not has_research_papers_table:
            logger.info("Creating research_papers table")
            cursor.execute("""
            CREATE TABLE research_papers (
                id INTEGER PRIMARY KEY,
                title TEXT NOT NULL,
                authors TEXT,
                abstract TEXT,
                publication TEXT,
                year INTEGER,
                url TEXT,
                doi TEXT UNIQUE,
                pdf_path TEXT,
                content TEXT,
                last_crawled TEXT
            )
            """)
            
            # Create index on doi
            cursor.execute("CREATE INDEX idx_research_papers_doi ON research_papers (doi)")
            cursor.execute("CREATE INDEX idx_research_papers_year ON research_papers (year)")
            logger.info("Created research_papers table and indices")
        
        # Migrate existing Instagram videos to ai_content if needed
        if has_ai_content_table:
            cursor.execute("SELECT COUNT(*) FROM ai_content WHERE source_type_id = 1")
            instagram_count = cursor.fetchone()[0]
            
            if instagram_count == 0:
                logger.info("Migrating existing Instagram videos to ai_content table")
                
                # Check the schema of the videos table
                cursor.execute("PRAGMA table_info(videos)")
                columns = {row[1]: row[0] for row in cursor.fetchall()}
                
                # Build the query based on available columns
                select_cols = ["id", "shortcode"]
                if "caption" in columns:
                    select_cols.append("caption")
                if "timestamp" in columns:
                    select_cols.append("timestamp")
                if "username" in columns:
                    select_cols.append("username")
                if "filepath" in columns:
                    select_cols.append("filepath")
                if "downloaded" in columns:
                    select_cols.append("downloaded")
                if "transcribed" in columns:
                    select_cols.append("transcribed")
                if "summarized" in columns:
                    select_cols.append("summarized")
                if "indexed" in columns:
                    select_cols.append("indexed")
                if "duration_seconds" in columns:
                    select_cols.append("duration_seconds")
                if "key_phrases" in columns:
                    select_cols.append("key_phrases")
                
                # Build the query with an appropriate WHERE clause
                query = f"SELECT {', '.join(select_cols)} FROM videos"
                # Add WHERE clause only if downloaded column exists
                if "downloaded" in columns:
                    query += " WHERE downloaded = 1"
                
                logger.info(f"Running query: {query}")
                
                # Get all videos
                cursor.execute(query)
                videos = cursor.fetchall()
                
                # Get Instagram source type ID
                source_type_id = 1  # Instagram
                
                for video in videos:
                    # Create a dictionary to hold values with appropriate defaults
                    video_data = {}
                    for i, col in enumerate(select_cols):
                        video_data[col] = video[i]
                    
                    # Extract required values with defaults
                    video_id = video_data.get("id")
                    shortcode = video_data.get("shortcode")
                    caption = video_data.get("caption", "")
                    timestamp = video_data.get("timestamp")
                    username = video_data.get("username", "unknown")
                    filepath = video_data.get("filepath", "")
                    downloaded = video_data.get("downloaded", 1)
                    transcribed = video_data.get("transcribed", 0)
                    summarized = video_data.get("summarized", 0)
                    indexed = video_data.get("indexed", 0)
                    duration_seconds = video_data.get("duration_seconds", 0)
                    key_phrases = video_data.get("key_phrases", "")
                    
                    # Create metadata
                    metadata = {
                        'username': username,
                        'shortcode': shortcode,
                        'duration_seconds': duration_seconds or 0,
                        'key_phrases': key_phrases or ''
                    }
                    
                    # Get content (transcript or summary) if available
                    content = None
                    if summarized == 1:
                        # Try to get summary
                        cursor.execute("SELECT summary FROM summaries WHERE video_id = ?", (video_id,))
                        summary_row = cursor.fetchone()
                        if summary_row:
                            content = summary_row[0]
                    
                    if not content and transcribed == 1:
                        # Try to get transcript
                        cursor.execute("SELECT transcript FROM transcripts WHERE video_id = ?", (video_id,))
                        transcript_row = cursor.fetchone()
                        if transcript_row:
                            content = transcript_row[0]
                    
                    # Insert into ai_content
                    try:
                        cursor.execute("""
                        INSERT INTO ai_content (
                            source_type_id, source_id, title, description, content,
                            url, date_created, date_collected, metadata, is_indexed
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            source_type_id,
                            video_id,
                            f"Instagram video from {username}",
                            caption or "",
                            content or "",
                            f"https://www.instagram.com/p/{shortcode}/" if shortcode else "",
                            datetime.now().isoformat(),
                            datetime.now().isoformat(),
                            json.dumps(metadata),
                            1 if indexed == 1 else 0
                        ))
                    except sqlite3.IntegrityError:
                        # Skip duplicates
                        logger.warning(f"Skipping duplicate video {video_id} during migration")
                    except Exception as e:
                        logger.error(f"Error migrating video {video_id}: {str(e)}")
                
                logger.info(f"Migrated {len(videos)} Instagram videos to ai_content table")
        
        # Commit changes
        conn.commit()
        logger.info("Database migration completed successfully")
        
        # Close connection
        conn.close()
        return True
    
    except Exception as e:
        logger.error(f"Error during database migration: {str(e)}")
        
        # Try to rollback
        try:
            conn.rollback()
        except:
            pass
        
        # Close connection
        try:
            conn.close()
        except:
            pass
        
        return False

if __name__ == "__main__":
    migrate_database() 

================================================================================
File: downloader.py
================================================================================

"""
Module for downloading Instagram content with proper rate limiting
"""
import os
import time
import json
import logging
import random
from datetime import datetime, timedelta
import instaloader
import sqlite3

# Add imports for proxy testing
import requests
from urllib.parse import urlparse
from urllib.parse import parse_qs
import socket
import json as json_lib
import urllib3

# Disable SSL warnings for testing
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

from config import (
    INSTAGRAM_ACCOUNTS, 
    INSTAGRAM_USERNAME, 
    INSTAGRAM_PASSWORD,
    INSTAGRAM_ACCOUNT_ROTATION,
    PROXY_SERVERS,
    PROXY_COUNTRY,
    DOWNLOAD_DIR, 
    DOWNLOAD_DELAY, 
    MAX_DOWNLOADS_PER_RUN,
    DATA_DIR,
    ACCOUNT_COOLDOWN_MINUTES,
    PROXY_COOLDOWN_MINUTES,
    DB_PATH,
    INSTAGRAM_CREDENTIALS,
    PROXY_CONFIG,
    RATE_LIMIT_WAIT,
    CONTENT_SOURCES
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(DATA_DIR, 'logs', 'downloader.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('downloader')

# Account state tracking
def setup_directories():
    """Create necessary directories if they don't exist"""
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    os.makedirs(os.path.join(DATA_DIR, "state"), exist_ok=True)
    
    # Create account-specific directories
    for account in INSTAGRAM_ACCOUNTS:
        account_dir = os.path.join(DOWNLOAD_DIR, account["username"])
        os.makedirs(account_dir, exist_ok=True)

def get_random_delay():
    """Return a more human-like delay between actions"""
    # Base delay plus random variation to appear more human-like
    return DOWNLOAD_DELAY + random.uniform(-2, 5)

def get_next_account():
    """Get the next available account from rotation"""
    # If no accounts in rotation, use the default credentials
    if not INSTAGRAM_ACCOUNT_ROTATION:
        return INSTAGRAM_USERNAME, INSTAGRAM_PASSWORD
    
    state_file = os.path.join(DATA_DIR, "state", "account_state.json")
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "account_states": {}}
    else:
        state = {"last_index": -1, "account_states": {}}
    
    # Find the next available account
    for i in range(len(INSTAGRAM_ACCOUNT_ROTATION)):
        idx = (state["last_index"] + i + 1) % len(INSTAGRAM_ACCOUNT_ROTATION)
        account = INSTAGRAM_ACCOUNT_ROTATION[idx]
        username = account["username"]
        
        # Skip accounts that are in cooldown
        account_state = state["account_states"].get(username, {})
        next_available_str = account_state.get("next_available")
        
        if not next_available_str or datetime.now().isoformat() >= next_available_str:
            # Update state
            state["last_index"] = idx
            with open(state_file, "w") as f:
                json.dump(state, f)
            
            logger.info(f"Using account {username} from rotation")
            return account["username"], account["password"]
    
    # If all accounts are in cooldown, fallback to default account
    logger.warning("All accounts in rotation are in cooldown, using default account")
    return INSTAGRAM_USERNAME, INSTAGRAM_PASSWORD

def mark_account_cooldown(username, cooldown_minutes=ACCOUNT_COOLDOWN_MINUTES):
    """Mark an account as in cooldown after a failure"""
    if not username:
        return
        
    state_file = os.path.join(DATA_DIR, "state", "account_state.json")
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "account_states": {}}
    else:
        state = {"last_index": -1, "account_states": {}}
    
    # Set cooldown period
    next_available = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
    
    # Update account state
    if username not in state["account_states"]:
        state["account_states"][username] = {}
        
    state["account_states"][username]["next_available"] = next_available
    state["account_states"][username]["last_failure"] = datetime.now().isoformat()
    
    # Save state
    with open(state_file, "w") as f:
        json.dump(state, f)
        
    logger.info(f"Account {username} marked for cooldown until {next_available}")

def get_proxy(country=None):
    """
    Get a proxy from the available pool
    
    Args:
        country: Optional two-letter country code (us, uk, etc.)
    """
    # If no proxies configured, return None
    if not PROXY_SERVERS:
        return None
    
    # Use country from config if not specified in function call
    if country is None and PROXY_COUNTRY:
        country = PROXY_COUNTRY
        
    state_file = os.path.join(DATA_DIR, "state", "proxy_state.json")
    
    # Make sure the directory exists
    os.makedirs(os.path.dirname(state_file), exist_ok=True)
    
    # Load proxy state
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "proxy_states": {}}
    else:
        state = {"last_index": -1, "proxy_states": {}}
    
    # Find the next available proxy
    for i in range(len(PROXY_SERVERS)):
        idx = (state["last_index"] + i + 1) % len(PROXY_SERVERS)
        base_proxy = PROXY_SERVERS[idx]
        
        # If country is specified, modify the proxy URL
        proxy = base_proxy
        if country and "zone-residential" in base_proxy:
            # Extract components from the proxy URL
            parts = base_proxy.split('@')
            if len(parts) == 2:
                auth_part = parts[0]
                host_part = parts[1]
                
                # Check if country parameter is already in the auth part
                if "country-" in auth_part:
                    # Replace existing country
                    auth_parts = auth_part.split('-country-')
                    if len(auth_parts) == 2:
                        country_and_after = auth_parts[1].split(':', 1)
                        if len(country_and_after) == 2:
                            new_auth = f"{auth_parts[0]}-country-{country}:{country_and_after[1]}"
                            proxy = f"{new_auth}@{host_part}"
                else:
                    # Add country before the password
                    auth_parts = auth_part.split(':')
                    if len(auth_parts) >= 2:
                        password_idx = len(auth_parts) - 1
                        auth_parts[password_idx] = f"country-{country}:{auth_parts[password_idx].split(':')[-1]}"
                        proxy = f"{':'.join(auth_parts)}@{host_part}"
        
        # Skip proxies that are in cooldown
        proxy_state = state["proxy_states"].get(proxy, {})
        next_available_str = proxy_state.get("next_available")
        
        if not next_available_str or datetime.now().isoformat() >= next_available_str:
            # Update state
            state["last_index"] = idx
            with open(state_file, "w") as f:
                json.dump(state, f)
            
            # Test the proxy before returning
            if test_proxy(proxy):
                logger.info(f"Using proxy: {proxy}")
                return proxy
            else:
                # Mark as in cooldown if test fails
                mark_proxy_cooldown(proxy, cooldown_minutes=30)
                logger.warning(f"Proxy test failed, marking for cooldown: {proxy}")
                continue
    
    # If all proxies are in cooldown, log warning and return None
    logger.warning("All proxies are in cooldown or not working, proceeding without proxy")
    return None

def test_proxy(proxy_url, test_url="https://www.instagram.com/favicon.ico", timeout=30):
    """Test if a proxy server is working correctly"""
    try:
        # Extract proxy username and password from URL
        parsed_url = urlparse(proxy_url)
        username = parsed_url.username or ""
        password = parsed_url.password or ""
        
        # Create proxy dictionary in the format required by requests
        scheme = parsed_url.scheme
        netloc = parsed_url.netloc
        if '@' in netloc:
            netloc = netloc.split('@')[1]  # Remove credentials from netloc
        
        proxies = {
            "http": f"{scheme}://{username}:{password}@{netloc}",
            "https": f"{scheme}://{username}:{password}@{netloc}"
        }
        
        # Test the proxy by making a request to the test URL
        response = requests.get(test_url, proxies=proxies, timeout=timeout, verify=False)
        
        if response.status_code == 200:
            logger.info(f"Proxy test successful: {netloc}")
            return True
        else:
            logger.warning(f"Proxy test failed with status code {response.status_code}: {netloc}")
            return False
            
    except Exception as e:
        logger.error(f"Error testing proxy: {str(e)}")
        return False

def mark_proxy_cooldown(proxy, cooldown_minutes=PROXY_COOLDOWN_MINUTES):
    """Mark a proxy as in cooldown after a failure"""
    if not proxy:
        return
        
    state_file = os.path.join(DATA_DIR, "state", "proxy_state.json")
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "proxy_states": {}}
    else:
        state = {"last_index": -1, "proxy_states": {}}
    
    # Set cooldown period
    next_available = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
    
    # Update proxy state
    if proxy not in state["proxy_states"]:
        state["proxy_states"][proxy] = {}
        
    state["proxy_states"][proxy]["next_available"] = next_available
    state["proxy_states"][proxy]["last_failure"] = datetime.now().isoformat()
    
    # Save state
    with open(state_file, "w") as f:
        json.dump(state, f)
        
    logger.info(f"Proxy {proxy} marked for cooldown until {next_available}")

def login_with_session(L, username, password):
    """Login with proper session management and error handling"""
    session_file = os.path.join(DATA_DIR, "state", f"insta_session_{username}.txt")
    
    # Try to load existing session first
    if os.path.exists(session_file) and username:
        try:
            L.load_session_from_file(username, session_file)
            logger.info(f"Loaded existing session for {username}")
            
            # Test the session validity by trying a simple operation
            try:
                test_profile = instaloader.Profile.from_username(L.context, username)
                logger.info("Session validation successful")
                return True
            except Exception:
                logger.warning("Loaded session is invalid, will login again")
        except Exception as e:
            logger.warning(f"Could not load session: {str(e)}")
    
    # If we need to login again
    if not username or not password:
        logger.warning("No login credentials provided")
        return False
        
    try:
        # Add random delay before login to look more human-like
        delay = random.uniform(1, 3)
        logger.info(f"Waiting {delay:.1f}s before login attempt...")
        time.sleep(delay)
        
        L.login(username, password)
        
        # Save the session for future use
        L.save_session_to_file(session_file)
        logger.info(f"Login successful and session saved for {username}")
        return True
    except Exception as e:
        logger.error(f"Login failed for {username}: {str(e)}")
        # Mark account for cooldown after failure
        mark_account_cooldown(username)
        return False

def create_instaloader_instance(use_login=True, account=None, proxy=None):
    """Create an Instaloader instance with appropriate settings for our use case"""
    
    # Get a random user agent to appear more human-like
    user_agent = get_random_user_agent()
    
    # Create an Instaloader instance with our required settings
    loader = instaloader.Instaloader(
        download_videos=True,
        download_video_thumbnails=False,
        download_geotags=False,
        download_comments=False,
        save_metadata=True,
        compress_json=False,
        user_agent=user_agent,
        max_connection_attempts=3,
        sleep=True,  # Respect Instagram's rate limits
    )
    
    logger.info(f"Initialized Instaloader with user agent: {user_agent[:30]}...")
    
    # If a proxy is provided, set it on the session
    if proxy:
        try:
            # Extract proxy username and password from URL
            parsed_url = urlparse(proxy)
            username = parsed_url.username or ""
            password = parsed_url.password or ""
            
            # Create proxy string in the format required by requests
            scheme = parsed_url.scheme
            netloc = parsed_url.netloc
            if '@' in netloc:
                netloc = netloc.split('@')[1]  # Remove credentials from netloc
            
            proxy_str = f"{scheme}://{username}:{password}@{netloc}"
            
            # Set the proxy on the session
            loader.context._session.proxies = {
                "http": proxy_str,
                "https": proxy_str
            }
            logger.info(f"Set proxy on Instaloader session: {netloc}")
        except Exception as e:
            logger.error(f"Error setting proxy on Instaloader session: {str(e)}")
    
    # Log in if requested and credentials are available
    if use_login:
        if account is None:
            # Use default credentials if no specific account is provided
            username = INSTAGRAM_USERNAME
            password = INSTAGRAM_PASSWORD
        else:
            # Use the provided account credentials
            username = account.get('username')
            password = account.get('password')
        
        if username and password:
            try:
                # Add a random delay before login to avoid detection
                delay = random.uniform(1, 3)
                logger.debug(f"Adding random delay of {delay:.2f}s before login attempt")
                time.sleep(delay)
                
                loader.login(username, password)
                logger.info(f"Logged in as {username}")
            except Exception as e:
                logger.error(f"Login failed for {username}: {str(e)}")
    
    return loader

def retry_with_backoff(func, max_retries=3, initial_delay=5):
    """Execute a function with exponential backoff retries"""
    retries = 0
    while retries <= max_retries:
        try:
            return func()
        except Exception as e:  # Use generic Exception instead of specific ones
            retries += 1
            if retries > max_retries:
                raise
            wait_time = initial_delay * (2 ** retries) + random.uniform(1, 5)
            logger.warning(f"Retry {retries}/{max_retries} after error: {str(e)}. Waiting {wait_time:.1f}s")
            time.sleep(wait_time)
    return None

def download_from_instagram(accounts=None):
    """
    Download content from Instagram accounts with proper rate limiting
    and proxy rotation
    """
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    downloaded_count = 0
    success_count = 0
    
    # Use the accounts from config if none are provided
    if accounts is None:
        accounts = INSTAGRAM_ACCOUNTS
    
    # Store failed accounts to retry later
    failed_accounts = []
    
    # Randomize the account order to distribute load
    accounts_to_process = list(accounts)
    random.shuffle(accounts_to_process)
    
    # Process each target account
    for account_idx, account_info in enumerate(accounts_to_process):
        # Extract the account name depending on the type
        if isinstance(account_info, dict):
            account_name = account_info.get("username")
        else:
            account_name = account_info
            
        if not account_name:
            logger.warning(f"Skipping invalid account info: {account_info}")
            continue
            
        # Check if we've reached the download limit
        if downloaded_count >= MAX_DOWNLOADS_PER_RUN:
            logger.info(f"Reached maximum download limit of {MAX_DOWNLOADS_PER_RUN}")
            break
            
        # Skip accounts that are not due for refresh
        if not is_account_due_for_refresh(account_name):
            logger.info(f"Skipping account {account_name} - not due for refresh")
            continue
            
        # Get a proxy if available
        proxy = get_proxy()
        if not proxy:
            logger.warning("No proxy available, proceeding without proxy")
        else:
            logger.info(f"Using proxy: {proxy}")
        
        # Log the attempt
        logger.info(f"Processing account {account_idx+1}/{len(accounts_to_process)}: {account_name}")
        
        # Add a delay before processing to avoid detection
        delay = random.uniform(2, 5)
        logger.debug(f"Adding random delay of {delay:.2f}s before processing account")
        time.sleep(delay)
        
        # Try up to 2 different methods to download content
        profile = None
        posts = None
        attempt_count = 0
        max_attempts = 2
        
        while attempt_count < max_attempts and posts is None:
            attempt_count += 1
            logger.info(f"Attempt {attempt_count}/{max_attempts} for account {account_name}")
            
            try:
                # Create Instaloader instance
                use_login = (attempt_count > 1)  # Try without login first, then with login if available
                L = create_instaloader_instance(use_login=use_login, proxy=proxy)
                
                # Try to get profile
                if profile is None:
                    profile = instaloader.Profile.from_username(L.context, account_name)
                    
                    # Check if the profile has posts
                    if not hasattr(profile, 'get_posts'):
                        logger.error(f"Profile {account_name} does not have get_posts method")
                        raise ValueError(f"Invalid profile structure for {account_name}")
                    
                    # Mark this account as processed now (whether it succeeds or fails)
                    mark_account_processed(account_name)
                    
                    # Handle private account
                    if profile.is_private and not use_login:
                        logger.warning(f"Account {account_name} is private - will retry with login if credentials available")
                        continue  # Skip to next attempt (which will use login)
                    
                    logger.info(f"Successfully retrieved profile for {account_name}")
                
                # Get the iterator for posts with proper error handling
                try:
                    # Add a random delay before fetching posts to appear more human-like
                    time.sleep(random.uniform(1, 3))
                    
                    # Get posts iterator
                    posts = profile.get_posts()
                    
                    # Try to access the first post to validate the iterator
                    next(posts)
                    # Reset the iterator
                    posts = profile.get_posts()
                    
                except StopIteration:
                    logger.warning(f"No posts found for account {account_name}")
                    posts = []  # Empty list to indicate success but no posts
                    
                except Exception as e:
                    error_msg = str(e).lower()
                    logger.error(f"Error getting posts for {account_name} (attempt {attempt_count}): {str(e)}")
                    
                    # Check for specific errors that indicate we should try alternative approaches
                    if "401" in error_msg or "unauthorized" in error_msg:
                        if attempt_count < max_attempts:
                            logger.info(f"401 Unauthorized error - will retry with different approach")
                            # Mark previous proxy for cooldown and get a new one for next attempt
                            if proxy:
                                mark_proxy_cooldown(proxy)
                                proxy = get_proxy()
                            
                            posts = None  # Reset so we try again
                            time.sleep(5)  # Wait before retrying
                            continue
                    
                    # If we get here, we couldn't get posts after all retries
                    failed_accounts.append(account_name)
                    break
                
                # Process posts if we have them
                if posts is not None:
                    process_posts(L, profile, account_name, posts, 
                                 downloaded_count, success_count)
                    
            except Exception as e:
                logger.error(f"Error processing account {account_name} (attempt {attempt_count}): {str(e)}")
                
                if attempt_count < max_attempts:
                    # Mark previous proxy for cooldown and get a new one for next attempt
                    if proxy:
                        mark_proxy_cooldown(proxy)
                        proxy = get_proxy()
                    time.sleep(5)  # Wait before retrying
                else:
                    failed_accounts.append(account_name)
    
    # Log summary
    logger.info(f"Download session completed. Downloaded {success_count} videos from {len(accounts_to_process) - len(failed_accounts)}/{len(accounts_to_process)} accounts")
    
    if failed_accounts:
        logger.warning(f"Failed to process {len(failed_accounts)} accounts: {', '.join(str(a) for a in failed_accounts)}")
    
    return success_count, downloaded_count, failed_accounts

def process_posts(L, profile, account_name, posts, downloaded_count, success_count):
    """Process posts for an account"""
    # Process posts
    posts_processed = 0
    
    # Create account directory
    account_dir = os.path.join(DOWNLOAD_DIR, account_name)
    os.makedirs(account_dir, exist_ok=True)
    
    # Create a directory for metadata
    metadata_dir = os.path.join(account_dir, 'metadata')
    os.makedirs(metadata_dir, exist_ok=True)
    
    try:
        for post in posts:
            try:
                # Check if we've reached the maximum downloads limit
                if downloaded_count >= MAX_DOWNLOADS_PER_RUN:
                    logger.info(f"Reached maximum download limit of {MAX_DOWNLOADS_PER_RUN}")
                    break
                
                # Skip if it's not a video
                if not post.is_video:
                    logger.debug(f"Skipping non-video post from {account_name}: {post.shortcode}")
                    continue
                
                # Check if this video has already been downloaded
                video_filename = f"{post.date_utc.strftime('%Y-%m-%d_%H-%M-%S')}_{post.shortcode}.mp4"
                video_path = os.path.join(account_dir, video_filename)
                
                if os.path.exists(video_path):
                    logger.debug(f"Skipping already downloaded video: {video_filename}")
                    continue
                
                # Download the post
                logger.info(f"Downloading video post from {account_name}: {post.shortcode}")
                
                try:
                    # Download only the video
                    L.download_post(post, target=account_dir)
                    downloaded_count += 1
                    success_count += 1
                    
                    # Save post metadata to a separate JSON file
                    metadata = {
                        'shortcode': post.shortcode,
                        'date_utc': post.date_utc.strftime('%Y-%m-%d %H:%M:%S'),
                        'caption': post.caption if post.caption else '',
                        'likes': post.likes,
                        'comments': post.comments,
                        'url': f"https://www.instagram.com/p/{post.shortcode}/",
                        'account': account_name
                    }
                    
                    metadata_path = os.path.join(metadata_dir, f"{post.shortcode}.json")
                    with open(metadata_path, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, ensure_ascii=False, indent=4)
                    
                    # Respect Instagram's rate limits by adding a delay between downloads
                    time.sleep(DOWNLOAD_DELAY)
                    posts_processed += 1
                    
                except Exception as e:
                    logger.error(f"Error downloading post {post.shortcode} from {account_name}: {str(e)}")
                    # Continue with the next post despite this error
                    continue
            
            except Exception as post_error:
                logger.error(f"Error processing post from {account_name}: {str(post_error)}")
                # Continue with the next post
                continue
        
        logger.info(f"Processed {posts_processed} posts from {account_name}, downloaded {success_count} videos")
        return True, posts_processed
        
    except Exception as e:
        logger.error(f"Error processing posts for {account_name}: {str(e)}")
        return False, 0

def get_posts_alternative(profile, username):
    """Alternative approach to fetch posts when the standard iterator fails"""
    try:
        # Try direct URL construction approach
        base_url = f"https://www.instagram.com/{username}/"
        logger.info(f"Attempting alternative post fetching from {base_url}")
        
        # Return posts that we already have in the directory
        metadata_dir = os.path.join(DOWNLOAD_DIR, username, "metadata")
        existing_posts = []
        
        if os.path.exists(metadata_dir):
            for file in os.listdir(metadata_dir):
                if file.endswith('.json'):
                    try:
                        with open(os.path.join(metadata_dir, file), 'r') as f:
                            metadata = json.load(f)
                            if 'shortcode' in metadata:
                                existing_posts.append(metadata['shortcode'])
                    except Exception as e:
                        logger.error(f"Error reading metadata file {file}: {str(e)}")
        
        logger.info(f"Found {len(existing_posts)} existing posts metadata to process")
        return existing_posts
        
    except Exception as e:
        logger.error(f"Alternative post fetching failed: {str(e)}")
        return []

def get_random_user_agent():
    """Return a random user agent to appear more human-like"""
    user_agents = [
        # Desktop browsers
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
        # Mobile browsers
        "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
        "Mozilla/5.0 (Linux; Android 13; SM-S908B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.43 Mobile Safari/537.36"
    ]
    return random.choice(user_agents)

def schedule_refresh(username, backoff_minutes=60):
    """Schedule a refresh attempt with exponential backoff"""
    state_file = os.path.join(DATA_DIR, "refresh_state.json")
    
    # Load or initialize state
    if os.path.exists(state_file):
        try:
            with open(state_file, 'r') as f:
                state = json.load(f)
        except json.JSONDecodeError:
            state = {"accounts": {}}
    else:
        state = {"accounts": {}}
    
    # Get account state or initialize
    account_state = state["accounts"].get(username, {
        "last_attempt": None,
        "backoff_minutes": backoff_minutes,
        "consecutive_failures": 0
    })
    
    # Update for next attempt
    now = datetime.now().isoformat()
    account_state["last_attempt"] = now
    
    if account_state["consecutive_failures"] > 0:
        # Exponential backoff
        account_state["backoff_minutes"] *= 2
    
    # Schedule next attempt
    next_attempt = (datetime.now() + 
                   timedelta(minutes=account_state["backoff_minutes"]))
    account_state["next_attempt"] = next_attempt.isoformat()
    
    # Save state
    state["accounts"][username] = account_state
    with open(state_file, 'w') as f:
        json.dump(state, f)
    
    logger.info(f"Scheduled next refresh for {username} at {next_attempt.isoformat()}")
    return next_attempt.isoformat()

def should_refresh_account(username):
    """Check if an account is due for refresh based on backoff schedule"""
    state_file = os.path.join(DATA_DIR, "refresh_state.json")
    
    # If no state file, always refresh
    if not os.path.exists(state_file):
        return True
        
    try:
        with open(state_file, 'r') as f:
            state = json.load(f)
            
        # If account not in state, always refresh
        if username not in state.get("accounts", {}):
            return True
            
        account_state = state["accounts"][username]
        next_attempt_str = account_state.get("next_attempt")
        
        # If no next attempt scheduled, always refresh
        if not next_attempt_str:
            return True
            
        # Parse next attempt time
        next_attempt = datetime.fromisoformat(next_attempt_str)
        
        # Check if we're past the scheduled time
        return datetime.now() >= next_attempt
        
    except Exception as e:
        logger.error(f"Error checking refresh schedule: {str(e)}")
        return True  # Default to allowing refresh on error

def is_account_due_for_refresh(account_name):
    """Check if an account is due for refresh based on its last processing time"""
    # Path to the account state file
    account_state_file = os.path.join(DATA_DIR, "logs", "account_states.json")
    
    # Default: account is due for refresh
    if not os.path.exists(account_state_file):
        os.makedirs(os.path.dirname(account_state_file), exist_ok=True)
        return True
    
    try:
        # Load account states
        with open(account_state_file, 'r') as f:
            account_states = json.load(f)
        
        # Check if account exists in states
        if account_name not in account_states:
            return True
        
        account_state = account_states[account_name]
        last_processed = account_state.get('last_processed')
        
        # If no last processed time, account is due for refresh
        if not last_processed:
            return True
        
        # Check if account is in cooldown
        cooldown_until = account_state.get('cooldown_until')
        if cooldown_until:
            cooldown_time = datetime.fromisoformat(cooldown_until)
            if datetime.now() < cooldown_time:
                logger.info(f"Account {account_name} is in cooldown until {cooldown_until}")
                return False
        
        # Check if enough time has passed since last processing
        last_processed_time = datetime.fromisoformat(last_processed)
        refresh_interval = account_state.get('refresh_interval', 24)  # Default: 24 hours
        
        next_refresh_time = last_processed_time + timedelta(hours=refresh_interval)
        
        if datetime.now() < next_refresh_time:
            logger.info(f"Account {account_name} not due for refresh until {next_refresh_time.isoformat()}")
            return False
            
        return True
        
    except Exception as e:
        logger.error(f"Error checking refresh status for {account_name}: {str(e)}")
        # Default to allowing refresh on error
        return True

def mark_account_processed(account_name, success=True):
    """Mark an account as processed and update its refresh schedule"""
    # Path to the account state file
    account_state_file = os.path.join(DATA_DIR, "logs", "account_states.json")
    
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(account_state_file), exist_ok=True)
        
        # Load existing account states or create new dict
        if os.path.exists(account_state_file):
            with open(account_state_file, 'r') as f:
                account_states = json.load(f)
        else:
            account_states = {}
        
        # Get or create account state
        if account_name not in account_states:
            account_states[account_name] = {}
        
        # Update account state
        account_states[account_name]['last_processed'] = datetime.now().isoformat()
        
        # Update success/failure count
        if success:
            account_states[account_name]['consecutive_failures'] = 0
            # Reset refresh interval to default on success
            account_states[account_name]['refresh_interval'] = 24  # Default: 24 hours
        else:
            # Increment failure count
            failures = account_states[account_name].get('consecutive_failures', 0) + 1
            account_states[account_name]['consecutive_failures'] = failures
            
            # Implement exponential backoff for failures
            backoff_hours = min(24 * (2 ** (failures - 1)), 168)  # Max 1 week
            account_states[account_name]['refresh_interval'] = backoff_hours
            
            # Set cooldown period for repeated failures
            if failures > 2:
                cooldown_minutes = ACCOUNT_COOLDOWN_MINUTES * (2 ** (failures - 3))  # Exponential cooldown
                cooldown_until = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
                account_states[account_name]['cooldown_until'] = cooldown_until
                logger.warning(f"Account {account_name} in cooldown until {cooldown_until} after {failures} failures")
        
        # Save updated account states
        with open(account_state_file, 'w') as f:
            json.dump(account_states, f, indent=4)
            
    except Exception as e:
        logger.error(f"Error marking account {account_name} as processed: {str(e)}")

def mark_account_cooldown(username, cooldown_minutes=ACCOUNT_COOLDOWN_MINUTES):
    """Mark an account for cooldown after a failure"""
    # Path to the account state file
    account_state_file = os.path.join(DATA_DIR, "logs", "account_states.json")
    
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(account_state_file), exist_ok=True)
        
        # Load existing account states or create new dict
        if os.path.exists(account_state_file):
            with open(account_state_file, 'r') as f:
                account_states = json.load(f)
        else:
            account_states = {}
        
        # Get or create account state
        if username not in account_states:
            account_states[username] = {}
        
        # Set cooldown period
        cooldown_until = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
        account_states[username]['cooldown_until'] = cooldown_until
        
        # Increment failure count
        failures = account_states[username].get('consecutive_failures', 0) + 1
        account_states[username]['consecutive_failures'] = failures
        
        logger.warning(f"Account {username} in cooldown until {cooldown_until}")
        
        # Save updated account states
        with open(account_state_file, 'w') as f:
            json.dump(account_states, f, indent=4)
            
    except Exception as e:
        logger.error(f"Error marking account {username} for cooldown: {str(e)}")

if __name__ == "__main__":
    download_from_instagram() 

================================================================================
File: github_collector.py
================================================================================

"""
GitHub Repository Collector Module
Collects and processes AI/ML repositories from GitHub for the knowledge base
"""
import os
import json
import logging
import time
import base64
import sqlite3
import requests
from datetime import datetime, timedelta
import config

# Configure logging
log_dir = os.path.join(config.DATA_DIR, 'logs')
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(log_dir, 'github_collector.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('github_collector')

# List of repositories to collect (used as fallback if search doesn't yield enough results)
GITHUB_REPOS = [
    # Foundational ML/DL libraries
    'tensorflow/tensorflow',
    'pytorch/pytorch',
    'scikit-learn/scikit-learn',
    'huggingface/transformers',
    'keras-team/keras',
    
    # LLM & generative AI projects
    'openai/whisper',
    'facebookresearch/llama',
    'anthropics/claude-api',
    'google/gemma',
    'mistralai/mistral-src',
    
    # Training & infrastructure tools
    'ray-project/ray',
    'microsoft/DeepSpeed',
    'google/jax',
    
    # Research implementations
    'facebookresearch/fairseq',
    'openai/CLIP',
    'LAION-AI/Open-Assistant',
    
    # Learning resources
    'datawhalechina/pumpkin-book',
    'afshinea/stanford-cs-229-machine-learning',
    'microsoft/ML-For-Beginners'
]

def get_github_session():
    """Create a requests session with GitHub API token if available"""
    session = requests.Session()
    
    # Add GitHub API token if available
    github_token = config.GITHUB_CONFIG.get('api_token', '')
    if github_token:
        session.headers.update({
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json'
        })
    else:
        session.headers.update({
            'Accept': 'application/vnd.github.v3+json'
        })
    
    # Set user agent
    session.headers.update({
        'User-Agent': 'AI-Knowledge-Base-Collector/1.0'
    })
    
    return session

def get_rate_limit_info(session):
    """Get the current GitHub API rate limit information"""
    try:
        response = session.get('https://api.github.com/rate_limit')
        if response.status_code == 200:
            data = response.json()
            core = data.get('resources', {}).get('core', {})
            remaining = core.get('remaining', 0)
            reset_timestamp = core.get('reset', 0)
            reset_time = datetime.fromtimestamp(reset_timestamp)
            
            logger.info(f"GitHub API rate limit: {remaining} requests remaining, resets at {reset_time}")
            return remaining, reset_time
        else:
            logger.warning(f"Failed to get rate limit info: {response.status_code}")
            return None, None
    except Exception as e:
        logger.error(f"Error getting rate limit info: {str(e)}")
        return None, None

def wait_for_rate_limit(session):
    """Check rate limit and wait if necessary"""
    remaining, reset_time = get_rate_limit_info(session)
    
    if remaining is None or reset_time is None:
        # If we can't get rate limit info, wait a conservative amount of time
        logger.warning("Could not get rate limit info, waiting 60 seconds")
        time.sleep(60)
        return
    
    if remaining <= 10:  # Keep a small buffer
        now = datetime.now()
        wait_seconds = (reset_time - now).total_seconds() + 5  # Add a small buffer
        
        if wait_seconds > 0:
            logger.warning(f"Rate limit almost reached. Waiting {wait_seconds:.1f} seconds until reset")
            time.sleep(wait_seconds)
        else:
            # If the reset time is in the past, wait a bit anyway
            logger.warning("Rate limit reset time is in the past, waiting 10 seconds as precaution")
            time.sleep(10)

def search_github_repos(session, topics, min_stars=1000, per_page=30, max_results=100):
    """Search for repositories based on topics and minimum stars"""
    repos = []
    topic_query = ' '.join([f'topic:{topic}' for topic in topics])
    query = f"{topic_query} stars:>={min_stars}"
    
    logger.info(f"Searching GitHub with query: {query}")
    
    page = 1
    while len(repos) < max_results:
        wait_for_rate_limit(session)
        
        try:
            response = session.get(
                'https://api.github.com/search/repositories',
                params={
                    'q': query,
                    'sort': 'stars',
                    'order': 'desc',
                    'per_page': per_page,
                    'page': page
                }
            )
            
            if response.status_code != 200:
                logger.error(f"GitHub search API returned error: {response.status_code} - {response.text}")
                break
                
            data = response.json()
            items = data.get('items', [])
            
            if not items:
                break
                
            repos.extend(items)
            total_count = data.get('total_count', 0)
            logger.info(f"Found {len(repos)}/{total_count} repositories (page {page})")
            
            if len(repos) >= max_results:
                logger.info(f"Reached maximum results limit of {max_results}")
                break
                
            # Check if we've reached the last page
            if len(items) < per_page:
                break
                
            page += 1
            
            # Add a small delay between requests
            time.sleep(2)
            
        except Exception as e:
            logger.error(f"Error searching GitHub repositories: {str(e)}")
            break
    
    return repos[:max_results]

def get_repo_info(session, repo_full_name):
    """Get detailed information about a repository"""
    wait_for_rate_limit(session)
    
    try:
        logger.info(f"Getting info for repository: {repo_full_name}")
        response = session.get(f'https://api.github.com/repos/{repo_full_name}')
        
        if response.status_code != 200:
            logger.error(f"GitHub API returned error for {repo_full_name}: {response.status_code} - {response.text}")
            return None
            
        return response.json()
    except Exception as e:
        logger.error(f"Error getting repository info for {repo_full_name}: {str(e)}")
        return None

def get_repo_readme(session, repo_full_name):
    """Get repository README content"""
    wait_for_rate_limit(session)
    
    try:
        logger.info(f"Getting README for repository: {repo_full_name}")
        response = session.get(f'https://api.github.com/repos/{repo_full_name}/readme')
        
        if response.status_code != 200:
            logger.warning(f"Could not find README for {repo_full_name}: {response.status_code}")
            return None
            
        data = response.json()
        content = data.get('content', '')
        encoding = data.get('encoding', 'base64')
        
        if content and encoding == 'base64':
            readme_content = base64.b64decode(content).decode('utf-8', errors='replace')
            
            # Limit readme length if necessary
            max_length = config.GITHUB_CONFIG.get('readme_max_length', 100000)
            if len(readme_content) > max_length:
                readme_content = readme_content[:max_length] + "... [truncated]"
                
            return readme_content
        
        return None
    except Exception as e:
        logger.error(f"Error getting README for {repo_full_name}: {str(e)}")
        return None

def should_update_repo(conn, repo_full_name):
    """Check if a repository should be updated based on last crawl time"""
    cursor = conn.cursor()
    
    try:
        # Check if the repo exists in the database
        cursor.execute("SELECT last_crawled FROM github_repos WHERE full_name = ?", (repo_full_name,))
        result = cursor.fetchone()
        
        if not result:
            # Repository not in database, should add it
            return True
            
        last_crawled = result[0]
        if not last_crawled:
            return True
            
        # Convert string to datetime
        last_crawled_dt = datetime.fromisoformat(last_crawled.replace('Z', '+00:00'))
        
        # Check if it's time to update
        update_frequency_days = config.GITHUB_CONFIG.get('update_frequency_days', 7)
        update_threshold = datetime.now() - timedelta(days=update_frequency_days)
        
        return last_crawled_dt < update_threshold
        
    except Exception as e:
        logger.error(f"Error checking if repo {repo_full_name} should be updated: {str(e)}")
        return True

def store_repo_in_db(conn, repo_data, readme):
    """Store repository information in the database"""
    cursor = conn.cursor()
    
    try:
        # Extract fields to store
        repo_full_name = repo_data.get('full_name')
        
        # Check if repo exists in database
        cursor.execute("SELECT id FROM github_repos WHERE full_name = ?", (repo_full_name,))
        existing_repo = cursor.fetchone()
        
        # Prepare data for insertion/update
        repo_values = {
            'id': repo_data.get('id'),
            'name': repo_data.get('name'),
            'full_name': repo_full_name,
            'description': repo_data.get('description') or '',
            'url': repo_data.get('html_url'),
            'stars': repo_data.get('stargazers_count', 0),
            'watchers': repo_data.get('watchers_count', 0),
            'forks': repo_data.get('forks_count', 0),
            'language': repo_data.get('language') or '',
            'last_push': repo_data.get('pushed_at'),
            'created_at': repo_data.get('created_at'),
            'updated_at': repo_data.get('updated_at'),
            'topics': json.dumps(repo_data.get('topics', [])),
            'readme': readme or '',
            'last_crawled': datetime.now().isoformat()
        }
        
        if existing_repo:
            # Update existing repo
            placeholders = ', '.join([f"{key} = ?" for key in repo_values.keys()])
            query = f"UPDATE github_repos SET {placeholders} WHERE full_name = ?"
            values = list(repo_values.values()) + [repo_full_name]
            cursor.execute(query, values)
            logger.info(f"Updated repository {repo_full_name} in database")
            
            # Get the repo ID for ai_content relation
            repo_id = existing_repo[0]
        else:
            # Insert new repo
            placeholders = ', '.join(['?'] * len(repo_values))
            columns = ', '.join(repo_values.keys())
            query = f"INSERT INTO github_repos ({columns}) VALUES ({placeholders})"
            cursor.execute(query, list(repo_values.values()))
            logger.info(f"Added new repository {repo_full_name} to database")
            
            # Get the inserted repo ID
            repo_id = cursor.lastrowid
        
        # Add entry to ai_content table
        content_values = {
            'title': repo_data.get('name'),
            'description': repo_data.get('description') or '',
            'content': readme or '',
            'source_type_id': 2,  # 2 = GitHub
            'source_id': str(repo_id),
            'url': repo_data.get('html_url'),
            'date_created': repo_data.get('created_at'),
            'date_collected': datetime.now().isoformat(),
            'metadata': json.dumps({
                'stars': repo_data.get('stargazers_count', 0),
                'language': repo_data.get('language') or '',
                'topics': repo_data.get('topics', []),
                'forks': repo_data.get('forks_count', 0)
            })
        }
        
        # Check if content already exists
        cursor.execute(
            "SELECT id FROM ai_content WHERE source_type_id = 2 AND source_id = ?", 
            (str(repo_id),)
        )
        existing_content = cursor.fetchone()
        
        if existing_content:
            # Update existing content
            placeholders = ', '.join([f"{key} = ?" for key in content_values.keys()])
            query = f"UPDATE ai_content SET {placeholders} WHERE source_type_id = 2 AND source_id = ?"
            values = list(content_values.values()) + [str(repo_id)]
            cursor.execute(query, values)
        else:
            # Insert new content
            placeholders = ', '.join(['?'] * len(content_values))
            columns = ', '.join(content_values.keys())
            query = f"INSERT INTO ai_content ({columns}) VALUES ({placeholders})"
            cursor.execute(query, list(content_values.values()))
        
        conn.commit()
        return True
        
    except Exception as e:
        conn.rollback()
        logger.error(f"Error storing repository {repo_data.get('full_name')} in database: {str(e)}")
        return False

def collect_github_repos(max_repos=None):
    """
    Main function to collect GitHub repositories
    Returns the number of successfully processed repositories
    """
    # Check if GitHub collection is enabled
    if not config.CONTENT_SOURCES.get('github', {}).get('enabled', False):
        logger.info("GitHub collection is disabled in configuration")
        return 0
    
    # Create necessary directories
    os.makedirs(config.DATA_DIR, exist_ok=True)
    os.makedirs(os.path.join(config.DATA_DIR, 'logs'), exist_ok=True)
    
    # Connect to database
    conn = sqlite3.connect(config.DB_PATH)
    
    # Initialize session
    session = get_github_session()
    
    # Set default max_repos if not specified
    if max_repos is None:
        max_repos = config.CONTENT_SOURCES.get('github', {}).get('max_repos_per_run', 10)
    
    success_count = 0
    
    try:
        # Check rate limits first
        remaining, reset_time = get_rate_limit_info(session)
        if remaining is not None and remaining < max_repos * 3:  # Each repo might need multiple API calls
            logger.warning(f"Only {remaining} API requests remaining before rate limit reset, which might not be enough")
        
        # Get repositories to process
        repos_to_process = []
        
        # First try to use the search API
        topics = config.CONTENT_SOURCES.get('github', {}).get('topics', ['machine-learning', 'deep-learning'])
        min_stars = config.CONTENT_SOURCES.get('github', {}).get('repo_stars_minimum', 1000)
        
        search_results = search_github_repos(
            session=session,
            topics=topics,
            min_stars=min_stars,
            max_results=max_repos
        )
        
        # Extract repo full names from search results
        for repo in search_results:
            if repo.get('full_name'):
                repos_to_process.append(repo.get('full_name'))
        
        # If we didn't get enough repos from search, add from the predefined list
        if len(repos_to_process) < max_repos:
            remaining_count = max_repos - len(repos_to_process)
            for repo in GITHUB_REPOS:
                if repo not in repos_to_process and len(repos_to_process) < max_repos:
                    repos_to_process.append(repo)
        
        # Process repositories
        logger.info(f"Processing {len(repos_to_process)} repositories")
        
        for i, repo_full_name in enumerate(repos_to_process):
            logger.info(f"Processing repository {i+1}/{len(repos_to_process)}: {repo_full_name}")
            
            # Check if we need to update this repository
            if not should_update_repo(conn, repo_full_name):
                logger.info(f"Skipping {repo_full_name} - recently updated")
                continue
            
            # Get repository information
            repo_data = get_repo_info(session, repo_full_name)
            if not repo_data:
                logger.warning(f"Could not get information for {repo_full_name}, skipping")
                continue
            
            # Get repository README
            readme = get_repo_readme(session, repo_full_name)
            
            # Store in database
            if store_repo_in_db(conn, repo_data, readme):
                success_count += 1
            
            # Add a delay between repositories
            time.sleep(2)
            
            # Check if we've reached the maximum
            if success_count >= max_repos:
                logger.info(f"Reached maximum repository limit of {max_repos}")
                break
        
        logger.info(f"GitHub collection completed. Successfully processed {success_count}/{len(repos_to_process)} repositories")
        
    except Exception as e:
        logger.error(f"Error in GitHub collection process: {str(e)}")
    
    finally:
        # Close database connection
        conn.close()
    
    return success_count

if __name__ == "__main__":
    collect_github_repos() 

================================================================================
File: indexer.py
================================================================================

"""
Module for indexing transcribed content into the knowledge base
"""
import os
import json
import glob
import sqlite3
import logging
from tqdm import tqdm

from config import (
    DB_PATH,
    TRANSCRIPT_DIR
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('indexer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('indexer')

def setup_database():
    """Ensure the database is set up correctly"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # Use the updated schema with summary field and FTS4
    cursor.executescript('''
    -- Content table stores all video data
    CREATE TABLE IF NOT EXISTS videos (
        id INTEGER PRIMARY KEY,
        shortcode TEXT UNIQUE,
        account TEXT,
        filename TEXT,
        caption TEXT,
        transcript TEXT,
        summary TEXT,
        timestamp TEXT,
        download_date TEXT,
        url TEXT,
        likes INTEGER,
        comments INTEGER,
        word_count INTEGER,
        duration_seconds INTEGER,
        key_phrases TEXT
    );
    
    -- Tags table for improved filtering
    CREATE TABLE IF NOT EXISTS tags (
        id INTEGER PRIMARY KEY,
        video_id INTEGER,
        tag TEXT,
        FOREIGN KEY (video_id) REFERENCES videos(id)
    );
    
    -- Improved indexing for better performance
    CREATE INDEX IF NOT EXISTS idx_videos_account ON videos(account);
    CREATE INDEX IF NOT EXISTS idx_videos_timestamp ON videos(timestamp);
    CREATE INDEX IF NOT EXISTS idx_tags_tag ON tags(tag);
    
    -- Virtual FTS4 table for full-text search
    CREATE VIRTUAL TABLE IF NOT EXISTS videos_fts USING fts4(
        shortcode,
        account,
        caption,
        transcript,
        summary,
        timestamp,
        content=videos,
        tokenize=porter
    );
    
    -- Create triggers to keep FTS table synchronized
    CREATE TRIGGER IF NOT EXISTS videos_ai AFTER INSERT ON videos BEGIN
        INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
        VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
    END;
    
    CREATE TRIGGER IF NOT EXISTS videos_ad AFTER DELETE ON videos BEGIN
        DELETE FROM videos_fts WHERE docid = old.id;
    END;
    
    CREATE TRIGGER IF NOT EXISTS videos_au AFTER UPDATE ON videos BEGIN
        DELETE FROM videos_fts WHERE docid = old.id;
        INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
        VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
    END;
    ''')
    
    conn.commit()
    conn.close()
    
    logger.info("Database setup complete")

def extract_tags_from_caption(caption):
    """Extract hashtags from captions"""
    if not caption:
        return []
    
    words = caption.split()
    tags = [word[1:] for word in words if word.startswith('#')]
    return tags

def calculate_word_count(text):
    """Calculate word count from text"""
    if not text:
        return 0
    return len(text.split())

def index_transcripts():
    """Index all transcripts into the database"""
    setup_database()
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # Get all transcript files
    all_transcripts = []
    for root, _, _ in os.walk(TRANSCRIPT_DIR):
        transcripts = glob.glob(os.path.join(root, "*.json"))
        all_transcripts.extend(transcripts)
    
    logger.info(f"Found {len(all_transcripts)} transcripts to index")
    
    # Process each transcript
    new_count = 0
    updated_count = 0
    
    for transcript_path in tqdm(all_transcripts, desc="Indexing transcripts"):
        try:
            with open(transcript_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Extract relevant fields, using empty strings for missing data
            shortcode = data.get("shortcode", "")
            account = data.get("account", "")
            filename = data.get("filename", "")
            caption = data.get("caption", "")
            transcript_text = data.get("text", "")
            timestamp = data.get("timestamp", "")
            download_date = data.get("download_date", "")
            url = data.get("url", "")
            likes = data.get("likes", 0)
            comments = data.get("comments", 0)
            
            # Calculate word count
            word_count = calculate_word_count(transcript_text)
            
            # Duration (if available)
            duration_seconds = data.get("duration_seconds", None)
            
            # Initially no summary or key phrases
            summary = ""
            key_phrases = ""
            
            # Check if this video is already in the database
            cursor.execute("SELECT id, summary, word_count FROM videos WHERE shortcode = ?", (shortcode,))
            result = cursor.fetchone()
            
            if result:
                # Update existing record (preserve summary if it exists)
                video_id = result[0]
                existing_summary = result[1] or ""
                existing_word_count = result[2] or 0
                
                cursor.execute('''
                UPDATE videos SET
                    account = ?,
                    filename = ?,
                    caption = ?,
                    transcript = ?,
                    timestamp = ?,
                    download_date = ?,
                    url = ?,
                    likes = ?,
                    comments = ?,
                    word_count = ?,
                    duration_seconds = ?,
                    key_phrases = ?
                WHERE id = ?
                ''', (account, filename, caption, transcript_text, timestamp, 
                      download_date, url, likes, comments, word_count,
                      duration_seconds, key_phrases, video_id))
                updated_count += 1
                
                # Remove old tags
                cursor.execute("DELETE FROM tags WHERE video_id = ?", (video_id,))
                
            else:
                # Insert new record
                cursor.execute('''
                INSERT INTO videos (
                    shortcode, account, filename, caption, transcript, summary,
                    timestamp, download_date, url, likes, comments, 
                    word_count, duration_seconds, key_phrases
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (shortcode, account, filename, caption, transcript_text, summary,
                      timestamp, download_date, url, likes, comments, 
                      word_count, duration_seconds, key_phrases))
                video_id = cursor.lastrowid
                new_count += 1
            
            # Extract and save tags
            tags = extract_tags_from_caption(caption)
            for tag in tags:
                cursor.execute('''
                INSERT INTO tags (video_id, tag) VALUES (?, ?)
                ''', (video_id, tag))
            
            # Commit every 100 records
            if (new_count + updated_count) % 100 == 0:
                conn.commit()
                
        except Exception as e:
            logger.error(f"Error indexing {transcript_path}: {str(e)}")
    
    # Final commit
    conn.commit()
    conn.close()
    
    logger.info(f"Indexing complete. Added {new_count} new records, updated {updated_count} existing records.")

if __name__ == "__main__":
    index_transcripts() 

================================================================================
File: init_db.py
================================================================================

"""
Initialize the SQLite database with proper schema
"""
import os
import sqlite3
import logging

from config import DB_PATH, DATA_DIR

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('init_db.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('init_db')

def init_database():
    """Initialize the SQLite database with proper schema"""
    # Ensure data directory exists
    os.makedirs(DATA_DIR, exist_ok=True)
    
    logger.info(f"Initializing database at {DB_PATH}")
    
    # Create or connect to database
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # Read SQL schema from file
    schema_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "create_db.sql")
    
    try:
        with open(schema_path, 'r') as f:
            schema_sql = f.read()
        
        # Execute schema script
        cursor.executescript(schema_sql)
        logger.info("Successfully executed schema script")
        
    except FileNotFoundError:
        logger.warning(f"Schema file not found at {schema_path}. Using embedded schema.")
        
        # Execute embedded schema if file not found
        cursor.executescript('''
        -- Content table stores all video data
        CREATE TABLE IF NOT EXISTS videos (
            id INTEGER PRIMARY KEY,
            shortcode TEXT UNIQUE,
            account TEXT,
            filename TEXT,
            caption TEXT,
            transcript TEXT,
            summary TEXT,
            timestamp TEXT,
            download_date TEXT,
            url TEXT,
            likes INTEGER,
            comments INTEGER,
            word_count INTEGER,
            duration_seconds INTEGER,
            key_phrases TEXT
        );

        -- Tags table for improved filtering
        CREATE TABLE IF NOT EXISTS tags (
            id INTEGER PRIMARY KEY,
            video_id INTEGER,
            tag TEXT,
            FOREIGN KEY (video_id) REFERENCES videos(id)
        );

        -- Improved indexing for better performance
        CREATE INDEX IF NOT EXISTS idx_videos_account ON videos(account);
        CREATE INDEX IF NOT EXISTS idx_videos_timestamp ON videos(timestamp);
        CREATE INDEX IF NOT EXISTS idx_tags_tag ON tags(tag);

        -- Virtual FTS4 table for full-text search (using FTS4 instead of FTS5 for better compatibility)
        CREATE VIRTUAL TABLE IF NOT EXISTS videos_fts USING fts4(
            shortcode,
            account,
            caption,
            transcript,
            summary,
            timestamp,
            content=videos,
            tokenize=porter
        );

        -- Create triggers to keep FTS table synchronized
        CREATE TRIGGER IF NOT EXISTS videos_ai AFTER INSERT ON videos BEGIN
            INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
            VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
        END;

        CREATE TRIGGER IF NOT EXISTS videos_ad AFTER DELETE ON videos BEGIN
            DELETE FROM videos_fts WHERE docid = old.id;
        END;

        CREATE TRIGGER IF NOT EXISTS videos_au AFTER UPDATE ON videos BEGIN
            DELETE FROM videos_fts WHERE docid = old.id;
            INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
            VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
        END;
        ''')
        logger.info("Successfully executed embedded schema")
    
    except Exception as e:
        logger.error(f"Error initializing database: {str(e)}")
        raise
    
    # Commit changes and close connection
    conn.commit()
    conn.close()
    
    logger.info(f"Database initialized at {DB_PATH}")
    logger.info("You can now run the pipeline to populate the database with your video data.")

if __name__ == "__main__":
    init_database() 

================================================================================
File: README.md
================================================================================

# Instagram-Scraper
Instagram scraper that i created to pass through instagram's strict rate limiting using cheeky tricks


================================================================================
File: run.py
================================================================================

"""
Main script to run the complete Instagram Knowledge Base system
"""
import os
import argparse
import logging
from time import time

# Import our modules
from config import DATA_DIR
import downloader
import transcriber
import indexer
import summarizer
from app import app

# Import the new modules
try:
    import db_migration
    import github_collector
    import arxiv_collector
    import concept_extractor
    has_additional_modules = True
except ImportError:
    has_additional_modules = False

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('instagram_kb.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('main')

def setup():
    """Setup necessary directories"""
    os.makedirs(DATA_DIR, exist_ok=True)

def run_downloader():
    """Run the Instagram downloader"""
    logger.info("Starting Instagram content download")
    start_time = time()
    downloader.download_from_instagram()
    logger.info(f"Download completed in {time() - start_time:.2f} seconds")

def run_transcriber():
    """Run the audio extraction and transcription"""
    logger.info("Starting audio extraction and transcription")
    start_time = time()
    transcriber.process_videos()
    logger.info(f"Transcription completed in {time() - start_time:.2f} seconds")

def run_summarizer():
    """Run the transcript summarization using Claude"""
    logger.info("Starting transcript summarization using Claude")
    start_time = time()
    summarizer.summarize_transcripts()
    logger.info(f"Summarization completed in {time() - start_time:.2f} seconds")

def run_indexer():
    """Run the knowledge base indexer"""
    logger.info("Starting indexing of transcripts")
    start_time = time()
    indexer.index_transcripts()
    logger.info(f"Indexing completed in {time() - start_time:.2f} seconds")

def run_web_interface():
    """Run the web interface"""
    logger.info("Starting web interface")
    app.run(host='0.0.0.0', port=5000)

def run_db_migration():
    """Run database migration to support multiple content sources"""
    if not has_additional_modules:
        logger.error("Database migration module not available")
        return
    
    logger.info("Starting database migration")
    start_time = time()
    success = db_migration.migrate_database()
    
    if success:
        logger.info(f"Database migration completed in {time() - start_time:.2f} seconds")
    else:
        logger.error(f"Database migration failed after {time() - start_time:.2f} seconds")

def run_github_collector(max_repos=None):
    """Run GitHub repository collection"""
    if not has_additional_modules:
        logger.error("GitHub collector module not available")
        return
    
    logger.info("Starting GitHub repository collection")
    start_time = time()
    success_count = github_collector.collect_github_repos(max_repos=max_repos)
    logger.info(f"GitHub collection completed in {time() - start_time:.2f} seconds, processed {success_count} repositories")

def run_papers_collector(max_papers=None, force_update=False):
    """Run ArXiv research paper collection"""
    if not has_additional_modules:
        logger.error("ArXiv collector module not available")
        return
    
    logger.info("Starting ArXiv research paper collection")
    start_time = time()
    papers_added = arxiv_collector.collect_papers(max_papers=max_papers, force_update=force_update)
    logger.info(f"ArXiv collection completed in {time() - start_time:.2f} seconds, added {papers_added} new papers")

def run_concept_extractor(limit=None, source_type=None):
    """Run concept extraction on content"""
    if not has_additional_modules:
        logger.error("Concept extractor module not available")
        return
    
    logger.info("Starting concept extraction")
    start_time = time()
    
    if source_type:
        logger.info(f"Processing {limit or 'all'} items from source type: {source_type}")
        processed = concept_extractor.process_unprocessed_content(limit=limit or 5, source_type=source_type)
        logger.info(f"Processed {processed} items from {source_type}")
    else:
        # Process some content from each source type
        total_processed = 0
        for src_type in ["research_paper", "github", "instagram"]:
            logger.info(f"Processing source type: {src_type}")
            processed = concept_extractor.process_unprocessed_content(limit=limit or 3, source_type=src_type)
            logger.info(f"Processed {processed} items from {src_type}")
            total_processed += processed
        
        logger.info(f"Concept extraction completed in {time() - start_time:.2f} seconds, processed {total_processed} items")

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='Instagram Knowledge Base')
    parser.add_argument('--download', action='store_true', help='Run the downloader module')
    parser.add_argument('--transcribe', action='store_true', help='Run the transcription module')
    parser.add_argument('--summarize', action='store_true', help='Run the summarization module')
    parser.add_argument('--index', action='store_true', help='Run the indexer module')
    parser.add_argument('--web', action='store_true', help='Run the web interface')
    parser.add_argument('--all', action='store_true', help='Run the complete pipeline')
    
    # Add new arguments for additional modules
    if has_additional_modules:
        parser.add_argument('--migrate', action='store_true', help='Run database migration')
        parser.add_argument('--github', action='store_true', help='Run GitHub repository collection')
        parser.add_argument('--github-max', type=int, help='Maximum number of GitHub repositories to collect')
        parser.add_argument('--papers', action='store_true', help='Run ArXiv research paper collection')
        parser.add_argument('--papers-max', type=int, help='Maximum number of papers to collect')
        parser.add_argument('--force-update', action='store_true', help='Force update of existing content')
        parser.add_argument('--concepts', action='store_true', help='Run AI concept extraction')
        parser.add_argument('--concepts-limit', type=int, help='Maximum number of items to process for concept extraction')
        parser.add_argument('--concepts-source', choices=['research_paper', 'github', 'instagram'], 
                            help='Only extract concepts from this source type')
    
    args = parser.parse_args()
    
    # Setup directories
    setup()
    
    # Run requested modules
    if args.all or args.download:
        run_downloader()
    
    if args.all or args.transcribe:
        run_transcriber()
    
    if args.all or args.summarize:
        run_summarizer()
    
    if args.all or args.index:
        run_indexer()
    
    # Run new modules if available
    if has_additional_modules:
        if args.all or args.migrate:
            run_db_migration()
        
        if args.all or args.github:
            max_repos = args.github_max if hasattr(args, 'github_max') else None
            run_github_collector(max_repos=max_repos)
            
        if args.all or args.papers:
            max_papers = args.papers_max if hasattr(args, 'papers_max') else None
            force_update = args.force_update if hasattr(args, 'force_update') else False
            run_papers_collector(max_papers=max_papers, force_update=force_update)
            
        if args.all or args.concepts:
            limit = args.concepts_limit if hasattr(args, 'concepts_limit') else None
            source_type = args.concepts_source if hasattr(args, 'concepts_source') else None
            run_concept_extractor(limit=limit, source_type=source_type)
    
    if args.all or args.web:
        run_web_interface()
    
    # If no arguments provided, show help
    no_args = not (args.download or args.transcribe or args.summarize or 
                  args.index or args.web or args.all)
    
    # Check for new arguments if modules are available
    if has_additional_modules:
        no_args = no_args and not (args.migrate or args.github or args.papers or args.concepts)
    
    if no_args:
        parser.print_help()

if __name__ == "__main__":
    main() 

================================================================================
File: summarizer.py
================================================================================

"""
Module for summarizing video transcripts using Claude API
"""
import os
import json
import time
import sqlite3
import logging
import re
from anthropic import Anthropic

from config import DB_PATH, TRANSCRIPT_DIR, DATA_DIR

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('summarizer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('summarizer')

class ClaudeSummarizer:
    def __init__(self, api_key=None):
        """Initialize the Claude summarizer with API key"""
        # Update to use the current API initialization pattern (v0.49.0)
        api_key = api_key or os.environ.get("ANTHROPIC_API_KEY", "")
        self.client = Anthropic(api_key=api_key)
        
        self.cache_dir = os.path.join(DATA_DIR, "summaries_cache")
        os.makedirs(self.cache_dir, exist_ok=True)
        self.cache_file = os.path.join(self.cache_dir, "summary_cache.json")
        self._load_cache()
    
    def _load_cache(self):
        """Load summary cache from file"""
        try:
            with open(self.cache_file, 'r') as f:
                self.cache = json.load(f)
                logger.info(f"Loaded {len(self.cache)} cached summaries")
        except (FileNotFoundError, json.JSONDecodeError):
            self.cache = {}
            logger.info("Created new summary cache")
    
    def _save_cache(self):
        """Save summary cache to file"""
        with open(self.cache_file, 'w') as f:
            json.dump(self.cache, f)
            logger.info(f"Saved {len(self.cache)} summaries to cache")
    
    def _create_enhanced_prompt(self, transcript, metadata=None):
        """Create an enhanced prompt for Claude with context about the video"""
        context = ""
        if metadata:
            context = f"Video by: {metadata.get('account', 'Unknown')}\n"
            if metadata.get('timestamp'):
                context += f"Posted: {metadata.get('timestamp', '').split('T')[0]}\n"
            if metadata.get('caption'):
                context += f"Caption: {metadata.get('caption', 'No caption')}\n"
        
        prompt = f"""Analyze this Instagram video transcript and provide structured information in your response:

{context}
TRANSCRIPT:
{transcript}

Please extract the following information:
1. Summary: Concise overview of the main points (80-100 words max)
2. Key Topics: 3-5 main topics covered
3. Entities Mentioned: People, products, services, or concepts mentioned
4. Tone & Style: Formal/informal, educational/conversational, etc.
5. Key Insights: 2-3 main takeaways or insights
6. Actionable Information: Any specific advice, steps, or actionable items
7. Content Type: What kind of content is this (tutorial, conversation, review, educational, etc.)

Provide your analysis in this format:

SUMMARY: [Your concise summary]

KEY TOPICS:
- [Topic 1]
- [Topic 2]
- [Topic 3]

ENTITIES MENTIONED:
- [Entity 1] (person/product/concept)
- [Entity 2] (person/product/concept)

TONE & STYLE: [Analysis of tone and presentation style]

KEY INSIGHTS:
- [Insight 1]
- [Insight 2]

ACTIONABLE INFORMATION:
- [Actionable item 1] 
- [Actionable item 2]

CONTENT TYPE: [content type classification]
"""
        return prompt
    
    def _extract_key_phrases(self, transcript, summary):
        """Extract key phrases and terms from transcript and summary"""
        combined_text = f"{summary} {transcript}"
        # Split into words, lowercase, and remove punctuation
        words = re.findall(r'\b\w+\b', combined_text.lower())
        
        # Count word frequencies (excluding common words)
        common_words = {'the', 'and', 'that', 'for', 'you', 'with', 'this', 'was', 'are', 'have', 
                       'its', 'they', 'from', 'but', 'not', 'what', 'all', 'were', 'when', 'your',
                       'can', 'said', 'there', 'use', 'been', 'has', 'would', 'each', 'which', 'she',
                       'how', 'their', 'will', 'other', 'about', 'out', 'many', 'then', 'them', 'these',
                       'some', 'her', 'him', 'into', 'more', 'could', 'know', 'like', 'just'}
        
        word_freq = {}
        for word in words:
            if word not in common_words and len(word) > 3:  # Exclude common words and short words
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Get top N words by frequency
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # Extract 2-3 word phrases using regex patterns
        phrase_patterns = [
            r'\b\w+\s+\w+\b',           # 2-word phrases
            r'\b\w+\s+\w+\s+\w+\b'      # 3-word phrases
        ]
        
        phrases = []
        for pattern in phrase_patterns:
            phrases.extend(re.findall(pattern, combined_text.lower()))
        
        # Count phrase frequencies and exclude common phrases
        phrase_freq = {}
        for phrase in phrases:
            if not any(w in common_words for w in phrase.split() if len(w) <= 3):
                if len(phrase.split()) > 1:  # Ensure it's a real phrase
                    phrase_freq[phrase] = phrase_freq.get(phrase, 0) + 1
        
        # Get top N phrases by frequency
        top_phrases = sorted(phrase_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # Combine top words and phrases
        key_phrases = [word for word, _ in top_words]
        key_phrases.extend([phrase for phrase, _ in top_phrases])
        
        # Remove duplicates and limit to 10 items
        unique_key_phrases = []
        for phrase in key_phrases:
            if not any(phrase in p for p in unique_key_phrases):
                unique_key_phrases.append(phrase)
                if len(unique_key_phrases) >= 10:
                    break
        
        return unique_key_phrases
    
    def _parse_structured_summary(self, claude_response):
        """Parse Claude's structured response into components"""
        result = {
            'summary': '',
            'key_topics': [],
            'entities': [],
            'tone': '',
            'key_insights': [],
            'actionable_items': [],
            'content_type': ''
        }
        
        # Extract the summary
        summary_match = re.search(r'SUMMARY:\s*(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if summary_match:
            result['summary'] = summary_match.group(1).strip()
        
        # Extract key topics
        topics_section = re.search(r'KEY TOPICS:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if topics_section:
            topics = re.findall(r'-\s*(.*?)(?=\n|$)', topics_section.group(1))
            result['key_topics'] = [topic.strip() for topic in topics if topic.strip()]
        
        # Extract entities
        entities_section = re.search(r'ENTITIES MENTIONED:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if entities_section:
            entities = re.findall(r'-\s*(.*?)(?=\n|$)', entities_section.group(1))
            result['entities'] = [entity.strip() for entity in entities if entity.strip()]
        
        # Extract tone
        tone_match = re.search(r'TONE & STYLE:\s*(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if tone_match:
            result['tone'] = tone_match.group(1).strip()
        
        # Extract insights
        insights_section = re.search(r'KEY INSIGHTS:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if insights_section:
            insights = re.findall(r'-\s*(.*?)(?=\n|$)', insights_section.group(1))
            result['key_insights'] = [insight.strip() for insight in insights if insight.strip()]
        
        # Extract actionable items
        actionable_section = re.search(r'ACTIONABLE INFORMATION:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if actionable_section:
            actions = re.findall(r'-\s*(.*?)(?=\n|$)', actionable_section.group(1))
            result['actionable_items'] = [action.strip() for action in actions if action.strip()]
        
        # Extract content type
        content_match = re.search(r'CONTENT TYPE:\s*(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if content_match:
            result['content_type'] = content_match.group(1).strip()
        
        return result
    
    def summarize(self, transcript, shortcode, metadata=None, max_retries=3):
        """Generate a summary using Claude API with retry logic"""
        # Check cache first
        if shortcode in self.cache:
            logger.info(f"Using cached summary for {shortcode}")
            return self.cache[shortcode]
        
        # If transcript is too short, use it as the summary
        if len(transcript.split()) < 30:
            logger.info(f"Transcript too short for {shortcode}, using as summary")
            
            # Even for short transcripts, provide some structure
            structured_response = {
                'summary': transcript,
                'key_topics': self._extract_key_phrases(transcript, transcript)[:3],
                'entities': [],
                'tone': 'Brief',
                'key_insights': [],
                'actionable_items': [],
                'content_type': 'Short clip'
            }
            
            summary_text = f"SUMMARY: {structured_response['summary']}\n\n"
            summary_text += "KEY TOPICS:\n" + "\n".join([f"- {topic}" for topic in structured_response['key_topics']])
            summary_text += "\n\nCONTENT TYPE: Short clip"
            
            self.cache[shortcode] = summary_text
            self._save_cache()
            return summary_text
        
        # Retry logic with exponential backoff
        retries = 0
        while retries <= max_retries:
            try:
                logger.info(f"Generating summary for {shortcode} (Attempt {retries+1}/{max_retries+1})")
                
                # Create enhanced prompt
                prompt = self._create_enhanced_prompt(transcript, metadata)
                
                # Use the current API pattern for Anthropic v0.49.0
                response = self.client.messages.create(
                    model="claude-3-haiku-20240307",  # More cost-effective model for batch processing
                    max_tokens=1024,
                    messages=[
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ]
                )
                
                # Get the text content from the response
                summary = response.content[0].text
                
                # Extract structured information from the response
                structured_data = self._parse_structured_summary(summary)
                
                # Extract key phrases if not already present in the response
                if not structured_data['key_topics'] or len(structured_data['key_topics']) < 3:
                    key_phrases = self._extract_key_phrases(transcript, structured_data['summary'])
                    structured_data['key_topics'] = key_phrases[:5]
                
                # Cache the result (store the full text response)
                self.cache[shortcode] = summary
                self._save_cache()
                
                logger.info(f"Successfully generated summary for {shortcode}")
                return summary
                
            except Exception as e:
                retries += 1
                logger.error(f"Error generating summary (Attempt {retries}/{max_retries+1}): {str(e)}")
                if retries <= max_retries:
                    wait_time = 2 ** retries + (retries * 2)  # Exponential backoff
                    logger.info(f"Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Failed to generate summary after {max_retries+1} attempts")
                    # Provide a basic structured response for failed summaries
                    basic_response = (
                        "SUMMARY: Summary generation failed due to API errors.\n\n"
                        "KEY TOPICS:\n- Unknown\n\n"
                        "CONTENT TYPE: Unknown"
                    )
                    return basic_response
        
        return "Summary generation failed"


def get_video_metadata(shortcode, conn):
    """Fetch metadata for a video from the database"""
    cursor = conn.cursor()
    cursor.execute(
        "SELECT account, timestamp, caption FROM videos WHERE shortcode = ?",
        (shortcode,)
    )
    result = cursor.fetchone()
    if result:
        return {
            'account': result['account'],
            'timestamp': result['timestamp'],
            'caption': result['caption']
        }
    return None


def analyze_content_quality(transcript):
    """Calculate metrics about content quality"""
    if not transcript:
        return {}
        
    # Calculate basic metrics
    words = transcript.split()
    word_count = len(words)
    
    # Estimate dialogue percentage (if there are colons, quotes, etc.)
    dialogue_markers = [":", "?", '"', "'", "says", "said", "asked"]
    dialogue_score = sum(transcript.count(marker) for marker in dialogue_markers) / max(1, len(transcript) / 100)
    
    # Calculate average word length (a proxy for complexity)
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)
    
    # Estimate reading time (average person reads ~200-250 words per minute)
    reading_time_seconds = (word_count / 200) * 60
    
    return {
        "word_count": word_count,
        "dialogue_ratio": min(1.0, dialogue_score),
        "avg_word_length": avg_word_length,
        "estimated_read_time_seconds": reading_time_seconds
    }


def process_transcripts_with_claude(batch_size=10, delay_between_batches=5):
    """Process transcripts in batches using Claude API"""
    try:
        # Check for API key
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            logger.error("ANTHROPIC_API_KEY environment variable not set. Please set it before running the summarizer.")
            return
            
        summarizer = ClaudeSummarizer()
        
        # Get videos that need summarization
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Find videos with transcripts but no summaries
        cursor.execute(
            "SELECT id, shortcode, account, transcript FROM videos WHERE transcript IS NOT NULL AND transcript != '' AND (summary IS NULL OR summary = '')"
        )
        videos = cursor.fetchall()
        
        total_videos = len(videos)
        logger.info(f"Found {total_videos} videos that need summarization")
        
        if total_videos == 0:
            logger.info("No videos to summarize. Exiting.")
            conn.close()
            return
        
        # Process in batches
        for i in range(0, total_videos, batch_size):
            batch = videos[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_videos - 1) // batch_size + 1
            
            logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} videos)")
            
            for video in batch:
                video_id = video['id']
                shortcode = video['shortcode']
                account = video['account']
                transcript = video['transcript']
                
                # Get full metadata for enhanced context
                metadata = get_video_metadata(shortcode, conn)
                
                # Calculate content quality metrics
                quality_metrics = analyze_content_quality(transcript)
                word_count = quality_metrics.get('word_count', 0)
                
                logger.info(f"Summarizing video {shortcode} ({word_count} words)")
                summary = summarizer.summarize(transcript, shortcode, metadata=metadata)
                
                # Extract key phrases for database storage
                structured_data = summarizer._parse_structured_summary(summary)
                key_phrases_json = json.dumps(structured_data.get('key_topics', []))
                
                # Update database with summary, word count and content metrics
                cursor.execute(
                    """UPDATE videos SET 
                       summary = ?, 
                       word_count = ?,
                       key_phrases = ?,
                       duration_seconds = ?
                       WHERE id = ?""",
                    (summary, word_count, key_phrases_json, 
                     quality_metrics.get('estimated_read_time_seconds', 0), video_id)
                )
                conn.commit()
                logger.info(f"Updated summary and metrics for video {shortcode}")
            
            # Respect API rate limits between batches
            processed_count = min(i + batch_size, total_videos)
            logger.info(f"Processed {processed_count}/{total_videos} videos")
            
            if i + batch_size < total_videos:
                logger.info(f"Waiting {delay_between_batches} seconds before next batch")
                time.sleep(delay_between_batches)
        
        conn.close()
        logger.info("Summarization process complete")
        
    except Exception as e:
        logger.error(f"Error in batch processing: {str(e)}")
        raise


def summarize_transcripts():
    """Main entry point for transcript summarization"""
    logger.info("Starting transcript summarization using Claude")
    start_time = time.time()
    process_transcripts_with_claude()
    logger.info(f"Summarization completed in {time.time() - start_time:.2f} seconds")


if __name__ == "__main__":
    summarize_transcripts() 

================================================================================
File: test_db.py
================================================================================

import sqlite3

# Connect to the database
conn = sqlite3.connect('data/knowledge_base.db')
cursor = conn.cursor()

# Get the list of tables
cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
tables = cursor.fetchall()
print("Tables in the database:")
for table in tables:
    print(f"- {table[0]}")

# Count records in videos table
cursor.execute("SELECT COUNT(*) FROM videos;")
count = cursor.fetchone()[0]
print(f"\nNumber of videos in database: {count}")

# Close the connection
conn.close()

print("\nDatabase access test completed successfully!") 

================================================================================
File: test_proxy.py
================================================================================

#!/usr/bin/env python
"""
Test script for Bright Data residential proxy
"""
import sys
import requests
import os
import json
from datetime import datetime
import urllib3

# Disable SSL warnings for testing
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def test_brightdata_proxy():
    """Test Bright Data residential proxy"""
    print("Testing Bright Data residential proxy...")
    
    # Proxy configuration
    proxy = "http://brd-customer-hl_c7bff232-zone-residential_proxy1-country-us:w46vs0z46xmc@brd.superproxy.io:33335"
    
    # Setup proxies for requests
    proxies = {
        "http": proxy,
        "https": proxy
    }
    
    try:
        # First, test with the Bright Data test endpoint
        response = requests.get(
            "https://geo.brdtest.com/welcome.txt?product=resi&method=native",
            proxies=proxies,
            timeout=10,
            verify=False  # Disable SSL verification
        )
        
        if response.status_code == 200:
            print("✅ Bright Data test successful!")
            print(f"Response: {response.text.strip()}")
        else:
            print(f"❌ Bright Data test failed with status code: {response.status_code}")
            print(f"Response: {response.text}")
        
        # Then test with Instagram
        print("\nTesting access to Instagram...")
        instagram_response = requests.get(
            "https://www.instagram.com/favicon.ico",
            proxies=proxies,
            timeout=10,
            verify=False,  # Disable SSL verification
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
        )
        
        if instagram_response.status_code == 200:
            print("✅ Instagram test successful!")
            print(f"Status code: {instagram_response.status_code}")
            print(f"Content length: {len(instagram_response.content)} bytes")
        else:
            print(f"❌ Instagram test failed with status code: {instagram_response.status_code}")
            
        # Save test results
        results = {
            "timestamp": datetime.now().isoformat(),
            "brightdata_test": {
                "status_code": response.status_code,
                "success": response.status_code == 200,
                "response_text": response.text.strip()
            },
            "instagram_test": {
                "status_code": instagram_response.status_code,
                "success": instagram_response.status_code == 200,
                "content_length": len(instagram_response.content)
            }
        }
        
        # Create data directory if it doesn't exist
        os.makedirs(os.path.join("data", "logs"), exist_ok=True)
        
        # Save results to a file
        with open(os.path.join("data", "logs", "proxy_test_results.json"), "w") as f:
            json.dump(results, f, indent=2)
            
        print("\nTest results saved to data/logs/proxy_test_results.json")
        
    except Exception as e:
        print(f"❌ Error testing proxy: {str(e)}")
        return False
    
    return True

if __name__ == "__main__":
    test_brightdata_proxy() 

================================================================================
File: transcriber.py
================================================================================

"""
Module for extracting audio from videos and transcribing with Whisper
"""
import os
import json
import glob
import subprocess
import logging
from tqdm import tqdm
import whisper

from config import (
    DOWNLOAD_DIR,
    AUDIO_DIR,
    TRANSCRIPT_DIR,
    WHISPER_MODEL
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('transcriber.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('transcriber')

def setup_directories():
    """Create necessary directories if they don't exist"""
    os.makedirs(AUDIO_DIR, exist_ok=True)
    os.makedirs(TRANSCRIPT_DIR, exist_ok=True)
    
    # Create account-specific directories
    for account_dir in os.listdir(DOWNLOAD_DIR):
        if os.path.isdir(os.path.join(DOWNLOAD_DIR, account_dir)):
            os.makedirs(os.path.join(AUDIO_DIR, account_dir), exist_ok=True)
            os.makedirs(os.path.join(TRANSCRIPT_DIR, account_dir), exist_ok=True)

def extract_audio(video_path, audio_path):
    """Extract audio from video using FFmpeg"""
    try:
        cmd = f'ffmpeg -i "{video_path}" -vn -acodec pcm_s16le -ar 16000 -ac 1 "{audio_path}" -y'
        subprocess.call(cmd, shell=True)
        return True
    except Exception as e:
        logger.error(f"Error extracting audio from {video_path}: {str(e)}")
        return False

def process_videos():
    """Process all downloaded videos that haven't been transcribed yet"""
    setup_directories()
    
    # Load Whisper model
    logger.info(f"Loading Whisper model: {WHISPER_MODEL}")
    model = whisper.load_model(WHISPER_MODEL)
    logger.info("Model loaded successfully")
    
    # Get all video files
    all_videos = []
    for account_dir in os.listdir(DOWNLOAD_DIR):
        account_path = os.path.join(DOWNLOAD_DIR, account_dir)
        if os.path.isdir(account_path):
            videos = glob.glob(os.path.join(account_path, "*.mp4"))
            all_videos.extend(videos)
    
    logger.info(f"Found {len(all_videos)} total videos to process")
    
    # Process each video
    for video_path in tqdm(all_videos, desc="Processing videos"):
        # Extract account name and filename
        parts = video_path.split(os.sep)
        account = parts[-2]
        filename = os.path.basename(video_path)
        base_name = os.path.splitext(filename)[0]
        
        # Define paths
        audio_path = os.path.join(AUDIO_DIR, account, f"{base_name}.wav")
        transcript_path = os.path.join(TRANSCRIPT_DIR, account, f"{base_name}.json")
        
        # Skip if already transcribed
        if os.path.exists(transcript_path):
            continue
        
        # Extract audio if needed
        if not os.path.exists(audio_path):
            logger.info(f"Extracting audio from {filename}")
            os.makedirs(os.path.dirname(audio_path), exist_ok=True)
            if not extract_audio(video_path, audio_path):
                continue
        
        # Transcribe audio
        try:
            logger.info(f"Transcribing {filename}")
            result = model.transcribe(audio_path)
            
            # Get metadata if available
            metadata_path = os.path.join(DOWNLOAD_DIR, account, "metadata", f"{base_name}.json")
            metadata = {}
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            
            # Create transcript with metadata
            transcript_data = {
                "text": result["text"],
                "segments": result["segments"],
                "language": result["language"],
                "filename": filename,
                "account": account,
                **metadata
            }
            
            # Save transcript
            os.makedirs(os.path.dirname(transcript_path), exist_ok=True)
            with open(transcript_path, 'w', encoding='utf-8') as f:
                json.dump(transcript_data, f, ensure_ascii=False, indent=4)
            
            logger.info(f"Transcription complete for {filename}")
            
        except Exception as e:
            logger.error(f"Error transcribing {filename}: {str(e)}")
    
    logger.info("Transcription process completed")

if __name__ == "__main__":
    process_videos() 