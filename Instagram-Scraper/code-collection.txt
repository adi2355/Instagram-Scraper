

================================================================================
File: app.py
================================================================================

"""
Web interface for the Instagram Knowledge Base
"""
import os
import re
import sqlite3
from functools import lru_cache
from flask import Flask, render_template, request, jsonify, g, send_from_directory, redirect, url_for

from config import (
    DB_PATH,
    WEB_PORT,
    DEBUG_MODE,
    DOWNLOAD_DIR,
    DATA_DIR
)

app = Flask(__name__)

def get_db():
    """Get database connection with row factory for easy access"""
    db = getattr(g, '_database', None)
    if db is None:
        db = g._database = sqlite3.connect(DB_PATH)
        db.row_factory = sqlite3.Row
    return db

@app.teardown_appcontext
def close_connection(exception):
    """Close database connection when app context ends"""
    db = getattr(g, '_database', None)
    if db is not None:
        db.close()

@app.route('/')
def index():
    """Home page with search interface"""
    return render_template('index.html')

@app.route('/search')
def search():
    """Search endpoint"""
    query = request.args.get('query', '')
    account_filter = request.args.get('account', '')
    tag_filter = request.args.get('tag', '')
    page = int(request.args.get('page', 1))
    per_page = int(request.args.get('per_page', 10))
    
    # Calculate offset
    offset = (page - 1) * per_page
    
    db = get_db()
    cursor = db.cursor()
    
    # Get available accounts for filtering
    cursor.execute("SELECT DISTINCT account FROM videos ORDER BY account")
    accounts = [row['account'] for row in cursor.fetchall()]
    
    # Get popular tags for filtering
    cursor.execute('''
    SELECT tag, COUNT(*) as count 
    FROM tags 
    GROUP BY tag 
    ORDER BY count DESC 
    LIMIT 20
    ''')
    tags = [{'tag': row['tag'], 'count': row['count']} for row in cursor.fetchall()]
    
    results = []
    total_results = 0
    
    if query or account_filter or tag_filter:
        # Base query
        sql = '''
        SELECT v.id, v.shortcode, v.account, v.caption, v.transcript, 
               v.timestamp, v.url, v.likes, v.comments
        FROM videos v
        '''
        
        params = []
        where_clauses = []
        
        # Add tag filter if specified
        if tag_filter:
            sql += "JOIN tags t ON v.id = t.video_id "
            where_clauses.append("t.tag = ?")
            params.append(tag_filter)
        
        # Add account filter if specified
        if account_filter:
            where_clauses.append("v.account = ?")
            params.append(account_filter)
        
        # Add search query if specified
        if query:
            sql += "JOIN videos_fts fts ON v.id = fts.docid "  # Changed from rowid to docid for FTS4
            where_clauses.append("videos_fts MATCH ?")
            params.append(query)
            
            # FTS4 doesn't support the snippet function, so we'll remove this line
            # and handle highlighting in application code later
        
        # Combine where clauses
        if where_clauses:
            sql += "WHERE " + " AND ".join(where_clauses)
        
        # Get total results count
        count_sql = f"SELECT COUNT(*) as count FROM ({sql})"
        cursor.execute(count_sql, params)
        total_results = cursor.fetchone()['count']
        
        # Add order and limit
        if query:
            # FTS4 doesn't have built-in rank function
            sql += " ORDER BY v.timestamp DESC"
        else:
            sql += " ORDER BY v.timestamp DESC"
            
        sql += " LIMIT ? OFFSET ?"
        params.extend([per_page, offset])
        
        # Execute query
        cursor.execute(sql, params)
        results = cursor.fetchall()
    
    # Format results for template
    formatted_results = []
    for row in results:
        # Get tags for this video
        cursor.execute("SELECT tag FROM tags WHERE video_id = ?", (row['id'],))
        video_tags = [tag['tag'] for tag in cursor.fetchall()]
        
        # Format the date
        timestamp = row['timestamp'].split('T')[0] if row['timestamp'] else 'Unknown'
        
        # Get video path
        video_path = f"/video/{row['account']}/{row['shortcode']}"
        
        # Create our own snippet since snippet() isn't available in FTS4
        transcript = row['transcript'] or ""
        
        # Default snippet (first ~200 chars of transcript)
        if len(transcript) > 200:
            snippet = transcript[:200] + '...'
        else:
            snippet = transcript
        
        # If we have a search query, try to find a relevant part of the transcript
        if query and transcript:
            query_words = query.lower().split()
            transcript_lower = transcript.lower()
            
            # Find the first occurrence of any query word
            position = -1
            for word in query_words:
                pos = transcript_lower.find(word)
                if pos != -1:
                    position = pos
                    break
            
            # Extract a section around the match if found
            if position != -1:
                # Get 50 chars before and 150 after the match position
                start = max(0, position - 50)
                end = min(len(transcript), position + 150)
                
                # Get the relevant section
                context = transcript[start:end]
                
                # Add ellipsis if needed
                if start > 0:
                    context = '...' + context
                if end < len(transcript):
                    context += '...'
                
                snippet = context
        
        # Simple highlighting for matched terms
        if query and transcript:
            for word in query.lower().split():
                # Case-insensitive replace with HTML highlighting
                word_pattern = word.lower()
                start = 0
                while True:
                    start_pos = snippet.lower().find(word_pattern, start)
                    if start_pos == -1:
                        break
                    
                    end_pos = start_pos + len(word)
                    original_word = snippet[start_pos:end_pos]
                    snippet = snippet[:start_pos] + f'<mark>{original_word}</mark>' + snippet[end_pos:]
                    
                    # Move past this match
                    start = start_pos + len(f'<mark>{original_word}</mark>')
        
        formatted_results.append({
            'id': row['id'],
            'shortcode': row['shortcode'],
            'account': row['account'],
            'caption': row['caption'],
            'snippet': snippet,
            'timestamp': timestamp,
            'url': row['url'],
            'likes': row['likes'],
            'comments': row['comments'],
            'tags': video_tags,
            'video_path': video_path
        })
    
    # Calculate pagination info
    total_pages = (total_results + per_page - 1) // per_page if total_results > 0 else 1
    has_prev = page > 1
    has_next = page < total_pages
    
    return render_template(
        'search.html',
        query=query,
        account_filter=account_filter,
        tag_filter=tag_filter,
        results=formatted_results,
        accounts=accounts,
        tags=tags,
        total_results=total_results,
        page=page,
        per_page=per_page,
        total_pages=total_pages,
        has_prev=has_prev,
        has_next=has_next
    )

@app.route('/video/<account>/<shortcode>')
def video(account, shortcode):
    """Video detail page"""
    db = get_db()
    cursor = db.cursor()
    
    # Get video details
    cursor.execute('''
    SELECT v.* FROM videos v
    WHERE v.account = ? AND v.shortcode = ?
    ''', (account, shortcode))
    video = cursor.fetchone()
    
    if not video:
        return "Video not found", 404
    
    # Get tags
    cursor.execute("SELECT tag FROM tags WHERE video_id = ?", (video['id'],))
    tags = [tag['tag'] for tag in cursor.fetchall()]
    
    # Find video file
    video_filename = None
    account_dir = os.path.join(DOWNLOAD_DIR, account)
    if os.path.exists(account_dir):
        for filename in os.listdir(account_dir):
            if shortcode in filename and filename.endswith('.mp4'):
                video_filename = f"/media/{account}/{filename}"
                break
    
    return render_template(
        'video.html',
        video=video,
        tags=tags,
        video_filename=video_filename
    )

@app.route('/api/search')
def api_search():
    """API endpoint for search (for AJAX requests)"""
    query = request.args.get('query', '')
    account = request.args.get('account', '')
    tag = request.args.get('tag', '')
    page = int(request.args.get('page', 1))
    per_page = int(request.args.get('per_page', 10))
    
    db = get_db()
    cursor = db.cursor()
    
    # Start building the query
    sql_select = """
        SELECT v.id, v.shortcode, v.account, v.caption, v.transcript, v.summary,
               v.timestamp, v.url, v.likes, v.comments, v.word_count, v.duration_seconds
    """
    
    sql_from = " FROM videos v"
    sql_where = ""
    sql_order = ""
    params = []
    
    # Add search condition if query provided
    if query:
        sql_from += " JOIN videos_fts fts ON v.id = fts.docid"
        sql_where = " WHERE videos_fts MATCH ?"
        params.append(query)
        
        # Custom snippets for transcript and summary
        sql_select += """,
            (SELECT substr(v.transcript, 
                max(0, instr(lower(v.transcript), lower(?)) - 50), 
                150)) AS transcript_snippet,
            (SELECT substr(v.summary, 
                max(0, instr(lower(v.summary), lower(?)) - 25), 
                100)) AS summary_snippet
        """
        params.extend([query, query])  # Add query params for snippets
        
        # Enhanced ordering that prioritizes matches in summary, then caption
        sql_order = """
        ORDER BY
            CASE 
                WHEN fts.summary MATCH ? THEN 1
                WHEN fts.caption MATCH ? THEN 2
                WHEN fts.transcript MATCH ? THEN 3
                ELSE 4
            END
        """
        params.extend([query, query, query])
    
    # Add account filter if specified
    if account:
        if sql_where:
            sql_where += " AND v.account = ?"
        else:
            sql_where = " WHERE v.account = ?"
        params.append(account)
    
    # Add tag filter if specified
    if tag:
        sql_from += " JOIN tags t ON v.id = t.video_id"
        if sql_where:
            sql_where += " AND t.tag = ?"
        else:
            sql_where = " WHERE t.tag = ?"
        params.append(tag)
    
    # Count total results for pagination
    count_sql = f"SELECT COUNT(*) as total_count {sql_from}{sql_where}"
    cursor.execute(count_sql, params)
    total_count = cursor.fetchone()['total_count']
    
    # Calculate total pages
    total_pages = (total_count + per_page - 1) // per_page
    
    # Add default sorting if no query or specific ordering
    if not sql_order:
        sql_order = " ORDER BY v.timestamp DESC"
    
    # Add pagination
    sql_limit = " LIMIT ? OFFSET ?"
    
    # Complete SQL query
    full_sql = f"{sql_select}{sql_from}{sql_where}{sql_order}{sql_limit}"
    
    # Execute with pagination parameters
    cursor.execute(full_sql, params + [per_page, (page - 1) * per_page])
    results = cursor.fetchall()
    
    # Convert to list of dicts with highlighted snippets
    result_list = []
    for row in results:
        result_dict = {key: row[key] for key in row.keys()}
        
        # Add highlighted snippets if search was performed
        if query:
            # Highlight the query term in snippets
            if 'transcript_snippet' in result_dict and result_dict['transcript_snippet']:
                result_dict['transcript_snippet'] = highlight_term(
                    result_dict['transcript_snippet'], query
                )
            
            if 'summary_snippet' in result_dict and result_dict['summary_snippet']:
                result_dict['summary_snippet'] = highlight_term(
                    result_dict['summary_snippet'], query
                )
        
        result_list.append(result_dict)
    
    return jsonify({
        'results': result_list,
        'total': total_count,
        'page': page,
        'per_page': per_page,
        'total_pages': total_pages,
        'query': query,
        'account': account,
        'tag': tag
    })

def highlight_term(text, term):
    """Highlight search term in text using HTML"""
    if not text or not term:
        return text
    
    # Simple case-insensitive replace
    # For more complex highlighting, consider using regex
    term_lower = term.lower()
    text_lower = text.lower()
    
    result = ""
    last_pos = 0
    
    # Find all occurrences of the term
    pos = text_lower.find(term_lower)
    while pos != -1:
        # Add text before the term
        result += text[last_pos:pos]
        # Add the highlighted term
        result += f"<mark>{text[pos:pos+len(term)]}</mark>"
        # Move past this occurrence
        last_pos = pos + len(term)
        # Find next occurrence
        pos = text_lower.find(term_lower, last_pos)
    
    # Add any remaining text
    result += text[last_pos:]
    
    return result

@app.route('/media/<path:path>')
def media(path):
    """Serve media files"""
    return send_from_directory(DOWNLOAD_DIR, path)

# Routes for static templates
@app.route('/about')
def about():
    """About page"""
    return render_template('about.html')

@app.route('/stats')
def stats():
    """Statistics page"""
    db = get_db()
    cursor = db.cursor()
    
    # Get general stats
    cursor.execute("SELECT COUNT(*) as total FROM videos")
    total_videos = cursor.fetchone()['total']
    
    # Get account stats
    cursor.execute('''
    SELECT account, COUNT(*) as count 
    FROM videos 
    GROUP BY account 
    ORDER BY count DESC
    ''')
    accounts = cursor.fetchall()
    
    # Get tag stats
    cursor.execute('''
    SELECT tag, COUNT(*) as count 
    FROM tags 
    GROUP BY tag 
    ORDER BY count DESC 
    LIMIT 50
    ''')
    tags = cursor.fetchall()
    
    # Get timeline stats
    cursor.execute('''
    SELECT substr(timestamp, 1, 7) as month, COUNT(*) as count 
    FROM videos 
    WHERE timestamp IS NOT NULL
    GROUP BY month 
    ORDER BY month
    ''')
    timeline = cursor.fetchall()
    
    return render_template(
        'stats.html',
        total_videos=total_videos,
        accounts=accounts,
        tags=tags,
        timeline=timeline
    )

# Caching utilities
@lru_cache(maxsize=100)
def get_video_by_shortcode(shortcode):
    """Get video details by shortcode with caching"""
    db = get_db()
    cursor = db.cursor()
    cursor.execute("SELECT * FROM videos WHERE shortcode = ?", [shortcode])
    return cursor.fetchone()

@lru_cache(maxsize=30)
def get_recent_videos(limit=10, account=None):
    """Get recent videos with caching"""
    db = get_db()
    cursor = db.cursor()
    
    if account:
        cursor.execute(
            "SELECT * FROM videos WHERE account = ? ORDER BY timestamp DESC LIMIT ?", 
            [account, limit]
        )
    else:
        cursor.execute(
            "SELECT * FROM videos ORDER BY timestamp DESC LIMIT ?", 
            [limit]
        )
    
    return cursor.fetchall()

@lru_cache(maxsize=20)
def get_video_statistics():
    """Get video statistics with caching"""
    db = get_db()
    cursor = db.cursor()
    
    # Get total videos
    cursor.execute("SELECT COUNT(*) as video_count FROM videos")
    total_videos = cursor.fetchone()['video_count']
    
    # Get videos per account
    cursor.execute(
        "SELECT account, COUNT(*) as count FROM videos GROUP BY account ORDER BY count DESC"
    )
    accounts = cursor.fetchall()
    
    # Get total duration (if available)
    cursor.execute(
        "SELECT SUM(duration_seconds) as total_duration FROM videos WHERE duration_seconds IS NOT NULL"
    )
    total_duration = cursor.fetchone()['total_duration'] or 0
    
    # Get total word count
    cursor.execute(
        "SELECT SUM(word_count) as total_words FROM videos WHERE word_count IS NOT NULL"
    )
    total_words = cursor.fetchone()['total_words'] or 0
    
    return {
        'total_videos': total_videos,
        'accounts': accounts,
        'total_duration_seconds': total_duration,
        'total_words': total_words
    }

# Clear caches when data changes
def clear_caches():
    """Clear all LRU caches"""
    get_video_by_shortcode.cache_clear()
    get_recent_videos.cache_clear()
    get_video_statistics.cache_clear()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=WEB_PORT, debug=DEBUG_MODE) 

================================================================================
File: collectCode.js
================================================================================

import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

// Default configuration for file collection
const DEFAULT_CONFIG = {
  extensions: ['.ts', '.tsx', '.js', '.md', '.json', '.py', '.sql'],
  excludeDirs: [
  '.expo',
  'android',
  'CanovaReactNativeApp/app/dataOverviews',
  'assets',
  'data',
  'ios',
  'node_modules',
  'static',
  'templates',
  'venv',
  
  
    
   
  ],
  excludeFiles: [
    '.test.',
    '.spec.',
    '.d.ts',
    '.map',
    'next-env.d.ts',
    '.gitignore',
    '.eslintrc.json',
    '.env.example',
    'components.json',
    'package-lock.json',
  ],
  excludePaths: [
    'lib/supabase.ts',
    'lib/rate-limit.ts',
    'lib/monitoring.ts',
    'lib/cache.ts',
    'lib/auth.ts',
    'scripts/test-agent1.ts',
    'scripts/test-agent2.ts',
    'scripts/test-agent3.ts',
    'scripts/test-orchestrator.ts',
    'lib/graphql/queries.ts',
    'lib/graphql/client.ts',
    'hooks/use-toast.ts',
    'components/WalletConnect.tsx',
    'components/Learn.tsx',
    'components/ContractInteraction.tsx',
    'components/Analytics.tsx',
    'app/providers.tsx',
    'app/page.tsx',
    'app/globals.css',
    'app/layout.tsx',


    'CanovaReactNativeApp/tsconfig.json',
    'CanovaReactNativeApp/app.json',
    
  ],
  maxFileSize: 1024 * 1024, // 1MB
};

// Helper function to get relative path
function getRelativePath(fullPath, rootDir) {
  return path.relative(rootDir, fullPath);
}

// Check if a file should be excluded based on config
function shouldExcludeFile(filePath, config) {
  const normalizedPath = path.normalize(filePath);

  if (config.excludeFiles.some((pattern) => normalizedPath.includes(pattern))) {
    return true;
  }

  if (config.excludePaths.some((excludePath) =>
    normalizedPath.includes(path.normalize(excludePath))
  )) {
    return true;
  }

  return false;
}

// Check if a directory should be excluded based on config
function shouldExcludeDir(dirPath, config) {
  const normalizedPath = path.normalize(dirPath);
  return config.excludeDirs.some((excludeDir) =>
    normalizedPath.includes(path.normalize(excludeDir))
  );
}

// Recursively collect files from directory
function collectFiles(dir, rootDir, config) {
  let results = [];
  const items = fs.readdirSync(dir, { withFileTypes: true });

  for (const item of items) {
    const fullPath = path.join(dir, item.name);
    const relativePath = getRelativePath(fullPath, rootDir);

    if (item.isDirectory()) {
      if (!shouldExcludeDir(fullPath, config)) {
        results = results.concat(collectFiles(fullPath, rootDir, config));
      }
    } else {
      const ext = path.extname(item.name).toLowerCase();

      if (config.extensions.includes(ext) && !shouldExcludeFile(relativePath, config)) {
        const stats = fs.statSync(fullPath);
        if (stats.size <= config.maxFileSize) {
          results.push({ path: fullPath, relativePath });
        }
      }
    }
  }

  return results;
}

// Main function to collect code
function collectCode(outputFile, customConfig = {}) {
  const config = { ...DEFAULT_CONFIG, ...customConfig };

  try {
    fs.writeFileSync(outputFile, '');

    const rootDir = process.cwd();
    console.log(`Processing project directory: ${rootDir}`);
    const files = collectFiles(rootDir, rootDir, config);

    files.sort((a, b) => a.relativePath.localeCompare(b.relativePath));

    files.forEach(({ path: filePath, relativePath }) => {
      const content = fs.readFileSync(filePath, 'utf8');
      const separator = '='.repeat(80);
      fs.appendFileSync(
        outputFile,
        `\n\n${separator}\nFile: ${relativePath}\n${separator}\n\n${content}`
      );
    });

    console.log('Collection complete!');
  } catch (error) {
    console.error('Error during collection:', error);
    process.exit(1);
  }
}

// Check if running as main module
const isMainModule = process.argv[1] === fileURLToPath(import.meta.url);

if (isMainModule) {
  const outputFile = process.argv[2] || 'code-collection.txt';
  collectCode(outputFile);
}

export { collectCode }; 

================================================================================
File: config.py
================================================================================

"""
Configuration file for Instagram Knowledge Base
"""
import os

# Content sources configuration
# Currently supports Instagram, with framework for adding more sources
CONTENT_SOURCES = [
    {"type": "instagram", "username": "rajistics", "private": False}
    # Add more sources as needed
    # {"type": "youtube", "channel_id": "CHANNEL_ID"},  # YouTube example
    # {"type": "twitter", "username": "username"},  # Twitter example
]

# Instagram accounts list (for backward compatibility)
INSTAGRAM_ACCOUNTS = [account for account in CONTENT_SOURCES if account["type"] == "instagram"]

# Instagram credentials (only needed for private accounts)
# IMPORTANT: Add your Instagram credentials here to avoid 401 Unauthorized errors
# You can either set environment variables or directly add your credentials below:
# INSTAGRAM_USERNAME = "your_instagram_username"
# INSTAGRAM_PASSWORD = "your_instagram_password"
INSTAGRAM_USERNAME = os.getenv("INSTAGRAM_USERNAME", "")
INSTAGRAM_PASSWORD = os.getenv("INSTAGRAM_PASSWORD", "")

# Multiple Instagram accounts for rotation (to reduce rate limiting)
# If using account rotation, populate this list with your accounts
INSTAGRAM_ACCOUNT_ROTATION = {
    "enabled": False,  # Disabled due to 2FA requirements
    "accounts": [
        {"username": "adi_ka_dusra_account", "password": "Ishaan07"},
        {"username": "adi_khetarpal", "password": "vijne1-Xingir-pomcob"},
        # Add your Instagram accounts here for rotation
    ]
}

# Proxy configuration
# Add your proxy servers here to rotate IPs and reduce rate limiting
PROXY_SERVERS = [
    "http://brd-customer-hl_c7bff232-zone-residential_proxy1-country-us:w46vs0z46xmc@brd.superproxy.io:33335",
    # Add more proxies as needed
]

# Configure if you want to use a specific country for proxies (US, UK, etc.)
PROXY_COUNTRY = "us"  # Change as needed, or set to None for random

# Claude API key for summarization
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")

# Directory settings
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
DOWNLOAD_DIR = os.path.join(DATA_DIR, "downloads")
AUDIO_DIR = os.path.join(DATA_DIR, "audio")
TRANSCRIPT_DIR = os.path.join(DATA_DIR, "transcripts")

# Whisper model size: tiny, base, small, medium, large
WHISPER_MODEL = "base"

# Database settings
DB_PATH = os.path.join(DATA_DIR, "knowledge_base.db")

# Web interface settings
WEB_PORT = 5001
DEBUG_MODE = True

# Rate limiting settings (to avoid IP blocks)
DOWNLOAD_DELAY = 10  # seconds between downloads (increased from 5 to reduce rate limiting)
MAX_DOWNLOADS_PER_RUN = 2  # Maximum videos to download per run
ACCOUNT_COOLDOWN_MINUTES = 60  # How long to wait before using an account again after a failure
PROXY_COOLDOWN_MINUTES = 30  # How long to wait before using a proxy again after a failure
RATE_LIMIT_WAIT = 3600  # Seconds to wait after hitting a rate limit 

================================================================================
File: create_db.sql
================================================================================

-- Content table stores all video data
CREATE TABLE IF NOT EXISTS videos (
    id INTEGER PRIMARY KEY,
    shortcode TEXT UNIQUE,
    account TEXT,
    filename TEXT,
    caption TEXT,
    transcript TEXT,
    summary TEXT,
    timestamp TEXT,
    download_date TEXT,
    url TEXT,
    likes INTEGER,
    comments INTEGER,
    word_count INTEGER,
    duration_seconds INTEGER,
    key_phrases TEXT
);

-- Tags table for improved filtering
CREATE TABLE IF NOT EXISTS tags (
    id INTEGER PRIMARY KEY,
    video_id INTEGER,
    tag TEXT,
    FOREIGN KEY (video_id) REFERENCES videos(id)
);

-- Improved indexing for better performance
CREATE INDEX IF NOT EXISTS idx_videos_account ON videos(account);
CREATE INDEX IF NOT EXISTS idx_videos_timestamp ON videos(timestamp);
CREATE INDEX IF NOT EXISTS idx_tags_tag ON tags(tag);

-- Virtual FTS4 table for full-text search (using FTS4 instead of FTS5 for better compatibility)
CREATE VIRTUAL TABLE IF NOT EXISTS videos_fts USING fts4(
    shortcode,
    account,
    caption,
    transcript,
    summary,
    timestamp,
    content=videos,
    tokenize=porter
);

-- Create triggers to keep FTS table synchronized
CREATE TRIGGER IF NOT EXISTS videos_ai AFTER INSERT ON videos BEGIN
    INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
    VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
END;

CREATE TRIGGER IF NOT EXISTS videos_ad AFTER DELETE ON videos BEGIN
    DELETE FROM videos_fts WHERE docid = old.id;
END;

CREATE TRIGGER IF NOT EXISTS videos_au AFTER UPDATE ON videos BEGIN
    DELETE FROM videos_fts WHERE docid = old.id;
    INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
    VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
END; 

================================================================================
File: downloader.py
================================================================================

"""
Module for downloading Instagram content with proper rate limiting
"""
import os
import time
import json
import logging
import random
from datetime import datetime, timedelta
import instaloader

# Add imports for proxy testing
import requests
from urllib.parse import urlparse
from urllib.parse import parse_qs
import socket
import json as json_lib
import urllib3

# Disable SSL warnings for testing
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

from config import (
    INSTAGRAM_ACCOUNTS, 
    INSTAGRAM_USERNAME, 
    INSTAGRAM_PASSWORD,
    INSTAGRAM_ACCOUNT_ROTATION,
    PROXY_SERVERS,
    PROXY_COUNTRY,
    DOWNLOAD_DIR, 
    DOWNLOAD_DELAY, 
    MAX_DOWNLOADS_PER_RUN,
    DATA_DIR,
    ACCOUNT_COOLDOWN_MINUTES,
    PROXY_COOLDOWN_MINUTES
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('downloader.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('downloader')

def setup_directories():
    """Create necessary directories if they don't exist"""
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    os.makedirs(os.path.join(DATA_DIR, "state"), exist_ok=True)
    
    # Create account-specific directories
    for account in INSTAGRAM_ACCOUNTS:
        account_dir = os.path.join(DOWNLOAD_DIR, account["username"])
        os.makedirs(account_dir, exist_ok=True)

def get_random_delay():
    """Return a more human-like delay between actions"""
    # Base delay plus random variation to appear more human-like
    return DOWNLOAD_DELAY + random.uniform(-2, 5)

def get_next_account():
    """Get the next available account from rotation"""
    # If no accounts in rotation, use the default credentials
    if not INSTAGRAM_ACCOUNT_ROTATION:
        return INSTAGRAM_USERNAME, INSTAGRAM_PASSWORD
    
    state_file = os.path.join(DATA_DIR, "state", "account_state.json")
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "account_states": {}}
    else:
        state = {"last_index": -1, "account_states": {}}
    
    # Find the next available account
    for i in range(len(INSTAGRAM_ACCOUNT_ROTATION)):
        idx = (state["last_index"] + i + 1) % len(INSTAGRAM_ACCOUNT_ROTATION)
        account = INSTAGRAM_ACCOUNT_ROTATION[idx]
        username = account["username"]
        
        # Skip accounts that are in cooldown
        account_state = state["account_states"].get(username, {})
        next_available_str = account_state.get("next_available")
        
        if not next_available_str or datetime.now().isoformat() >= next_available_str:
            # Update state
            state["last_index"] = idx
            with open(state_file, "w") as f:
                json.dump(state, f)
            
            logger.info(f"Using account {username} from rotation")
            return account["username"], account["password"]
    
    # If all accounts are in cooldown, fallback to default account
    logger.warning("All accounts in rotation are in cooldown, using default account")
    return INSTAGRAM_USERNAME, INSTAGRAM_PASSWORD

def mark_account_cooldown(username, cooldown_minutes=ACCOUNT_COOLDOWN_MINUTES):
    """Mark an account as in cooldown after a failure"""
    if not username:
        return
        
    state_file = os.path.join(DATA_DIR, "state", "account_state.json")
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "account_states": {}}
    else:
        state = {"last_index": -1, "account_states": {}}
    
    # Set cooldown period
    next_available = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
    
    # Update account state
    if username not in state["account_states"]:
        state["account_states"][username] = {}
        
    state["account_states"][username]["next_available"] = next_available
    state["account_states"][username]["last_failure"] = datetime.now().isoformat()
    
    # Save state
    with open(state_file, "w") as f:
        json.dump(state, f)
        
    logger.info(f"Account {username} marked for cooldown until {next_available}")

def get_proxy(country=None):
    """
    Get a proxy from the available pool
    
    Args:
        country: Optional two-letter country code (us, uk, etc.)
    """
    # If no proxies configured, return None
    if not PROXY_SERVERS:
        return None
    
    # Use country from config if not specified in function call
    if country is None and PROXY_COUNTRY:
        country = PROXY_COUNTRY
        
    state_file = os.path.join(DATA_DIR, "state", "proxy_state.json")
    
    # Make sure the directory exists
    os.makedirs(os.path.dirname(state_file), exist_ok=True)
    
    # Load proxy state
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "proxy_states": {}}
    else:
        state = {"last_index": -1, "proxy_states": {}}
    
    # Find the next available proxy
    for i in range(len(PROXY_SERVERS)):
        idx = (state["last_index"] + i + 1) % len(PROXY_SERVERS)
        base_proxy = PROXY_SERVERS[idx]
        
        # If country is specified, modify the proxy URL
        proxy = base_proxy
        if country and "zone-residential" in base_proxy:
            # Extract components from the proxy URL
            parts = base_proxy.split('@')
            if len(parts) == 2:
                auth_part = parts[0]
                host_part = parts[1]
                
                # Check if country parameter is already in the auth part
                if "country-" in auth_part:
                    # Replace existing country
                    auth_parts = auth_part.split('-country-')
                    if len(auth_parts) == 2:
                        country_and_after = auth_parts[1].split(':', 1)
                        if len(country_and_after) == 2:
                            new_auth = f"{auth_parts[0]}-country-{country}:{country_and_after[1]}"
                            proxy = f"{new_auth}@{host_part}"
                else:
                    # Add country before the password
                    auth_parts = auth_part.split(':')
                    if len(auth_parts) >= 2:
                        password_idx = len(auth_parts) - 1
                        auth_parts[password_idx] = f"country-{country}:{auth_parts[password_idx].split(':')[-1]}"
                        proxy = f"{':'.join(auth_parts)}@{host_part}"
        
        # Skip proxies that are in cooldown
        proxy_state = state["proxy_states"].get(proxy, {})
        next_available_str = proxy_state.get("next_available")
        
        if not next_available_str or datetime.now().isoformat() >= next_available_str:
            # Update state
            state["last_index"] = idx
            with open(state_file, "w") as f:
                json.dump(state, f)
            
            # Test the proxy before returning
            if test_proxy(proxy):
                logger.info(f"Using proxy: {proxy}")
                return proxy
            else:
                # Mark as in cooldown if test fails
                mark_proxy_cooldown(proxy, cooldown_minutes=30)
                logger.warning(f"Proxy test failed, marking for cooldown: {proxy}")
                continue
    
    # If all proxies are in cooldown, log warning and return None
    logger.warning("All proxies are in cooldown or not working, proceeding without proxy")
    return None

def test_proxy(proxy_url, test_url="https://www.instagram.com/favicon.ico", timeout=30):
    """Test if a proxy server is working correctly"""
    try:
        # Extract proxy username and password from URL
        parsed_url = urlparse(proxy_url)
        username = parsed_url.username or ""
        password = parsed_url.password or ""
        
        # Create proxy dictionary in the format required by requests
        scheme = parsed_url.scheme
        netloc = parsed_url.netloc
        if '@' in netloc:
            netloc = netloc.split('@')[1]  # Remove credentials from netloc
        
        proxies = {
            "http": f"{scheme}://{username}:{password}@{netloc}",
            "https": f"{scheme}://{username}:{password}@{netloc}"
        }
        
        # Test the proxy by making a request to the test URL
        response = requests.get(test_url, proxies=proxies, timeout=timeout, verify=False)
        
        if response.status_code == 200:
            logger.info(f"Proxy test successful: {netloc}")
            return True
        else:
            logger.warning(f"Proxy test failed with status code {response.status_code}: {netloc}")
            return False
            
    except Exception as e:
        logger.error(f"Error testing proxy: {str(e)}")
        return False

def mark_proxy_cooldown(proxy, cooldown_minutes=PROXY_COOLDOWN_MINUTES):
    """Mark a proxy as in cooldown after a failure"""
    if not proxy:
        return
        
    state_file = os.path.join(DATA_DIR, "state", "proxy_state.json")
    if os.path.exists(state_file):
        try:
            with open(state_file) as f:
                state = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            state = {"last_index": -1, "proxy_states": {}}
    else:
        state = {"last_index": -1, "proxy_states": {}}
    
    # Set cooldown period
    next_available = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
    
    # Update proxy state
    if proxy not in state["proxy_states"]:
        state["proxy_states"][proxy] = {}
        
    state["proxy_states"][proxy]["next_available"] = next_available
    state["proxy_states"][proxy]["last_failure"] = datetime.now().isoformat()
    
    # Save state
    with open(state_file, "w") as f:
        json.dump(state, f)
        
    logger.info(f"Proxy {proxy} marked for cooldown until {next_available}")

def login_with_session(L, username, password):
    """Login with proper session management and error handling"""
    session_file = os.path.join(DATA_DIR, "state", f"insta_session_{username}.txt")
    
    # Try to load existing session first
    if os.path.exists(session_file) and username:
        try:
            L.load_session_from_file(username, session_file)
            logger.info(f"Loaded existing session for {username}")
            
            # Test the session validity by trying a simple operation
            try:
                test_profile = instaloader.Profile.from_username(L.context, username)
                logger.info("Session validation successful")
                return True
            except Exception:
                logger.warning("Loaded session is invalid, will login again")
        except Exception as e:
            logger.warning(f"Could not load session: {str(e)}")
    
    # If we need to login again
    if not username or not password:
        logger.warning("No login credentials provided")
        return False
        
    try:
        # Add random delay before login to look more human-like
        delay = random.uniform(1, 3)
        logger.info(f"Waiting {delay:.1f}s before login attempt...")
        time.sleep(delay)
        
        L.login(username, password)
        
        # Save the session for future use
        L.save_session_to_file(session_file)
        logger.info(f"Login successful and session saved for {username}")
        return True
    except Exception as e:
        logger.error(f"Login failed for {username}: {str(e)}")
        # Mark account for cooldown after failure
        mark_account_cooldown(username)
        return False

def create_instaloader_instance(use_login=True, account=None, proxy=None):
    """Create an Instaloader instance with appropriate settings for our use case"""
    
    # Get a random user agent to appear more human-like
    user_agent = get_random_user_agent()
    
    # Create an Instaloader instance with our required settings
    loader = instaloader.Instaloader(
        download_videos=True,
        download_video_thumbnails=False,
        download_geotags=False,
        download_comments=False,
        save_metadata=True,
        compress_json=False,
        user_agent=user_agent,
        max_connection_attempts=3,
        sleep=True,  # Respect Instagram's rate limits
    )
    
    logger.info(f"Initialized Instaloader with user agent: {user_agent[:30]}...")
    
    # If a proxy is provided, set it on the session
    if proxy:
        try:
            # Extract proxy username and password from URL
            parsed_url = urlparse(proxy)
            username = parsed_url.username or ""
            password = parsed_url.password or ""
            
            # Create proxy string in the format required by requests
            scheme = parsed_url.scheme
            netloc = parsed_url.netloc
            if '@' in netloc:
                netloc = netloc.split('@')[1]  # Remove credentials from netloc
            
            proxy_str = f"{scheme}://{username}:{password}@{netloc}"
            
            # Set the proxy on the session
            loader.context._session.proxies = {
                "http": proxy_str,
                "https": proxy_str
            }
            logger.info(f"Set proxy on Instaloader session: {netloc}")
        except Exception as e:
            logger.error(f"Error setting proxy on Instaloader session: {str(e)}")
    
    # Log in if requested and credentials are available
    if use_login:
        if account is None:
            # Use default credentials if no specific account is provided
            username = INSTAGRAM_USERNAME
            password = INSTAGRAM_PASSWORD
        else:
            # Use the provided account credentials
            username = account.get('username')
            password = account.get('password')
        
        if username and password:
            try:
                # Add a random delay before login to avoid detection
                delay = random.uniform(1, 3)
                logger.debug(f"Adding random delay of {delay:.2f}s before login attempt")
                time.sleep(delay)
                
                loader.login(username, password)
                logger.info(f"Logged in as {username}")
            except Exception as e:
                logger.error(f"Login failed for {username}: {str(e)}")
    
    return loader

def retry_with_backoff(func, max_retries=3, initial_delay=5):
    """Execute a function with exponential backoff retries"""
    retries = 0
    while retries <= max_retries:
        try:
            return func()
        except Exception as e:  # Use generic Exception instead of specific ones
            retries += 1
            if retries > max_retries:
                raise
            wait_time = initial_delay * (2 ** retries) + random.uniform(1, 5)
            logger.warning(f"Retry {retries}/{max_retries} after error: {str(e)}. Waiting {wait_time:.1f}s")
            time.sleep(wait_time)
    return None

def download_from_instagram(accounts=None):
    """
    Download content from Instagram accounts with proper rate limiting
    and proxy rotation
    """
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    downloaded_count = 0
    success_count = 0
    
    # Use the accounts from config if none are provided
    if accounts is None:
        accounts = INSTAGRAM_ACCOUNTS
    
    # Store failed accounts to retry later
    failed_accounts = []
    
    # Randomize the account order to distribute load
    accounts_to_process = list(accounts)
    random.shuffle(accounts_to_process)
    
    # Process each target account
    for account_idx, account_info in enumerate(accounts_to_process):
        # Extract the account name depending on the type
        if isinstance(account_info, dict):
            account_name = account_info.get("username")
        else:
            account_name = account_info
            
        if not account_name:
            logger.warning(f"Skipping invalid account info: {account_info}")
            continue
            
        # Check if we've reached the download limit
        if downloaded_count >= MAX_DOWNLOADS_PER_RUN:
            logger.info(f"Reached maximum download limit of {MAX_DOWNLOADS_PER_RUN}")
            break
            
        # Skip accounts that are not due for refresh
        if not is_account_due_for_refresh(account_name):
            logger.info(f"Skipping account {account_name} - not due for refresh")
            continue
            
        # Get a proxy if available
        proxy = get_proxy()
        if not proxy:
            logger.warning("No proxy available, proceeding without proxy")
        else:
            logger.info(f"Using proxy: {proxy}")
        
        # Log the attempt
        logger.info(f"Processing account {account_idx+1}/{len(accounts_to_process)}: {account_name}")
        
        # Add a delay before processing to avoid detection
        delay = random.uniform(2, 5)
        logger.debug(f"Adding random delay of {delay:.2f}s before processing account")
        time.sleep(delay)
        
        # Try up to 2 different methods to download content
        profile = None
        posts = None
        attempt_count = 0
        max_attempts = 2
        
        while attempt_count < max_attempts and posts is None:
            attempt_count += 1
            logger.info(f"Attempt {attempt_count}/{max_attempts} for account {account_name}")
            
            try:
                # Create Instaloader instance
                use_login = (attempt_count > 1)  # Try without login first, then with login if available
                L = create_instaloader_instance(use_login=use_login, proxy=proxy)
                
                # Try to get profile
                if profile is None:
                    profile = instaloader.Profile.from_username(L.context, account_name)
                    
                    # Check if the profile has posts
                    if not hasattr(profile, 'get_posts'):
                        logger.error(f"Profile {account_name} does not have get_posts method")
                        raise ValueError(f"Invalid profile structure for {account_name}")
                    
                    # Mark this account as processed now (whether it succeeds or fails)
                    mark_account_processed(account_name)
                    
                    # Handle private account
                    if profile.is_private and not use_login:
                        logger.warning(f"Account {account_name} is private - will retry with login if credentials available")
                        continue  # Skip to next attempt (which will use login)
                    
                    logger.info(f"Successfully retrieved profile for {account_name}")
                
                # Get the iterator for posts with proper error handling
                try:
                    # Add a random delay before fetching posts to appear more human-like
                    time.sleep(random.uniform(1, 3))
                    
                    # Get posts iterator
                    posts = profile.get_posts()
                    
                    # Try to access the first post to validate the iterator
                    next(posts)
                    # Reset the iterator
                    posts = profile.get_posts()
                    
                except StopIteration:
                    logger.warning(f"No posts found for account {account_name}")
                    posts = []  # Empty list to indicate success but no posts
                    
                except Exception as e:
                    error_msg = str(e).lower()
                    logger.error(f"Error getting posts for {account_name} (attempt {attempt_count}): {str(e)}")
                    
                    # Check for specific errors that indicate we should try alternative approaches
                    if "401" in error_msg or "unauthorized" in error_msg:
                        if attempt_count < max_attempts:
                            logger.info(f"401 Unauthorized error - will retry with different approach")
                            # Mark previous proxy for cooldown and get a new one for next attempt
                            if proxy:
                                mark_proxy_cooldown(proxy)
                                proxy = get_proxy()
                            
                            posts = None  # Reset so we try again
                            time.sleep(5)  # Wait before retrying
                            continue
                    
                    # If we get here, we couldn't get posts after all retries
                    failed_accounts.append(account_name)
                    break
                
                # Process posts if we have them
                if posts is not None:
                    process_posts(L, profile, account_name, posts, 
                                 downloaded_count, success_count)
                    
            except Exception as e:
                logger.error(f"Error processing account {account_name} (attempt {attempt_count}): {str(e)}")
                
                if attempt_count < max_attempts:
                    # Mark previous proxy for cooldown and get a new one for next attempt
                    if proxy:
                        mark_proxy_cooldown(proxy)
                        proxy = get_proxy()
                    time.sleep(5)  # Wait before retrying
                else:
                    failed_accounts.append(account_name)
    
    # Log summary
    logger.info(f"Download session completed. Downloaded {success_count} videos from {len(accounts_to_process) - len(failed_accounts)}/{len(accounts_to_process)} accounts")
    
    if failed_accounts:
        logger.warning(f"Failed to process {len(failed_accounts)} accounts: {', '.join(str(a) for a in failed_accounts)}")
    
    return success_count, downloaded_count, failed_accounts

def process_posts(L, profile, account_name, posts, downloaded_count, success_count):
    """Process posts for an account"""
    # Process posts
    posts_processed = 0
    
    # Create account directory
    account_dir = os.path.join(DOWNLOAD_DIR, account_name)
    os.makedirs(account_dir, exist_ok=True)
    
    # Create a directory for metadata
    metadata_dir = os.path.join(account_dir, 'metadata')
    os.makedirs(metadata_dir, exist_ok=True)
    
    try:
        for post in posts:
            try:
                # Check if we've reached the maximum downloads limit
                if downloaded_count >= MAX_DOWNLOADS_PER_RUN:
                    logger.info(f"Reached maximum download limit of {MAX_DOWNLOADS_PER_RUN}")
                    break
                
                # Skip if it's not a video
                if not post.is_video:
                    logger.debug(f"Skipping non-video post from {account_name}: {post.shortcode}")
                    continue
                
                # Check if this video has already been downloaded
                video_filename = f"{post.date_utc.strftime('%Y-%m-%d_%H-%M-%S')}_{post.shortcode}.mp4"
                video_path = os.path.join(account_dir, video_filename)
                
                if os.path.exists(video_path):
                    logger.debug(f"Skipping already downloaded video: {video_filename}")
                    continue
                
                # Download the post
                logger.info(f"Downloading video post from {account_name}: {post.shortcode}")
                
                try:
                    # Download only the video
                    L.download_post(post, target=account_dir)
                    downloaded_count += 1
                    success_count += 1
                    
                    # Save post metadata to a separate JSON file
                    metadata = {
                        'shortcode': post.shortcode,
                        'date_utc': post.date_utc.strftime('%Y-%m-%d %H:%M:%S'),
                        'caption': post.caption if post.caption else '',
                        'likes': post.likes,
                        'comments': post.comments,
                        'url': f"https://www.instagram.com/p/{post.shortcode}/",
                        'account': account_name
                    }
                    
                    metadata_path = os.path.join(metadata_dir, f"{post.shortcode}.json")
                    with open(metadata_path, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, ensure_ascii=False, indent=4)
                    
                    # Respect Instagram's rate limits by adding a delay between downloads
                    time.sleep(DOWNLOAD_DELAY)
                    posts_processed += 1
                    
                except Exception as e:
                    logger.error(f"Error downloading post {post.shortcode} from {account_name}: {str(e)}")
                    # Continue with the next post despite this error
                    continue
            
            except Exception as post_error:
                logger.error(f"Error processing post from {account_name}: {str(post_error)}")
                # Continue with the next post
                continue
        
        logger.info(f"Processed {posts_processed} posts from {account_name}, downloaded {success_count} videos")
        return True, posts_processed
        
    except Exception as e:
        logger.error(f"Error processing posts for {account_name}: {str(e)}")
        return False, 0

def get_posts_alternative(profile, username):
    """Alternative approach to fetch posts when the standard iterator fails"""
    try:
        # Try direct URL construction approach
        base_url = f"https://www.instagram.com/{username}/"
        logger.info(f"Attempting alternative post fetching from {base_url}")
        
        # Return posts that we already have in the directory
        metadata_dir = os.path.join(DOWNLOAD_DIR, username, "metadata")
        existing_posts = []
        
        if os.path.exists(metadata_dir):
            for file in os.listdir(metadata_dir):
                if file.endswith('.json'):
                    try:
                        with open(os.path.join(metadata_dir, file), 'r') as f:
                            metadata = json.load(f)
                            if 'shortcode' in metadata:
                                existing_posts.append(metadata['shortcode'])
                    except Exception as e:
                        logger.error(f"Error reading metadata file {file}: {str(e)}")
        
        logger.info(f"Found {len(existing_posts)} existing posts metadata to process")
        return existing_posts
        
    except Exception as e:
        logger.error(f"Alternative post fetching failed: {str(e)}")
        return []

def get_random_user_agent():
    """Return a random user agent to appear more human-like"""
    user_agents = [
        # Desktop browsers
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
        # Mobile browsers
        "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
        "Mozilla/5.0 (Linux; Android 13; SM-S908B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.43 Mobile Safari/537.36"
    ]
    return random.choice(user_agents)

def schedule_refresh(username, backoff_minutes=60):
    """Schedule a refresh attempt with exponential backoff"""
    state_file = os.path.join(DATA_DIR, "refresh_state.json")
    
    # Load or initialize state
    if os.path.exists(state_file):
        try:
            with open(state_file, 'r') as f:
                state = json.load(f)
        except json.JSONDecodeError:
            state = {"accounts": {}}
    else:
        state = {"accounts": {}}
    
    # Get account state or initialize
    account_state = state["accounts"].get(username, {
        "last_attempt": None,
        "backoff_minutes": backoff_minutes,
        "consecutive_failures": 0
    })
    
    # Update for next attempt
    now = datetime.now().isoformat()
    account_state["last_attempt"] = now
    
    if account_state["consecutive_failures"] > 0:
        # Exponential backoff
        account_state["backoff_minutes"] *= 2
    
    # Schedule next attempt
    next_attempt = (datetime.now() + 
                   timedelta(minutes=account_state["backoff_minutes"]))
    account_state["next_attempt"] = next_attempt.isoformat()
    
    # Save state
    state["accounts"][username] = account_state
    with open(state_file, 'w') as f:
        json.dump(state, f)
    
    logger.info(f"Scheduled next refresh for {username} at {next_attempt.isoformat()}")
    return next_attempt.isoformat()

def should_refresh_account(username):
    """Check if an account is due for refresh based on backoff schedule"""
    state_file = os.path.join(DATA_DIR, "refresh_state.json")
    
    # If no state file, always refresh
    if not os.path.exists(state_file):
        return True
        
    try:
        with open(state_file, 'r') as f:
            state = json.load(f)
            
        # If account not in state, always refresh
        if username not in state.get("accounts", {}):
            return True
            
        account_state = state["accounts"][username]
        next_attempt_str = account_state.get("next_attempt")
        
        # If no next attempt scheduled, always refresh
        if not next_attempt_str:
            return True
            
        # Parse next attempt time
        next_attempt = datetime.fromisoformat(next_attempt_str)
        
        # Check if we're past the scheduled time
        return datetime.now() >= next_attempt
        
    except Exception as e:
        logger.error(f"Error checking refresh schedule: {str(e)}")
        return True  # Default to allowing refresh on error

def is_account_due_for_refresh(account_name):
    """Check if an account is due for refresh based on its last processing time"""
    # Path to the account state file
    account_state_file = os.path.join(DATA_DIR, "logs", "account_states.json")
    
    # Default: account is due for refresh
    if not os.path.exists(account_state_file):
        os.makedirs(os.path.dirname(account_state_file), exist_ok=True)
        return True
    
    try:
        # Load account states
        with open(account_state_file, 'r') as f:
            account_states = json.load(f)
        
        # Check if account exists in states
        if account_name not in account_states:
            return True
        
        account_state = account_states[account_name]
        last_processed = account_state.get('last_processed')
        
        # If no last processed time, account is due for refresh
        if not last_processed:
            return True
        
        # Check if account is in cooldown
        cooldown_until = account_state.get('cooldown_until')
        if cooldown_until:
            cooldown_time = datetime.fromisoformat(cooldown_until)
            if datetime.now() < cooldown_time:
                logger.info(f"Account {account_name} is in cooldown until {cooldown_until}")
                return False
        
        # Check if enough time has passed since last processing
        last_processed_time = datetime.fromisoformat(last_processed)
        refresh_interval = account_state.get('refresh_interval', 24)  # Default: 24 hours
        
        next_refresh_time = last_processed_time + timedelta(hours=refresh_interval)
        
        if datetime.now() < next_refresh_time:
            logger.info(f"Account {account_name} not due for refresh until {next_refresh_time.isoformat()}")
            return False
            
        return True
        
    except Exception as e:
        logger.error(f"Error checking refresh status for {account_name}: {str(e)}")
        # Default to allowing refresh on error
        return True

def mark_account_processed(account_name, success=True):
    """Mark an account as processed and update its refresh schedule"""
    # Path to the account state file
    account_state_file = os.path.join(DATA_DIR, "logs", "account_states.json")
    
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(account_state_file), exist_ok=True)
        
        # Load existing account states or create new dict
        if os.path.exists(account_state_file):
            with open(account_state_file, 'r') as f:
                account_states = json.load(f)
        else:
            account_states = {}
        
        # Get or create account state
        if account_name not in account_states:
            account_states[account_name] = {}
        
        # Update account state
        account_states[account_name]['last_processed'] = datetime.now().isoformat()
        
        # Update success/failure count
        if success:
            account_states[account_name]['consecutive_failures'] = 0
            # Reset refresh interval to default on success
            account_states[account_name]['refresh_interval'] = 24  # Default: 24 hours
        else:
            # Increment failure count
            failures = account_states[account_name].get('consecutive_failures', 0) + 1
            account_states[account_name]['consecutive_failures'] = failures
            
            # Implement exponential backoff for failures
            backoff_hours = min(24 * (2 ** (failures - 1)), 168)  # Max 1 week
            account_states[account_name]['refresh_interval'] = backoff_hours
            
            # Set cooldown period for repeated failures
            if failures > 2:
                cooldown_minutes = ACCOUNT_COOLDOWN_MINUTES * (2 ** (failures - 3))  # Exponential cooldown
                cooldown_until = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
                account_states[account_name]['cooldown_until'] = cooldown_until
                logger.warning(f"Account {account_name} in cooldown until {cooldown_until} after {failures} failures")
        
        # Save updated account states
        with open(account_state_file, 'w') as f:
            json.dump(account_states, f, indent=4)
            
    except Exception as e:
        logger.error(f"Error marking account {account_name} as processed: {str(e)}")

def mark_account_cooldown(username, cooldown_minutes=ACCOUNT_COOLDOWN_MINUTES):
    """Mark an account for cooldown after a failure"""
    # Path to the account state file
    account_state_file = os.path.join(DATA_DIR, "logs", "account_states.json")
    
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(account_state_file), exist_ok=True)
        
        # Load existing account states or create new dict
        if os.path.exists(account_state_file):
            with open(account_state_file, 'r') as f:
                account_states = json.load(f)
        else:
            account_states = {}
        
        # Get or create account state
        if username not in account_states:
            account_states[username] = {}
        
        # Set cooldown period
        cooldown_until = (datetime.now() + timedelta(minutes=cooldown_minutes)).isoformat()
        account_states[username]['cooldown_until'] = cooldown_until
        
        # Increment failure count
        failures = account_states[username].get('consecutive_failures', 0) + 1
        account_states[username]['consecutive_failures'] = failures
        
        logger.warning(f"Account {username} in cooldown until {cooldown_until}")
        
        # Save updated account states
        with open(account_state_file, 'w') as f:
            json.dump(account_states, f, indent=4)
            
    except Exception as e:
        logger.error(f"Error marking account {username} for cooldown: {str(e)}")

if __name__ == "__main__":
    download_from_instagram() 

================================================================================
File: indexer.py
================================================================================

"""
Module for indexing transcribed content into the knowledge base
"""
import os
import json
import glob
import sqlite3
import logging
from tqdm import tqdm

from config import (
    DB_PATH,
    TRANSCRIPT_DIR
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('indexer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('indexer')

def setup_database():
    """Ensure the database is set up correctly"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # Use the updated schema with summary field and FTS4
    cursor.executescript('''
    -- Content table stores all video data
    CREATE TABLE IF NOT EXISTS videos (
        id INTEGER PRIMARY KEY,
        shortcode TEXT UNIQUE,
        account TEXT,
        filename TEXT,
        caption TEXT,
        transcript TEXT,
        summary TEXT,
        timestamp TEXT,
        download_date TEXT,
        url TEXT,
        likes INTEGER,
        comments INTEGER,
        word_count INTEGER,
        duration_seconds INTEGER,
        key_phrases TEXT
    );
    
    -- Tags table for improved filtering
    CREATE TABLE IF NOT EXISTS tags (
        id INTEGER PRIMARY KEY,
        video_id INTEGER,
        tag TEXT,
        FOREIGN KEY (video_id) REFERENCES videos(id)
    );
    
    -- Improved indexing for better performance
    CREATE INDEX IF NOT EXISTS idx_videos_account ON videos(account);
    CREATE INDEX IF NOT EXISTS idx_videos_timestamp ON videos(timestamp);
    CREATE INDEX IF NOT EXISTS idx_tags_tag ON tags(tag);
    
    -- Virtual FTS4 table for full-text search
    CREATE VIRTUAL TABLE IF NOT EXISTS videos_fts USING fts4(
        shortcode,
        account,
        caption,
        transcript,
        summary,
        timestamp,
        content=videos,
        tokenize=porter
    );
    
    -- Create triggers to keep FTS table synchronized
    CREATE TRIGGER IF NOT EXISTS videos_ai AFTER INSERT ON videos BEGIN
        INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
        VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
    END;
    
    CREATE TRIGGER IF NOT EXISTS videos_ad AFTER DELETE ON videos BEGIN
        DELETE FROM videos_fts WHERE docid = old.id;
    END;
    
    CREATE TRIGGER IF NOT EXISTS videos_au AFTER UPDATE ON videos BEGIN
        DELETE FROM videos_fts WHERE docid = old.id;
        INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
        VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
    END;
    ''')
    
    conn.commit()
    conn.close()
    
    logger.info("Database setup complete")

def extract_tags_from_caption(caption):
    """Extract hashtags from captions"""
    if not caption:
        return []
    
    words = caption.split()
    tags = [word[1:] for word in words if word.startswith('#')]
    return tags

def calculate_word_count(text):
    """Calculate word count from text"""
    if not text:
        return 0
    return len(text.split())

def index_transcripts():
    """Index all transcripts into the database"""
    setup_database()
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # Get all transcript files
    all_transcripts = []
    for root, _, _ in os.walk(TRANSCRIPT_DIR):
        transcripts = glob.glob(os.path.join(root, "*.json"))
        all_transcripts.extend(transcripts)
    
    logger.info(f"Found {len(all_transcripts)} transcripts to index")
    
    # Process each transcript
    new_count = 0
    updated_count = 0
    
    for transcript_path in tqdm(all_transcripts, desc="Indexing transcripts"):
        try:
            with open(transcript_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Extract relevant fields, using empty strings for missing data
            shortcode = data.get("shortcode", "")
            account = data.get("account", "")
            filename = data.get("filename", "")
            caption = data.get("caption", "")
            transcript_text = data.get("text", "")
            timestamp = data.get("timestamp", "")
            download_date = data.get("download_date", "")
            url = data.get("url", "")
            likes = data.get("likes", 0)
            comments = data.get("comments", 0)
            
            # Calculate word count
            word_count = calculate_word_count(transcript_text)
            
            # Duration (if available)
            duration_seconds = data.get("duration_seconds", None)
            
            # Initially no summary or key phrases
            summary = ""
            key_phrases = ""
            
            # Check if this video is already in the database
            cursor.execute("SELECT id, summary, word_count FROM videos WHERE shortcode = ?", (shortcode,))
            result = cursor.fetchone()
            
            if result:
                # Update existing record (preserve summary if it exists)
                video_id = result[0]
                existing_summary = result[1] or ""
                existing_word_count = result[2] or 0
                
                cursor.execute('''
                UPDATE videos SET
                    account = ?,
                    filename = ?,
                    caption = ?,
                    transcript = ?,
                    timestamp = ?,
                    download_date = ?,
                    url = ?,
                    likes = ?,
                    comments = ?,
                    word_count = ?,
                    duration_seconds = ?,
                    key_phrases = ?
                WHERE id = ?
                ''', (account, filename, caption, transcript_text, timestamp, 
                      download_date, url, likes, comments, word_count,
                      duration_seconds, key_phrases, video_id))
                updated_count += 1
                
                # Remove old tags
                cursor.execute("DELETE FROM tags WHERE video_id = ?", (video_id,))
                
            else:
                # Insert new record
                cursor.execute('''
                INSERT INTO videos (
                    shortcode, account, filename, caption, transcript, summary,
                    timestamp, download_date, url, likes, comments, 
                    word_count, duration_seconds, key_phrases
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (shortcode, account, filename, caption, transcript_text, summary,
                      timestamp, download_date, url, likes, comments, 
                      word_count, duration_seconds, key_phrases))
                video_id = cursor.lastrowid
                new_count += 1
            
            # Extract and save tags
            tags = extract_tags_from_caption(caption)
            for tag in tags:
                cursor.execute('''
                INSERT INTO tags (video_id, tag) VALUES (?, ?)
                ''', (video_id, tag))
            
            # Commit every 100 records
            if (new_count + updated_count) % 100 == 0:
                conn.commit()
                
        except Exception as e:
            logger.error(f"Error indexing {transcript_path}: {str(e)}")
    
    # Final commit
    conn.commit()
    conn.close()
    
    logger.info(f"Indexing complete. Added {new_count} new records, updated {updated_count} existing records.")

if __name__ == "__main__":
    index_transcripts() 

================================================================================
File: init_db.py
================================================================================

"""
Initialize the SQLite database with proper schema
"""
import os
import sqlite3
import logging

from config import DB_PATH, DATA_DIR

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('init_db.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('init_db')

def init_database():
    """Initialize the SQLite database with proper schema"""
    # Ensure data directory exists
    os.makedirs(DATA_DIR, exist_ok=True)
    
    logger.info(f"Initializing database at {DB_PATH}")
    
    # Create or connect to database
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # Read SQL schema from file
    schema_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "create_db.sql")
    
    try:
        with open(schema_path, 'r') as f:
            schema_sql = f.read()
        
        # Execute schema script
        cursor.executescript(schema_sql)
        logger.info("Successfully executed schema script")
        
    except FileNotFoundError:
        logger.warning(f"Schema file not found at {schema_path}. Using embedded schema.")
        
        # Execute embedded schema if file not found
        cursor.executescript('''
        -- Content table stores all video data
        CREATE TABLE IF NOT EXISTS videos (
            id INTEGER PRIMARY KEY,
            shortcode TEXT UNIQUE,
            account TEXT,
            filename TEXT,
            caption TEXT,
            transcript TEXT,
            summary TEXT,
            timestamp TEXT,
            download_date TEXT,
            url TEXT,
            likes INTEGER,
            comments INTEGER,
            word_count INTEGER,
            duration_seconds INTEGER,
            key_phrases TEXT
        );

        -- Tags table for improved filtering
        CREATE TABLE IF NOT EXISTS tags (
            id INTEGER PRIMARY KEY,
            video_id INTEGER,
            tag TEXT,
            FOREIGN KEY (video_id) REFERENCES videos(id)
        );

        -- Improved indexing for better performance
        CREATE INDEX IF NOT EXISTS idx_videos_account ON videos(account);
        CREATE INDEX IF NOT EXISTS idx_videos_timestamp ON videos(timestamp);
        CREATE INDEX IF NOT EXISTS idx_tags_tag ON tags(tag);

        -- Virtual FTS4 table for full-text search (using FTS4 instead of FTS5 for better compatibility)
        CREATE VIRTUAL TABLE IF NOT EXISTS videos_fts USING fts4(
            shortcode,
            account,
            caption,
            transcript,
            summary,
            timestamp,
            content=videos,
            tokenize=porter
        );

        -- Create triggers to keep FTS table synchronized
        CREATE TRIGGER IF NOT EXISTS videos_ai AFTER INSERT ON videos BEGIN
            INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
            VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
        END;

        CREATE TRIGGER IF NOT EXISTS videos_ad AFTER DELETE ON videos BEGIN
            DELETE FROM videos_fts WHERE docid = old.id;
        END;

        CREATE TRIGGER IF NOT EXISTS videos_au AFTER UPDATE ON videos BEGIN
            DELETE FROM videos_fts WHERE docid = old.id;
            INSERT INTO videos_fts(docid, shortcode, account, caption, transcript, summary, timestamp)
            VALUES (new.id, new.shortcode, new.account, new.caption, new.transcript, new.summary, new.timestamp);
        END;
        ''')
        logger.info("Successfully executed embedded schema")
    
    except Exception as e:
        logger.error(f"Error initializing database: {str(e)}")
        raise
    
    # Commit changes and close connection
    conn.commit()
    conn.close()
    
    logger.info(f"Database initialized at {DB_PATH}")
    logger.info("You can now run the pipeline to populate the database with your video data.")

if __name__ == "__main__":
    init_database() 

================================================================================
File: README.md
================================================================================

# Instagram Knowledge Base

A comprehensive solution for building a personal knowledge base from Instagram videos. This system downloads videos from Instagram accounts, transcribes them using OpenAI's Whisper, summarizes them using Claude, and makes them searchable through a web interface.

## Features

- **Multi-account support**: Configure multiple Instagram accounts in the config file
- **Metadata extraction**: Stores engagement metrics, hashtags, captions, and timestamps
- **AI-powered summarization**: Generate concise summaries of video transcripts using Claude
- **Full-text search**: Find specific content across all transcripts and summaries
- **Prioritized search results**: Results from summaries and captions are prioritized over full transcripts
- **Filter capabilities**: Filter by account, hashtag, or date
- **Visualization**: Statistics dashboard to explore your content collection
- **Rate limiting**: Respectful downloading with exponential backoff to avoid IP blocks
- **Modular design**: Run individual components as needed
- **Performance optimization**: Caching for frequently accessed data
- **Resilient downloads**: Robust retry mechanism with exponential backoff

## Requirements

- Python 3.8+
- FFmpeg (for audio extraction)
- Instagram account (for accessing content)
- Anthropic API key (for Claude summarization)

## Installation

1. Clone this repository
2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Install FFmpeg if not already installed (system dependent):
   - Ubuntu/Debian: `sudo apt-get install ffmpeg`
   - macOS: `brew install ffmpeg`
   - Windows: Download from [ffmpeg.org](https://ffmpeg.org/download.html)

## Configuration

1. Edit `config.py` to add Instagram accounts you have permission to access
2. Set environment variables:
   ```bash
   # For Instagram authentication (strongly recommended)
   export INSTAGRAM_USERNAME="your_username"
   export INSTAGRAM_PASSWORD="your_password"
   
   # For Claude summarization
   export ANTHROPIC_API_KEY="your_anthropic_api_key"
   ```
   - For Windows, use `set` instead of `export`
   - For production, consider using a `.env` file with python-dotenv

## Usage

### Running the Complete Pipeline

Run all steps (download, transcribe, summarize, index, web):

```bash
python run.py --all
```

### Running Individual Components

Download videos:
```bash
python run.py --download
```

Transcribe videos:
```bash
python run.py --transcribe
```

Summarize transcripts using Claude:
```bash
python run.py --summarize
```

Index transcripts and summaries:
```bash
python run.py --index
```

Start the web interface:
```bash
python run.py --web
```

## Accessing the Web Interface

Once running, access the web interface at:
```
http://localhost:5000
```

## Advanced Features

### Transcript Summarization

The system now includes Claude-powered summarization that:
- Generates concise summaries (150-200 words) for each video transcript
- Highlights key insights and actionable information
- Implements efficient batch processing to respect API rate limits
- Caches summaries to avoid redundant API calls
- Enhances search by prioritizing matches in summaries

### Performance Optimizations

- **LRU Caching**: Frequently accessed data is cached to reduce database load
- **Improved Indexing**: Better database schema with optimized indices for common query patterns
- **Enhanced Search**: Prioritized search results with highlighted matches
- **Resilient Downloads**: Smart retry with exponential backoff for more reliable downloads

### Content Analytics

The system tracks:
- Word count for each transcript
- Video duration (when available)
- Key content metrics accessible through the statistics dashboard

## Ethical Usage

This tool is for personal use only, with content you have permission to access. Always respect copyright and terms of service for the platforms you use.

## License

MIT License 

================================================================================
File: run.py
================================================================================

"""
Main script to run the complete Instagram Knowledge Base system
"""
import os
import argparse
import logging
from time import time

# Import our modules
from config import DATA_DIR
import downloader
import transcriber
import indexer
import summarizer
from app import app

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('instagram_kb.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('main')

def setup():
    """Setup necessary directories"""
    os.makedirs(DATA_DIR, exist_ok=True)

def run_downloader():
    """Run the Instagram downloader"""
    logger.info("Starting Instagram content download")
    start_time = time()
    downloader.download_from_instagram()
    logger.info(f"Download completed in {time() - start_time:.2f} seconds")

def run_transcriber():
    """Run the audio extraction and transcription"""
    logger.info("Starting audio extraction and transcription")
    start_time = time()
    transcriber.process_videos()
    logger.info(f"Transcription completed in {time() - start_time:.2f} seconds")

def run_summarizer():
    """Run the transcript summarization using Claude"""
    logger.info("Starting transcript summarization using Claude")
    start_time = time()
    summarizer.summarize_transcripts()
    logger.info(f"Summarization completed in {time() - start_time:.2f} seconds")

def run_indexer():
    """Run the knowledge base indexer"""
    logger.info("Starting indexing of transcripts")
    start_time = time()
    indexer.index_transcripts()
    logger.info(f"Indexing completed in {time() - start_time:.2f} seconds")

def run_web_interface():
    """Run the web interface"""
    logger.info("Starting web interface")
    app.run(host='0.0.0.0', port=5000)

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='Instagram Knowledge Base')
    parser.add_argument('--download', action='store_true', help='Run the downloader module')
    parser.add_argument('--transcribe', action='store_true', help='Run the transcription module')
    parser.add_argument('--summarize', action='store_true', help='Run the summarization module')
    parser.add_argument('--index', action='store_true', help='Run the indexer module')
    parser.add_argument('--web', action='store_true', help='Run the web interface')
    parser.add_argument('--all', action='store_true', help='Run the complete pipeline')
    
    args = parser.parse_args()
    
    # Setup directories
    setup()
    
    # Run requested modules
    if args.all or args.download:
        run_downloader()
    
    if args.all or args.transcribe:
        run_transcriber()
    
    if args.all or args.summarize:
        run_summarizer()
    
    if args.all or args.index:
        run_indexer()
    
    if args.all or args.web:
        run_web_interface()
    
    # If no arguments provided, show help
    if not (args.download or args.transcribe or args.summarize or args.index or args.web or args.all):
        parser.print_help()

if __name__ == "__main__":
    main() 

================================================================================
File: summarizer.py
================================================================================

"""
Module for summarizing video transcripts using Claude API
"""
import os
import json
import time
import sqlite3
import logging
import re
from anthropic import Anthropic

from config import DB_PATH, TRANSCRIPT_DIR, DATA_DIR

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('summarizer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('summarizer')

class ClaudeSummarizer:
    def __init__(self, api_key=None):
        """Initialize the Claude summarizer with API key"""
        # Update to use the current API initialization pattern (v0.49.0)
        api_key = api_key or os.environ.get("ANTHROPIC_API_KEY", "")
        self.client = Anthropic(api_key=api_key)
        
        self.cache_dir = os.path.join(DATA_DIR, "summaries_cache")
        os.makedirs(self.cache_dir, exist_ok=True)
        self.cache_file = os.path.join(self.cache_dir, "summary_cache.json")
        self._load_cache()
    
    def _load_cache(self):
        """Load summary cache from file"""
        try:
            with open(self.cache_file, 'r') as f:
                self.cache = json.load(f)
                logger.info(f"Loaded {len(self.cache)} cached summaries")
        except (FileNotFoundError, json.JSONDecodeError):
            self.cache = {}
            logger.info("Created new summary cache")
    
    def _save_cache(self):
        """Save summary cache to file"""
        with open(self.cache_file, 'w') as f:
            json.dump(self.cache, f)
            logger.info(f"Saved {len(self.cache)} summaries to cache")
    
    def _create_enhanced_prompt(self, transcript, metadata=None):
        """Create an enhanced prompt for Claude with context about the video"""
        context = ""
        if metadata:
            context = f"Video by: {metadata.get('account', 'Unknown')}\n"
            if metadata.get('timestamp'):
                context += f"Posted: {metadata.get('timestamp', '').split('T')[0]}\n"
            if metadata.get('caption'):
                context += f"Caption: {metadata.get('caption', 'No caption')}\n"
        
        prompt = f"""Analyze this Instagram video transcript and provide structured information in your response:

{context}
TRANSCRIPT:
{transcript}

Please extract the following information:
1. Summary: Concise overview of the main points (80-100 words max)
2. Key Topics: 3-5 main topics covered
3. Entities Mentioned: People, products, services, or concepts mentioned
4. Tone & Style: Formal/informal, educational/conversational, etc.
5. Key Insights: 2-3 main takeaways or insights
6. Actionable Information: Any specific advice, steps, or actionable items
7. Content Type: What kind of content is this (tutorial, conversation, review, educational, etc.)

Provide your analysis in this format:

SUMMARY: [Your concise summary]

KEY TOPICS:
- [Topic 1]
- [Topic 2]
- [Topic 3]

ENTITIES MENTIONED:
- [Entity 1] (person/product/concept)
- [Entity 2] (person/product/concept)

TONE & STYLE: [Analysis of tone and presentation style]

KEY INSIGHTS:
- [Insight 1]
- [Insight 2]

ACTIONABLE INFORMATION:
- [Actionable item 1] 
- [Actionable item 2]

CONTENT TYPE: [content type classification]
"""
        return prompt
    
    def _extract_key_phrases(self, transcript, summary):
        """Extract key phrases and terms from transcript and summary"""
        combined_text = f"{summary} {transcript}"
        # Split into words, lowercase, and remove punctuation
        words = re.findall(r'\b\w+\b', combined_text.lower())
        
        # Count word frequencies (excluding common words)
        common_words = {'the', 'and', 'that', 'for', 'you', 'with', 'this', 'was', 'are', 'have', 
                       'its', 'they', 'from', 'but', 'not', 'what', 'all', 'were', 'when', 'your',
                       'can', 'said', 'there', 'use', 'been', 'has', 'would', 'each', 'which', 'she',
                       'how', 'their', 'will', 'other', 'about', 'out', 'many', 'then', 'them', 'these',
                       'some', 'her', 'him', 'into', 'more', 'could', 'know', 'like', 'just'}
        
        word_freq = {}
        for word in words:
            if word not in common_words and len(word) > 3:  # Exclude common words and short words
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Get top N words by frequency
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # Extract 2-3 word phrases using regex patterns
        phrase_patterns = [
            r'\b\w+\s+\w+\b',           # 2-word phrases
            r'\b\w+\s+\w+\s+\w+\b'      # 3-word phrases
        ]
        
        phrases = []
        for pattern in phrase_patterns:
            phrases.extend(re.findall(pattern, combined_text.lower()))
        
        # Count phrase frequencies and exclude common phrases
        phrase_freq = {}
        for phrase in phrases:
            if not any(w in common_words for w in phrase.split() if len(w) <= 3):
                if len(phrase.split()) > 1:  # Ensure it's a real phrase
                    phrase_freq[phrase] = phrase_freq.get(phrase, 0) + 1
        
        # Get top N phrases by frequency
        top_phrases = sorted(phrase_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # Combine top words and phrases
        key_phrases = [word for word, _ in top_words]
        key_phrases.extend([phrase for phrase, _ in top_phrases])
        
        # Remove duplicates and limit to 10 items
        unique_key_phrases = []
        for phrase in key_phrases:
            if not any(phrase in p for p in unique_key_phrases):
                unique_key_phrases.append(phrase)
                if len(unique_key_phrases) >= 10:
                    break
        
        return unique_key_phrases
    
    def _parse_structured_summary(self, claude_response):
        """Parse Claude's structured response into components"""
        result = {
            'summary': '',
            'key_topics': [],
            'entities': [],
            'tone': '',
            'key_insights': [],
            'actionable_items': [],
            'content_type': ''
        }
        
        # Extract the summary
        summary_match = re.search(r'SUMMARY:\s*(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if summary_match:
            result['summary'] = summary_match.group(1).strip()
        
        # Extract key topics
        topics_section = re.search(r'KEY TOPICS:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if topics_section:
            topics = re.findall(r'-\s*(.*?)(?=\n|$)', topics_section.group(1))
            result['key_topics'] = [topic.strip() for topic in topics if topic.strip()]
        
        # Extract entities
        entities_section = re.search(r'ENTITIES MENTIONED:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if entities_section:
            entities = re.findall(r'-\s*(.*?)(?=\n|$)', entities_section.group(1))
            result['entities'] = [entity.strip() for entity in entities if entity.strip()]
        
        # Extract tone
        tone_match = re.search(r'TONE & STYLE:\s*(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if tone_match:
            result['tone'] = tone_match.group(1).strip()
        
        # Extract insights
        insights_section = re.search(r'KEY INSIGHTS:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if insights_section:
            insights = re.findall(r'-\s*(.*?)(?=\n|$)', insights_section.group(1))
            result['key_insights'] = [insight.strip() for insight in insights if insight.strip()]
        
        # Extract actionable items
        actionable_section = re.search(r'ACTIONABLE INFORMATION:(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if actionable_section:
            actions = re.findall(r'-\s*(.*?)(?=\n|$)', actionable_section.group(1))
            result['actionable_items'] = [action.strip() for action in actions if action.strip()]
        
        # Extract content type
        content_match = re.search(r'CONTENT TYPE:\s*(.*?)(?=\n\n|\Z)', claude_response, re.DOTALL)
        if content_match:
            result['content_type'] = content_match.group(1).strip()
        
        return result
    
    def summarize(self, transcript, shortcode, metadata=None, max_retries=3):
        """Generate a summary using Claude API with retry logic"""
        # Check cache first
        if shortcode in self.cache:
            logger.info(f"Using cached summary for {shortcode}")
            return self.cache[shortcode]
        
        # If transcript is too short, use it as the summary
        if len(transcript.split()) < 30:
            logger.info(f"Transcript too short for {shortcode}, using as summary")
            
            # Even for short transcripts, provide some structure
            structured_response = {
                'summary': transcript,
                'key_topics': self._extract_key_phrases(transcript, transcript)[:3],
                'entities': [],
                'tone': 'Brief',
                'key_insights': [],
                'actionable_items': [],
                'content_type': 'Short clip'
            }
            
            summary_text = f"SUMMARY: {structured_response['summary']}\n\n"
            summary_text += "KEY TOPICS:\n" + "\n".join([f"- {topic}" for topic in structured_response['key_topics']])
            summary_text += "\n\nCONTENT TYPE: Short clip"
            
            self.cache[shortcode] = summary_text
            self._save_cache()
            return summary_text
        
        # Retry logic with exponential backoff
        retries = 0
        while retries <= max_retries:
            try:
                logger.info(f"Generating summary for {shortcode} (Attempt {retries+1}/{max_retries+1})")
                
                # Create enhanced prompt
                prompt = self._create_enhanced_prompt(transcript, metadata)
                
                # Use the current API pattern for Anthropic v0.49.0
                response = self.client.messages.create(
                    model="claude-3-haiku-20240307",  # More cost-effective model for batch processing
                    max_tokens=1024,
                    messages=[
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ]
                )
                
                # Get the text content from the response
                summary = response.content[0].text
                
                # Extract structured information from the response
                structured_data = self._parse_structured_summary(summary)
                
                # Extract key phrases if not already present in the response
                if not structured_data['key_topics'] or len(structured_data['key_topics']) < 3:
                    key_phrases = self._extract_key_phrases(transcript, structured_data['summary'])
                    structured_data['key_topics'] = key_phrases[:5]
                
                # Cache the result (store the full text response)
                self.cache[shortcode] = summary
                self._save_cache()
                
                logger.info(f"Successfully generated summary for {shortcode}")
                return summary
                
            except Exception as e:
                retries += 1
                logger.error(f"Error generating summary (Attempt {retries}/{max_retries+1}): {str(e)}")
                if retries <= max_retries:
                    wait_time = 2 ** retries + (retries * 2)  # Exponential backoff
                    logger.info(f"Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Failed to generate summary after {max_retries+1} attempts")
                    # Provide a basic structured response for failed summaries
                    basic_response = (
                        "SUMMARY: Summary generation failed due to API errors.\n\n"
                        "KEY TOPICS:\n- Unknown\n\n"
                        "CONTENT TYPE: Unknown"
                    )
                    return basic_response
        
        return "Summary generation failed"


def get_video_metadata(shortcode, conn):
    """Fetch metadata for a video from the database"""
    cursor = conn.cursor()
    cursor.execute(
        "SELECT account, timestamp, caption FROM videos WHERE shortcode = ?",
        (shortcode,)
    )
    result = cursor.fetchone()
    if result:
        return {
            'account': result['account'],
            'timestamp': result['timestamp'],
            'caption': result['caption']
        }
    return None


def analyze_content_quality(transcript):
    """Calculate metrics about content quality"""
    if not transcript:
        return {}
        
    # Calculate basic metrics
    words = transcript.split()
    word_count = len(words)
    
    # Estimate dialogue percentage (if there are colons, quotes, etc.)
    dialogue_markers = [":", "?", '"', "'", "says", "said", "asked"]
    dialogue_score = sum(transcript.count(marker) for marker in dialogue_markers) / max(1, len(transcript) / 100)
    
    # Calculate average word length (a proxy for complexity)
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)
    
    # Estimate reading time (average person reads ~200-250 words per minute)
    reading_time_seconds = (word_count / 200) * 60
    
    return {
        "word_count": word_count,
        "dialogue_ratio": min(1.0, dialogue_score),
        "avg_word_length": avg_word_length,
        "estimated_read_time_seconds": reading_time_seconds
    }


def process_transcripts_with_claude(batch_size=10, delay_between_batches=5):
    """Process transcripts in batches using Claude API"""
    try:
        # Check for API key
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            logger.error("ANTHROPIC_API_KEY environment variable not set. Please set it before running the summarizer.")
            return
            
        summarizer = ClaudeSummarizer()
        
        # Get videos that need summarization
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Find videos with transcripts but no summaries
        cursor.execute(
            "SELECT id, shortcode, account, transcript FROM videos WHERE transcript IS NOT NULL AND transcript != '' AND (summary IS NULL OR summary = '')"
        )
        videos = cursor.fetchall()
        
        total_videos = len(videos)
        logger.info(f"Found {total_videos} videos that need summarization")
        
        if total_videos == 0:
            logger.info("No videos to summarize. Exiting.")
            conn.close()
            return
        
        # Process in batches
        for i in range(0, total_videos, batch_size):
            batch = videos[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_videos - 1) // batch_size + 1
            
            logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} videos)")
            
            for video in batch:
                video_id = video['id']
                shortcode = video['shortcode']
                account = video['account']
                transcript = video['transcript']
                
                # Get full metadata for enhanced context
                metadata = get_video_metadata(shortcode, conn)
                
                # Calculate content quality metrics
                quality_metrics = analyze_content_quality(transcript)
                word_count = quality_metrics.get('word_count', 0)
                
                logger.info(f"Summarizing video {shortcode} ({word_count} words)")
                summary = summarizer.summarize(transcript, shortcode, metadata=metadata)
                
                # Extract key phrases for database storage
                structured_data = summarizer._parse_structured_summary(summary)
                key_phrases_json = json.dumps(structured_data.get('key_topics', []))
                
                # Update database with summary, word count and content metrics
                cursor.execute(
                    """UPDATE videos SET 
                       summary = ?, 
                       word_count = ?,
                       key_phrases = ?,
                       duration_seconds = ?
                       WHERE id = ?""",
                    (summary, word_count, key_phrases_json, 
                     quality_metrics.get('estimated_read_time_seconds', 0), video_id)
                )
                conn.commit()
                logger.info(f"Updated summary and metrics for video {shortcode}")
            
            # Respect API rate limits between batches
            processed_count = min(i + batch_size, total_videos)
            logger.info(f"Processed {processed_count}/{total_videos} videos")
            
            if i + batch_size < total_videos:
                logger.info(f"Waiting {delay_between_batches} seconds before next batch")
                time.sleep(delay_between_batches)
        
        conn.close()
        logger.info("Summarization process complete")
        
    except Exception as e:
        logger.error(f"Error in batch processing: {str(e)}")
        raise


def summarize_transcripts():
    """Main entry point for transcript summarization"""
    logger.info("Starting transcript summarization using Claude")
    start_time = time.time()
    process_transcripts_with_claude()
    logger.info(f"Summarization completed in {time.time() - start_time:.2f} seconds")


if __name__ == "__main__":
    summarize_transcripts() 

================================================================================
File: test_db.py
================================================================================

import sqlite3

# Connect to the database
conn = sqlite3.connect('data/knowledge_base.db')
cursor = conn.cursor()

# Get the list of tables
cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
tables = cursor.fetchall()
print("Tables in the database:")
for table in tables:
    print(f"- {table[0]}")

# Count records in videos table
cursor.execute("SELECT COUNT(*) FROM videos;")
count = cursor.fetchone()[0]
print(f"\nNumber of videos in database: {count}")

# Close the connection
conn.close()

print("\nDatabase access test completed successfully!") 

================================================================================
File: test_proxy.py
================================================================================

#!/usr/bin/env python
"""
Test script for Bright Data residential proxy
"""
import sys
import requests
import os
import json
from datetime import datetime
import urllib3

# Disable SSL warnings for testing
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def test_brightdata_proxy():
    """Test Bright Data residential proxy"""
    print("Testing Bright Data residential proxy...")
    
    # Proxy configuration
    proxy = "http://brd-customer-hl_c7bff232-zone-residential_proxy1-country-us:w46vs0z46xmc@brd.superproxy.io:33335"
    
    # Setup proxies for requests
    proxies = {
        "http": proxy,
        "https": proxy
    }
    
    try:
        # First, test with the Bright Data test endpoint
        response = requests.get(
            "https://geo.brdtest.com/welcome.txt?product=resi&method=native",
            proxies=proxies,
            timeout=10,
            verify=False  # Disable SSL verification
        )
        
        if response.status_code == 200:
            print("✅ Bright Data test successful!")
            print(f"Response: {response.text.strip()}")
        else:
            print(f"❌ Bright Data test failed with status code: {response.status_code}")
            print(f"Response: {response.text}")
        
        # Then test with Instagram
        print("\nTesting access to Instagram...")
        instagram_response = requests.get(
            "https://www.instagram.com/favicon.ico",
            proxies=proxies,
            timeout=10,
            verify=False,  # Disable SSL verification
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
        )
        
        if instagram_response.status_code == 200:
            print("✅ Instagram test successful!")
            print(f"Status code: {instagram_response.status_code}")
            print(f"Content length: {len(instagram_response.content)} bytes")
        else:
            print(f"❌ Instagram test failed with status code: {instagram_response.status_code}")
            
        # Save test results
        results = {
            "timestamp": datetime.now().isoformat(),
            "brightdata_test": {
                "status_code": response.status_code,
                "success": response.status_code == 200,
                "response_text": response.text.strip()
            },
            "instagram_test": {
                "status_code": instagram_response.status_code,
                "success": instagram_response.status_code == 200,
                "content_length": len(instagram_response.content)
            }
        }
        
        # Create data directory if it doesn't exist
        os.makedirs(os.path.join("data", "logs"), exist_ok=True)
        
        # Save results to a file
        with open(os.path.join("data", "logs", "proxy_test_results.json"), "w") as f:
            json.dump(results, f, indent=2)
            
        print("\nTest results saved to data/logs/proxy_test_results.json")
        
    except Exception as e:
        print(f"❌ Error testing proxy: {str(e)}")
        return False
    
    return True

if __name__ == "__main__":
    test_brightdata_proxy() 

================================================================================
File: transcriber.py
================================================================================

"""
Module for extracting audio from videos and transcribing with Whisper
"""
import os
import json
import glob
import subprocess
import logging
from tqdm import tqdm
import whisper

from config import (
    DOWNLOAD_DIR,
    AUDIO_DIR,
    TRANSCRIPT_DIR,
    WHISPER_MODEL
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('transcriber.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('transcriber')

def setup_directories():
    """Create necessary directories if they don't exist"""
    os.makedirs(AUDIO_DIR, exist_ok=True)
    os.makedirs(TRANSCRIPT_DIR, exist_ok=True)
    
    # Create account-specific directories
    for account_dir in os.listdir(DOWNLOAD_DIR):
        if os.path.isdir(os.path.join(DOWNLOAD_DIR, account_dir)):
            os.makedirs(os.path.join(AUDIO_DIR, account_dir), exist_ok=True)
            os.makedirs(os.path.join(TRANSCRIPT_DIR, account_dir), exist_ok=True)

def extract_audio(video_path, audio_path):
    """Extract audio from video using FFmpeg"""
    try:
        cmd = f'ffmpeg -i "{video_path}" -vn -acodec pcm_s16le -ar 16000 -ac 1 "{audio_path}" -y'
        subprocess.call(cmd, shell=True)
        return True
    except Exception as e:
        logger.error(f"Error extracting audio from {video_path}: {str(e)}")
        return False

def process_videos():
    """Process all downloaded videos that haven't been transcribed yet"""
    setup_directories()
    
    # Load Whisper model
    logger.info(f"Loading Whisper model: {WHISPER_MODEL}")
    model = whisper.load_model(WHISPER_MODEL)
    logger.info("Model loaded successfully")
    
    # Get all video files
    all_videos = []
    for account_dir in os.listdir(DOWNLOAD_DIR):
        account_path = os.path.join(DOWNLOAD_DIR, account_dir)
        if os.path.isdir(account_path):
            videos = glob.glob(os.path.join(account_path, "*.mp4"))
            all_videos.extend(videos)
    
    logger.info(f"Found {len(all_videos)} total videos to process")
    
    # Process each video
    for video_path in tqdm(all_videos, desc="Processing videos"):
        # Extract account name and filename
        parts = video_path.split(os.sep)
        account = parts[-2]
        filename = os.path.basename(video_path)
        base_name = os.path.splitext(filename)[0]
        
        # Define paths
        audio_path = os.path.join(AUDIO_DIR, account, f"{base_name}.wav")
        transcript_path = os.path.join(TRANSCRIPT_DIR, account, f"{base_name}.json")
        
        # Skip if already transcribed
        if os.path.exists(transcript_path):
            continue
        
        # Extract audio if needed
        if not os.path.exists(audio_path):
            logger.info(f"Extracting audio from {filename}")
            os.makedirs(os.path.dirname(audio_path), exist_ok=True)
            if not extract_audio(video_path, audio_path):
                continue
        
        # Transcribe audio
        try:
            logger.info(f"Transcribing {filename}")
            result = model.transcribe(audio_path)
            
            # Get metadata if available
            metadata_path = os.path.join(DOWNLOAD_DIR, account, "metadata", f"{base_name}.json")
            metadata = {}
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            
            # Create transcript with metadata
            transcript_data = {
                "text": result["text"],
                "segments": result["segments"],
                "language": result["language"],
                "filename": filename,
                "account": account,
                **metadata
            }
            
            # Save transcript
            os.makedirs(os.path.dirname(transcript_path), exist_ok=True)
            with open(transcript_path, 'w', encoding='utf-8') as f:
                json.dump(transcript_data, f, ensure_ascii=False, indent=4)
            
            logger.info(f"Transcription complete for {filename}")
            
        except Exception as e:
            logger.error(f"Error transcribing {filename}: {str(e)}")
    
    logger.info("Transcription process completed")

if __name__ == "__main__":
    process_videos() 